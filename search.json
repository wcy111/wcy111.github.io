[{"title":"Norm basic","url":"/2022/02/15/Norm-basic/","content":"\n{% meting \"1407648820\" \"netease\" \"song\" %}\n\nå…³äºèŒƒæ•°(Norm)çš„ä¸€äº›åŸºç¡€çŸ¥è¯†\n\n<!-- more -->\n\n{% asset_img image.png %}\n\n\nä¸ºäº†é˜²æ­¢é—å¿˜ + å¿«é€Ÿæ£€ç´¢\n# Vector Norm\n\n## Definition\n\nA vector norm is any real-valued function $\\|\\cdot\\|$ that satisfies the following properties\n\n- if $x \\neq 0$  then $\\|x\\|\\geq0$\n\n- for any $\\alpha \\in \\mathbb{R}, \\text { then }\\|\\alpha \\boldsymbol{x}\\|=|\\alpha|\\|\\boldsymbol{x}\\|$\n- triangle inequality: $\\|\\boldsymbol{x}+\\boldsymbol{y}\\| \\leq\\|\\boldsymbol{x}\\|+\\|\\boldsymbol{y}\\|$\n\nIt can be valid, $\\|\\boldsymbol{x}\\|_{p}=\\left(\\sum_{i}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$, for any $p \\geq 1$\n$$\n\\begin{aligned}\n&\\|\\boldsymbol{x}\\|_{1}=\\sum_{i}\\left|x_{i}\\right| \\\\\n&\\|\\boldsymbol{x}\\|_{2}=\\sqrt{\\sum_{i} x_{i}^{2}} \\\\\n&\\|\\boldsymbol{x}\\|_{\\infty}=\\max _{i}\\left|x_{i}\\right| .\n\\end{aligned}\n$$\n\n##  Property\n\n- Larger p results in smaller norm: $\\|x\\|_{\\infty}\\leq\\|x\\|_{2}\\leq\\|x\\|_{1}$\n\n- Some  inequalities:\n\n$$\n\\begin{aligned}\n  &\\|\\boldsymbol{x}\\|_{\\infty} \\leq\\|\\boldsymbol{x}\\|_{1} \\leq n\\|\\boldsymbol{x}\\|_{\\infty} \\\\\n  &\\|\\boldsymbol{x}\\|_{2} \\leq\\|\\boldsymbol{x}\\|_{1} \\leq \\sqrt{n}\\|\\boldsymbol{x}\\|_{2} \\\\\n  &\\|\\boldsymbol{x}\\|_{\\infty} \\leq\\|\\boldsymbol{x}\\|_{2} \\leq \\sqrt{n}\\|\\boldsymbol{x}\\|_{\\infty}\n  \\end{aligned}\n$$\n\nwhere $n$ is dimensions of the vector.\n\n## Norm for optimization problem\n\n$\\|\\boldsymbol{x}\\|_{0}$ is not a norm, optimizing (P0) is NP-hard\n$\\|\\boldsymbol{x}\\|_{1}$ is a convex surrogate of $\\|\\boldsymbol{x}\\|_{0}$, can be efficiently optimized\n\n$(P 0) \\quad \\min _{\\boldsymbol{x}}\\|\\boldsymbol{x}\\|_{0}, \\quad$ s.t. $\\quad \\boldsymbol{y}=\\boldsymbol{A} \\boldsymbol{x}$\n$(P 1) \\quad\\min _{\\boldsymbol{x}}\\|\\boldsymbol{x}\\|_{1}, \\quad$ s.t. $\\quad \\boldsymbol{y}=\\boldsymbol{A} \\boldsymbol{x}$\n\nPS: $\n\\|x\\|_{0}=\\lim _{p \\searrow 0}\\|x\\|_{p}=\\sum_{i=1}^{n} \\mathbb{1}_{x_{i} \\neq 0}, \\text { where } \\mathbb{1} . \\text { is an indicator function. }\n$\n\n\n\n# Matrix Norm\n\n-Spectral norm\n\n-Frobenius norm.\n\n-Schatten p-norm\n\n-nuclear/trace norm\n\n-Unitary invariant matrix norm\n\n\n\nç©ºäº†å†æ•´ç†\n","tags":["Optimization","Math","Norm"],"categories":["Math","Optimization","Norm"]},{"title":"Docker","url":"/2022/02/13/Docker/","content":"\n{% asset_img docker.jpeg %}\n\nç”¨åˆ°äº†dockeræ¥deploy toy project,  æ²¡æ€ä¹ˆæ¥è§¦è¿‡, æµ…äº†è§£ä¸€ä¸‹.\n\n<!-- more -->\n\n## Motivation\nåŒä¸€applicationæ”¾åˆ°ä¸åŒçš„æ“ä½œç³»ç»Ÿ, ç”±äºç¯å¢ƒé—®é¢˜ä¼šå‡ºç°å„ç§æ¯›ç—…, åº”ç”¨dockerå¯ä»¥è§£å†³æ­¤ç±»é—®é¢˜ã€‚\n\n## Basic idea\n\nDocker file, Docker image, Containerä¹‹é—´çš„å…³ç³»å¦‚ä¸‹å›¾:\n\n{% asset_img relation.png %}\n\n### Dockerfile\n\nDockerfile æ˜¯ä¸€ä¸ªç”¨æ¥æ„å»ºé•œåƒçš„æ–‡æœ¬æ–‡ä»¶ï¼Œæ–‡æœ¬å†…å®¹åŒ…å«äº†ä¸€æ¡æ¡æ„å»ºé•œåƒæ‰€éœ€çš„æŒ‡ä»¤å’Œè¯´æ˜ã€‚\n\n**Docker file**  builds what's called a **docker image** which contains all the project code. (Also any installments of programs that it need)\n\n### Docker image\n\nDocker æŠŠåº”ç”¨ç¨‹åºåŠå…¶ä¾èµ–, æ‰“åŒ…åœ¨é•œåƒ(image)æ–‡ä»¶é‡Œé¢ã€‚åªæœ‰é€šè¿‡è¿™ä¸ªæ–‡ä»¶, æ‰èƒ½ç”Ÿæˆ Dockerå®¹å™¨ã€‚é•œåƒæ–‡ä»¶å¯ä»¥çœ‹ä½œæ˜¯å®¹å™¨çš„è®¾è®¡è“å›¾ã€‚Docker æ ¹æ®é•œåƒæ–‡ä»¶ç”Ÿæˆå®¹å™¨çš„å®ä¾‹ã€‚åŒä¸€ä¸ª é•œåƒæ–‡ä»¶, å¯ä»¥ç”Ÿæˆå¤šä¸ªåŒæ—¶è¿è¡Œçš„å®¹å™¨å®ä¾‹ã€‚\n\n### Container\n\né•œåƒè¿è¡Œæ—¶çš„å®ä½“, é•œåƒ(Image)å’Œå®¹å™¨(Container)çš„å…³ç³»ï¼Œå°±åƒæ˜¯é¢å‘å¯¹è±¡ç¨‹åºè®¾è®¡ä¸­çš„ç±»å’Œå®ä¾‹ä¸€æ ·ï¼Œé•œåƒæ˜¯é™æ€çš„å®šä¹‰ï¼Œå®¹å™¨æ˜¯é•œåƒè¿è¡Œæ—¶çš„å®ä½“ã€‚å®¹å™¨å¯ä»¥è¢«åˆ›å»ºã€å¯åŠ¨ã€åœæ­¢ã€åˆ é™¤ã€æš‚åœç­‰.\n\n## Reference\n\nhttp://dockone.io/article/6051\n\nhttps://turingplanet.org/2020/11/12/docker-intro-1/\n","tags":["Machine Learning","Linux","Docker"],"categories":["Linux","Server"]},{"title":"Convergence of GD","url":"/2022/02/03/Convergence of GD/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\nto be written\n","tags":["Optimization","Math","Gradient decent"],"categories":["Math","Optimization"]},{"title":"Active learning","url":"/2022/02/02/ä¸»åŠ¨å­¦ä¹ /","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\nML system project proposalè¦åŒ…å«active learning component æœ‰ç‚¹è¿·, äº†è§£ä¸‹æ¦‚å¿µå¥½å§ã€‚\n\n## Active learning\n\nä¸»åŠ¨å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡æŠ€æœ¯æ‰‹æ®µæˆ–è€…æ•°å­¦æ–¹æ³•é™ä½äººä»¬æ ‡æ³¨æˆæœ¬çš„ç ”ç©¶é¢†åŸŸã€‚\n\n### Basic idea\n\nâ€œä¸»åŠ¨å­¦ä¹ â€œ ä¼šæŠŠæ¯”è¾ƒéš¾åˆ†ç±»çš„æ ·æœ¬æŒ‘å‡ºæ¥ç„¶åäººå·¥æ‰“æ ‡ç­¾ã€‚ç›¸æ¯”è¾ƒä¸éšæœºæŠ½æ ·ç›¸åŒæ•°é‡çš„æ ·æœ¬è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œä¸»åŠ¨å­¦ä¹ ä¼šå¾—åˆ°æ›´åŠ ç†æƒ³çš„æ•ˆæœã€‚\n\n### Recipe\n\n1. å°†å¾…æ ‡æ³¨çš„æ•°æ®å †å åœ¨unlabeled poolæ•°æ®åº“é‡Œ, é€šè¿‡ä¸»åŠ¨å­¦ä¹ ä¸­çš„query functionæ¥æå–æœ‰ä»·å€¼çš„æ ·æœ¬ã€‚\n2. äººå·¥æ ‡æ³¨å¾—åˆ°æ ‡ç­¾ï¼Œç›¸å½“äºåŠ å…¥ä¸“å®¶ç»éªŒæˆ–è€…ä¸šåŠ¡ç»éªŒçš„å…ˆéªŒã€‚\n3. æœºå™¨å­¦ä¹ æ¨¡å‹æ›´æ–°ï¼Œé€šè¿‡å¢é‡å­¦ä¹ æˆ–è€…é‡æ–°å­¦ä¹ çš„æ–¹å¼æ›´æ–°æ¨¡å‹ï¼Œä»è€Œå°†äººå·¥æ ‡æ³¨çš„æ•°æ®èå…¥æ¨¡å‹ä¸­ï¼Œæå‡æ€§èƒ½ã€‚\n\n![img](https://miro.medium.com/max/1400/1*HTY-rnkzaufCPOxBjSk_oQ.png)\n\n### Application scenario\n\n- ä¸ªæ€§åŒ–åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨è®­ç»ƒ\n- å¼‚å¸¸æ£€æµ‹\n","tags":["Machine learning","Active learning"],"categories":["Machine learning"]},{"title":"å¼€å¼ äº†","url":"/2022/01/24/Blogåˆå¼€å¼ äº†/","content":"{% meting \"28996919\" \"netease\" \"song\" %}\n\nå› ä¸ºå¿™ï¼ˆå®åˆ™æ‡’ï¼‰åœæ›´åšå®¢ä¿©ä¸ªæœˆã€‚ä½†æ˜¯ï¼Œç°åœ¨å…¨ä½“ç›®å…‰å‘æˆ‘çœ‹é½ï¼Œæˆ‘å®£å¸ƒè¿™ä¸ªblogé‡æ–°å¼€å¼ äº†ã€‚\n\nå¹¶ä¸”æƒ³ä»¥æ­¤ç¯‡ä¸ºç»“ç‚¹ï¼Œè®°å½•ä¸‹æˆ‘åœ¨UMç¬¬ä¸€å­¦æœŸçš„ä¸€äº›ç»å†ä½œä¸ºä¸€ä¸ªå°å°çš„milestoneã€‚æœ€åç®€å•çš„è§„åˆ’ä¸‹æ¥ä¸‹æ¥çš„æ‰“ç®—ã€‚ç„¶åå°±å‡†å¤‡å¼€å§‹æ›´æ–°2022ç¬¬ä¸€ç¯‡å­¦ä¹ åšå®¢ã€‚\n\n<!-- more -->\n\n## ç»å†\n\n### å­¦ä¹ \n\næœ€å¤§çš„æ„Ÿå—å°±æ˜¯æˆ‘é€Ÿæˆçš„å¡‘æ–™ğŸ‚ğŸ´è‹±è¯­åœ¨çœŸçš„è‹±è¯­ç¯å¢ƒé­å—åˆ°äº†æ¯ç­æ€§æ‰“å‡»ï¼\n\nç„¶åç›´æ¥è¾å°„åˆ°æˆ‘çš„è¯¾å ‚çŸ¥è¯†çš„å¸æ”¶ï¼Œå¯¼è‡´äº†åˆšå¼€å­¦æˆ‘ç„¦è™‘åˆ°å¿ƒæ€å˜å½¢ã€‚\n\nå¹¸è¿çš„æ˜¯æœ‰å½•å±ï¼Œä¸å¹¸çš„æ˜¯å‘ç°è¯¾éš¾å¾—ä¸€åŒ¹ï¼Œä½œä¸šç¦»è°±çš„ä¹Ÿå¾ˆï¼Œæ›´éš¾é¡¶çš„æ˜¯å‘ç°å¤ªä¹…æ²¡ç¢°æ•°å­¦è‡ªå·±å·²ç»è¿ç§¯åˆ†æ±‚å¯¼æ³•åˆ™éƒ½å¿˜å·®ä¸å¤šäº†ã€‚ï¼ˆæ‰€ä»¥è¯´å·¥ç§‘é¡¹ç›®å½•å–åè¡¥ä¸‹æ•°å­¦ä¸“ä¸šçŸ¥è¯†è¿˜æ˜¯è›®é‡è¦çš„ï¼Œå½“ç„¶è¿˜æ˜¯å¯¹æœ¬æ¥å°±æ¯”è¾ƒèœåˆå®¹æ˜“å¿˜çš„èœç‹—æˆ‘æ¥è¯´çš„ï¼‰\n\nä½†æ˜¯å¤šèŠ±æ—¶é—´æ…¢æ…¢å•ƒè¿˜æ˜¯å¯ä»¥æŠŠè®²çš„å¼„æ‡‚ï¼Œè¯¾åä½œä¸šèŠ±å¾ˆå¤šæ—¶é—´å’Œå¤§ä½¬ä»¬äº¤æµä¸‹å»å»office hourè¿˜æ˜¯å¯ä»¥å‹‰å¼ºåº”ä»˜ã€‚è™½ç„¶æœ€åéƒ½æ‹¿äº†Aï¼Œä½†æ˜¯æ„Ÿè§‰è‡ªå·±å·²ç»è¢«æ¶çš„æ²¡å•¥å¿ƒæ°”äº†ã€‚\n\nå‡æœŸé‡Œå’Œå…„å¼Ÿä»¬æ‰“äº†ä¸ªkaggleæ¯”èµ›ï¼Œå› ä¸ºè¿‡äºè¿·ä¿¡LBåˆ†æ•°æ— è§†local CVç—›å¤±å¥–ç‰Œã€‚\n\n### ç”Ÿæ´»\n\nå‘ç°è‡ªå·±ç”Ÿæ´»æŠ€èƒ½ç›´æ¥åå‘æ‹‰æ»¡ï¼Œå¾ˆå¤šç”Ÿæ´»å¸¸è¯†éƒ½ä¸çŸ¥é“ï¼Œæäº†å¾ˆå¤šå°´å°¬çš„äº‹æƒ…ã€‚\n\nç„¶åå‘ç°ç”Ÿæ´»å’Œå­¦ä¹ æœ‰æ—¶å€™æ¯”è¾ƒéš¾å¹³è¡¡ï¼Œç‰¹åˆ«æ˜¯å½“ç”Ÿæ´»ä¸Šæœ‰è®¸å¤šçäº‹çš„æ—¶å€™ä¼šç›´æ¥å½±å“å­¦ä¹ ï¼Œæ—¶é—´å®‰æ’ç®¡ç†è›®é‡è¦ã€‚\n\nåŸºæœ¬æ²¡ä¸Šè¿‡æ—©è¯¾ï¼Œæ—©ä¸Šæ¯å¤©éƒ½ç¡åˆ°å¤§ä¸­åˆï¼Œæœ‰æ—¶å€™é¢å¯¹å‹åŠ›å³ä½¿æ—©èµ·äº†æ˜æ˜ä¸å›°ï¼Œä¹Ÿç»å¸¸ç¡å›ç¬¼è§‰å»é€ƒé¿ã€‚\n\né»‘äº”ä¹°äº†ä¸ªç‰›é©¬ç”µè„‘ï¼Œä¿®å¤å„ç§é”™è¯¯èŠ±äº†æˆ‘å¥½å¤šæ—¶é—´ã€‚\n\n---\n\nç§Ÿæˆ¿æ‰¾å®¤å‹æ˜¯é—¨å­¦é—®ï¼Œæˆ‘ç°åœ¨ç»ˆäºç›¸ä¿¡éšä¾¿æ‰¾çš„å®¤å‹åˆå¾—æ¥çš„å°‘ä¹‹ç”šå°‘è¿™å¥è¯ã€‚ï¼ˆæ ¹æ®æˆ‘è‡ªèº«è¿™ä¸ªpowerful datapointä»¥åŠèº«è¾¹æ—å‹çš„æŠ½æ ·è¯å®äº†è¿™ä¸ªè®ºç‚¹ï¼‰\n\n## æ”¶è·\n\n- è‹±è¯­æ°´å¹³ä»ç‰›é©¬levelåˆ°äº†çBBlevel\n- ç”Ÿæ´»æŠ€èƒ½å¾—åˆ°äº†æé«˜\n- å¯¹ä¸€äº›äº‹ç‰©çš„å¿è€åº¦æœ‰æ‰€ä¸Šå‡ï¼Œå¿ƒæ€çˆ†ç‚¸çš„é˜ˆå€¼æé«˜ã€‚\n- è„¸çš®å˜åšï¼Œç¤¾äº¤æ°´å¹³æå‡ä¸€ä¸¢ä¸¢\n\n## æ€»ç»“\n\n- æœ€é‡è¦çš„ä¸€ç‚¹æ˜¯æ— è®ºæ˜¯å–œæ¬¢çš„äººè¿˜æ˜¯æƒ³åšçš„äº‹ï¼Œéƒ½ä¸èƒ½ç­‰åˆ°è‡ªå·±è§‰å¾—qualifiedçš„æ—¶å€™å†è¡ŒåŠ¨ï¼Œæœ¬è´¨ä¸Šè¿™æ˜¯ä¸€ç§æ‹–å»¶å’Œé€ƒé¿ã€‚çœŸçš„è§‰å¾—è‡ªå·±è¡Œäº†çš„æ—¶å€™è¦ä¹ˆæœºä¼šå·²ç»æ— äº†è¦ä¹ˆåˆæœ‰äº†å…¶ä»–çš„å€Ÿå£ã€‚ï¼ˆå¾ˆåæ‚”é«˜ä¸‰&å¤§ä¸€çš„æ—¶å€™éƒ½æ²¡èƒ½è¯´å‡ºå£.....ï¼‰\n\n### å­¦ä¹ \n\n- èƒ½åŠ›ä¸è¶³ä»¥æ»¡è¶³æ¬²æœ›åˆç¼ºä¹è€å¿ƒå°†ç›´æ¥å¯¼è‡´ç„¦è™‘ï¼Œç„¦è™‘è®©å¿ƒæ€å¤±è¡¡ï¼Œä¸åˆ©äºçŒ¥çå‘è‚²ã€‚ä¸åŠæ—¶è°ƒæ•´å°±ä¼šå‘ç”Ÿé€†é£æµªï¼Œæµªå®Œffçš„æ‚²æƒ¨ç»“å±€ã€‚\n- æ•ˆç‡ä½çš„æ—¶å€™ï¼Œä¸è¦æç€å»æƒ³å¤šèŠ±æ—¶é—´è€Œæ˜¯è¦ä»ç³»ç»Ÿå±‚é¢ï¼Œæ–¹æ³•è®ºä¸Šå»ä¼˜åŒ–ï¼ŒåŠæ—¶å¤ç›˜ã€‚ï¼ˆè¿™å­¦æœŸä¸€ç›´æ²¡åšå¥½ï¼Œä¸€ç›´è¢«å°è¥å°åˆ©è¯±æƒ‘å¹²äº†å¾ˆå¤šé‡å¤åŠ³åŠ¨æµªè´¹æ—¶é—´çš„äº‹æƒ…ï¼‰æ¢å¥è¯è¯´æ˜¯ä¸è¦åœ¨æ„ä¸€æ—¶å¾—å¤±ï¼Œç›¸ä¿¡scale.\n- æœ‰æ—¶å€™æ€»å–œæ¬¢æ³›æ³›çš„å­¦ï¼Œä¸æ·±å…¥ä½†æ˜¯èŠ±æ—¶é—´æ²¡æœ‰è¿›è¡Œç¼œå¯†çš„æ€è€ƒï¼Œæˆ‘å‘ç°å¤§å¤šæ—¶å€™è¿™ä¹ˆåšéƒ½æ˜¯ä½æ•ˆæ— ç›Šçš„ï¼Œä¸å¦‚å¤šèŠ±æ—¶é—´å»æŠŠä¸€ä¸ªç‚¹ç¢ç£¨é€ã€‚ï¼ˆæ¯”å¦‚è¯´è‹±è¯­å¬åŠ›æ³›å¬å°±æ˜¯ä¸€ä¸ªé™·é˜±ï¼‰\n\n### ç”Ÿæ´»\n\n- å¯¹ä¸èµ·è¿™å­¦æœŸæŸä¸ªè€å§ï¼Œæˆ‘çœŸçš„åªæ˜¯æœ‰ç‚¹ç¤¾æåŠ ä¸è‡ªä¿¡ï¼Œæˆ‘åº”è¯¥ç¨å¾®ä¸»åŠ¨ç‚¹çš„\n\n- å¤šå»å°è¯•ï¼Œä¸è¦æƒ³å¤ªå¤š\n- ä½è½çš„æ—¶å€™æ‰¾æœ‹å‹ä»¬èŠèŠå¤©\n\n- é‡åˆ°è§‰å¾—ä¸å¯¹åŠ²çš„äººå¼€æºœå°±å®Œäº‹äº†, e.g. é æ‰“å‹ä»–äººæ¥æå‡è‡ªå·±è‡ªä¿¡æˆ–æƒ…ç»ªä¸ç¨³å®šçš„...\n\n## è®¡åˆ’\n\n- å°‘æ²¾ç‚¹æŠ½è±¡\n\n- æ™šä¸Šä¸è¦æ‰“è‹±é›„è”ç›Ÿè¿‡åº¦ï¼Œæ—©èµ·è…¾å‡ºèµ–åºŠçš„æ—¶é—´æ¥è‹±è¯­æå‡ï¼Œå…»æˆä¹ æƒ¯[] \n- æ‰¾åˆ°labåšç‚¹é¡¹ç›®åº¦è¿‡æš‘å‡ã€‚[]\n- å‘ç°å’Œå¤§ä¸€ç™½æœˆå…‰ç›¸ä¼¼åº¦90%çš„å¥³ç”Ÿï¼Œå­¦æœŸç»“æŸå‰è¦åˆ°è”ç³»æ–¹å¼hhhhh.   []\n\n----\n\næœ€ååœ¨è¿™ç¯‡blogç»“æŸä¹‹é™…ï¼Œç¥ç¦è¯¸ä½æ–°å¹´å¿«ä¹ï¼Œç¥è‡ªå·±å¿ƒæƒ³äº‹æˆå“ˆå“ˆå“ˆã€‚\n\n{% asset_img tiger.jpg %}\n\n","tags":["Life","conclusion"],"categories":["Life","Record"]},{"title":"SVD(Singular value decomposition)","url":"/2022/01/24/SVD/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n## \n","tags":["Math","SVD"],"categories":["Math","Linear Algebra"]},{"title":"Poem2","url":"/2021/11/12/Poem2/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n# O Me! O Life!\n\nby Walt Whitman - 1819-1892\n\nO Me! O life! of the questions of these recurring,\nOf the endless trains of the faithless, of cities fillâ€™d with the foolish,\nOf myself forever reproaching myself, (for who more foolish than I, and who more faithless?)\nOf eyes that vainly crave the light, of the objects mean, of the struggle ever renewâ€™d,\nOf the poor results of all, of the plodding and sordid crowds I see around me,\nOf the empty and useless years of the rest, with the rest me intertwined,\nThe question, O me! so sad, recurringâ€”What good amid these, O me, O life?\n\n*Answer*.\n\nThat you are hereâ€”that life exists and identity,\nThat the powerful play goes on, and you may contribute a verse.\n\n\n\n","tags":["Life","Poem"],"categories":["Life","Poem"]},{"title":"Kernel function and Kernel trick","url":"/2021/11/09/Kernel-function and-Kernel-trick/","content":"\n## Nonlinear Feature Map\n\nTo creat a nonlinear method for regression or classifier is to transform **feature vector** via a **nonlinear feature map** \n$$\n\\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{m} \\tag{1.1}\n$$\n\n\nThis way is pretty intuitive since we can just image adding some features on the original feature vector. Then apply this feature map over whole dataset get $(\\Phi(x_{i}),y_{i})\\quad \\forall i)$\n\n\n\nThus, in regression, the classifier is :\n$$\nf(\\boldsymbol{x})=\\boldsymbol{w}^{T} \\boldsymbol{\\phi}(\\boldsymbol{x})+b \\tag{1.2}\n$$\nWhere $\\boldsymbol{w}\\in \\mathbb{R}^m\\quad b \\in\\mathbb{R}$\n\n## Inner Product Kernels\n\n- Many ML algorithms depend on $\\boldsymbol{\\phi}(\\boldsymbol{x})$ only via *inner products*\n  $$\n  \\langle \\Phi(x),\\phi(x^{\\prime})\\rangle  \\tag{1.3}\n  $$\n\n- for certain $\\Phi$, the function\n  $$\n  k(u,v)=\\langle\\Phi(u),\\Phi(v)\\rangle \\tag{1.4}\n  $$\n  can be computed efficiently even m is huge or possibly infinite. k is called an inner product kernel.\n\n---\n\n### Some important kernels\n\nHomogeneous polynomia kernel\n$$\nk(u,v)=(u^Tv)^{p} \\tag{1.5}\n$$\nInhomogeneous polynomia kernel\n$$\nk(u,v)=(u^Tv+b)^{p} \\tag{1.6}\n$$\nGaussian kernel (not list here)\n\n### SPD kernels\n\n- One way to determine an inner product kernel is to construct $\\Phi$ explicitly.\n\n- Another way is to verify that k is an kernel if it satisfies the following properties\n\n  - $k:\\mathbb{R}^{d}\\times \\mathbb{R}^{d} \\rightarrow\\mathbb{R}$. k is symmetric i.e. $k(u,v)=k(v,u) \\quad\\forall u,v$\n\n  - $k$ is positive definite if:\n    $$\n    \\left[\\begin{array}{ccc}\n    k\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{n}\\right) \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    k\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{n}\\right)\n    \\end{array}\\right]\n    $$\n    is a PSD matrix for all $n\\in \\mathbb{N}$, and $x_{1}...x_{n}\\in\\mathbb{R}^{d}$\n\n- **Theorem**: k is an SPD kernel iff k is an inner product kernel.\n\n---\n\n## The Kernel Trick\n\nA machine learning algorithm is said to be **kernelizable** if it is possible to formulate the algorithm such that all training instances $x_{i}$ and any test instance $x$ occur in inner products of the form$\\langle x_{i},x{_{j}} \\rangle,\\langle x_{i},x \\rangle,\\text{or}\\langle x,x \\rangle$\n\n----\n\nSince we can suppose $\\Phi$ is a feature map associatecd to an inner product kernel k, if we apply a kernelizable algorithm to the training data  like constructing :\n$$\n\\left(\\boldsymbol{\\Phi}\\left(\\boldsymbol{x}_{1}\\right), y_{1}\\right), \\ldots,\\left(\\boldsymbol{\\Phi}\\left(\\boldsymbol{x}_{n}\\right), y_{n}\\right) \\tag{1.7}\n$$\nthen we can formulate the algorithm such that transformed feature vectors only appear via inner products $\\langle\\boldsymbol{\\Phi(x),\\Phi(x^{\\prime})}\\rangle$ with other similar transformed feature vectores.\n\n- this can be implemented by evaluating $k(u,v)=\\langle\\Phi(u),\\Phi(v)\\rangle$ which eliminates the need to ever compute $\\Phi(x)$.\n\n---\n\n## Kernel Ridge Regression\n\nRidge regression is kernelizable. use the kernel trick to extend it to a nonlinear method called ridge regression.\n\n---\n\n### Kernel ridge regresion(without offset)\n\nsince some $\\Phi$ contain a constant term, as with the inhomogeous polynomial kernel, the offset is not always needed\n\nFor the KRR(no offset), we have objective function \n$$\n\\begin{aligned}\n\\min _{w}\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)^{2}+\\lambda\\|\\boldsymbol{w}\\|^{2} &=\\frac{1}{n}\\|\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w}\\|^{2}+\\lambda\\|\\boldsymbol{w}\\|^{2} \\\\\n& \\propto \\boldsymbol{w}^{T}\\left(\\boldsymbol{X}^{T} \\boldsymbol{X}+n \\lambda \\boldsymbol{I}\\right) \\boldsymbol{w}-2 \\boldsymbol{y}^{T} \\boldsymbol{X} \\boldsymbol{w}+\\boldsymbol{y}^{T} \\boldsymbol{y}\n\\end{aligned}\n \\tag{1.8}\n$$\nThe solution is\n$$\n\\widehat{\\boldsymbol{w}}=\\left(\\boldsymbol{X}^{T} \\boldsymbol{X}+n \\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{X}^{T} \\boldsymbol{y}\\tag{1.9}\n$$\nWhere \n$$\n\\boldsymbol{y}=\\left[\\begin{array}{c}\ny_{1} \\\\\n\\vdots \\\\\ny_{n}\n\\end{array}\\right], \n\n\\quad \\boldsymbol{X}=\\left[\\begin{array}{c}\n\\boldsymbol{x}_{1}^{T} \\\\\n\\vdots \\\\\n\\boldsymbol{x}_{n}^{T}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times d}\n$$\nNote $X^TX$ is not the gram matrix of the training data (it belongs to $\\mathbb{R^{d\\times d}}$), thus we need to further transform the solution\n\n- apply matrix inversion lemma:\n  $$\n  (\\mathbf{P}+\\mathbf{Q R S})^{-1}=\\mathbf{P}^{-1}-\\mathbf{P}^{-1} \\mathbf{Q}\\left(\\mathbf{R}^{-1}+\\mathbf{S P}^{-1} \\mathbf{Q}\\right)^{-1} \\mathbf{S P}^{-1} \\tag{1.10}\n  $$\n\nAfter simplification we get \n$$\n\\widehat{\\boldsymbol{w}}^{T}=\n\\boldsymbol{y}^{T}\\left(\\mu \\boldsymbol{I}+\\boldsymbol{X} \\boldsymbol{X}^{T}\\right)^{-1} \\boldsymbol{X} \\tag{1.11}\n$$\nNote the Gram matrix $G=XX^{T}$ appears, although the method is still not kernelized because of the matrix $X$ . However, this can be resolved by taking the inner product of $\\widehat{w}$ with a test instance $x$ . Thus introduce the notation \n$$\n\\boldsymbol{G}:=\\left[\\begin{array}{ccc}\n\\left\\langle\\boldsymbol{x}_{1}, \\boldsymbol{x}_{1}\\right\\rangle & \\cdots & \\left\\langle\\boldsymbol{x}_{1}, \\boldsymbol{x}_{n}\\right\\rangle \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\left\\langle\\boldsymbol{x}_{n}, \\boldsymbol{x}_{1}\\right\\rangle & \\cdots & \\left\\langle\\boldsymbol{x}_{n}, \\boldsymbol{x}_{n}\\right\\rangle\n\\end{array}\\right] \\quad \\boldsymbol{g}(\\boldsymbol{x}):=\\left[\\begin{array}{c}\n\\left\\langle\\boldsymbol{x}_{1}, \\boldsymbol{x}\\right\\rangle \\\\\n\\vdots \\\\\n\\left\\langle\\boldsymbol{x}_{n}, \\boldsymbol{x}\\right\\rangle\n\\end{array}\\right]\n$$\nThen we can have classifier as\n$$\n\\begin{aligned}\n\\widehat{f}(\\boldsymbol{x}) &=\\widehat{\\boldsymbol{w}}^{T} \\boldsymbol{x} \\\\\n&=\\boldsymbol{y}^{T}\\left(\\boldsymbol{X} \\boldsymbol{X}^{T}+n \\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{X} \\boldsymbol{x} \\\\\n&=\\boldsymbol{y}^{T}(\\boldsymbol{G}+n \\lambda \\boldsymbol{I})^{-1} \\boldsymbol{g}(\\boldsymbol{x})\n\\end{aligned} \\tag{1.12}\n$$\nThis shows that KRR w/o is kernelizable. So we apply kernek trick by simply selecting a kerenel k and replace $\\langle u,v\\rangle$ with $k(u,v)$ . After substituion, $G$ and $g(x)$ are replaced by \n$$\n\\boldsymbol{K}:=\\left[\\begin{array}{ccc}\nk\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{n}\\right) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nk\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{n}\\right)\n\\end{array}\\right], \\quad \\boldsymbol{k}(\\boldsymbol{x}):=\\left[\\begin{array}{c}\nk\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}\\right) \\\\\n\\vdots \\\\\nk\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}\\right)\n\\end{array}\\right]\n$$\n$K$ is called the kernel matrix. Now, the final form of the KRR(w/o) offset predictor is \n$$\n\\widehat{f}(\\boldsymbol{x})=\\boldsymbol{y}^{T}(\\boldsymbol{K}+n \\lambda \\boldsymbol{I})^{-1} \\boldsymbol{k}(\\boldsymbol{x}) \\tag{1.13}\n$$\nNote entire process is equivalent to first applying the feature map $\\Phi$ to all feature vectors, and applying ridge regression w/o in the new feature space. Because all we need is to obtain a nonlinear regression predictor and once we have selected a kernel all the calculation needed for predicting a new point is implicitly finished.\n\n#### Computational Complexity of KRR w/o\n\nThe computational complexity of KRR without offset is $O(n^3)$ which comes from having to invert an $n\\times n$ matrix.  As with regular regression, this can be accelerated using **gradient descent** and related methods\n\n## Kernel Ridge Regression  with Offset\n\nThe derivation of kernel ridge regression with offset is similar as KRR without offset, but with one important additional concept.\n\nFirst, the solution to ridge regression with offset is\n$$\n\\begin{aligned}\n&\\widehat{\\boldsymbol{w}}=\\left(\\tilde{\\boldsymbol{X}}^{T} \\tilde{\\boldsymbol{X}}+n \\lambda \\boldsymbol{I}\\right)^{-1} \\tilde{\\boldsymbol{X}}^{T} \\tilde{\\boldsymbol{y}} \\\\\n&\\widehat{b}=\\bar{y}-\\widehat{\\boldsymbol{w}}^{T} \\overline{\\boldsymbol{x}}\n\\end{aligned} \\tag{2.1}\n$$\nwhere\n$$\n\\tilde{\\boldsymbol{y}}=\\left[\\begin{array}{c}\n\\tilde{y}_{1} \\\\\n\\vdots \\\\\n\\tilde{y}_{n}\n\\end{array}\\right], \\quad \\tilde{y}_{i}=y_{i}-\\bar{y}, \\quad \\tilde{\\boldsymbol{X}}=\\left[\\begin{array}{c}\n\\tilde{\\boldsymbol{x}}_{1}^{T} \\\\\n\\vdots \\\\\n\\tilde{\\boldsymbol{x}}_{n}^{T}\n\\end{array}\\right], \\quad \\tilde{\\boldsymbol{x}}_{i}=\\boldsymbol{x}_{i}-\\overline{\\boldsymbol{x}}\n$$\nHere $\\overline{x}=\\sum _{i}x_{i}$, $\\overline{y}=\\sum _{i}y_{i}$. The regression func estimate is then\n$$\n\\widehat{f}(\\boldsymbol{x})=\\widehat{\\boldsymbol{w}}^{T} \\boldsymbol{x}+\\widehat{b}=\\bar{y}+\\widehat{\\boldsymbol{w}}^{T}(\\boldsymbol{x}-\\overline{\\boldsymbol{x}}) \\tag{2.2}\n$$\nTo kernelize this funciton, we can follow the exact same steps as for KRR without offset to arrive at \n$$\n\\widehat{\\boldsymbol{w}}^{T}=\\tilde{\\boldsymbol{y}}^{T}(\\tilde{\\boldsymbol{G}}+n \\lambda \\boldsymbol{I})^{-1} \\tilde{\\boldsymbol{X}} \\tag{2.3}\n$$\nthen \n$$\n\\widehat{f}(\\boldsymbol{x})=\\bar{y}+\\tilde{\\boldsymbol{y}}^{T}(\\tilde{\\boldsymbol{G}}+n \\lambda \\boldsymbol{I})^{-1} \\tilde{\\boldsymbol{g}}(\\tilde{\\boldsymbol{x}}) \\tag{2.4}\n$$\nWhere $\\tilde{x}=x-\\overline{x}$ and\n$$\n\\tilde{\\boldsymbol{G}}:=\\left[\\begin{array}{ccc}\n\\left\\langle\\tilde{\\boldsymbol{x}}_{1}, \\tilde{\\boldsymbol{x}}_{1}\\right\\rangle & \\cdots & \\left\\langle\\tilde{\\boldsymbol{x}}_{1}, \\tilde{\\boldsymbol{x}}_{n}\\right\\rangle \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\left\\langle\\tilde{\\boldsymbol{x}}_{n}, \\tilde{\\boldsymbol{x}}_{1}\\right\\rangle & \\cdots & \\left\\langle\\tilde{\\boldsymbol{x}}_{n}, \\tilde{\\boldsymbol{x}}_{n}\\right\\rangle\n\\end{array}\\right], \\quad \\tilde{\\boldsymbol{g}}(\\tilde{\\boldsymbol{x}}):=\\left[\\begin{array}{c}\n\\left\\langle\\tilde{\\boldsymbol{x}}, \\tilde{\\boldsymbol{x}}_{1}\\right\\rangle \\\\\n\\vdots \\\\\n\\left\\langle\\tilde{\\boldsymbol{x}}, \\tilde{\\boldsymbol{x}}_{n}\\right\\rangle\n\\end{array}\\right]\n$$\nBasicallly KRR w/  is like KRR w/o offset applied to the mean-centered feature space. In addition, $\\overline{y}$ is added to the predicted output.\n\n---\n\nTo see $\\tilde{G} ~\\text{and}~ \\tilde{g}(\\tilde{x})$ is kernelizable, we can expand the entries in the matrix as \n$$\n\\begin{aligned}\n\\left\\langle\\tilde{x}_{i}, \\tilde{x}_{j}\\right\\rangle &=\\left\\langle x_{i}-\\bar{x}, x_{j}-\\bar{x}\\right\\rangle \\\\\n&=\\left\\langle x_{i}, x_{j}\\right\\rangle-\\frac{1}{n} \\sum_{r=1}^{n}\\left\\langle x_{i}, x_{r}\\right\\rangle-\\frac{1}{n} \\sum_{s=1}^{n}\\left\\langle x_{s}, x_{j}\\right\\rangle+\\frac{1}{n^{2}} \\sum_{r=1}^{n} \\sum_{s=1}^{n}\\left\\langle x_{r}, x_{s}\\right\\rangle\n\\end{aligned}\\tag{2.5}\n$$\n--(calculation a bit tricky)\n\nand\n$$\n\\begin{aligned}\n\\left\\langle\\tilde{\\boldsymbol{x}}_{i}, \\tilde{\\boldsymbol{x}}\\right\\rangle &=\\left\\langle\\boldsymbol{x}_{i} -\\overline{\\boldsymbol{x}}, \\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\right\\rangle \\\\\n&=\\left\\langle\\boldsymbol{x}_{i}, \\boldsymbol{x}\\right\\rangle-\\frac{1}{n} \\sum_{r}\\left\\langle\\boldsymbol{x}_{i}, \\boldsymbol{x}_{r}\\right\\rangle-\\frac{1}{n} \\sum_{s}\\left\\langle\\boldsymbol{x}, \\boldsymbol{x}_{s}\\right\\rangle+\\frac{1}{n^{2}} \\sum_{r} \\sum_{s}\\left\\langle\\boldsymbol{x}_{r}, \\boldsymbol{x}_{s}\\right\\rangle .\n\\end{aligned} \\tag{2.6}\n$$\nTherefore to kernelize ridge regression with offset, we select a kernel k and and replace $\\tilde{G}$\n\nwith $\\tilde{K}$  and $\\tilde{g}(x)$ with $\\tilde{k}(x)$.\n\nThen for $\\tilde{K}$ \n$$\nk\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)-\\frac{1}{n} \\sum_{r=1}^{n} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{r}\\right)-\\frac{1}{n} \\sum_{s=1}^{n} k\\left(\\boldsymbol{x}_{s}, \\boldsymbol{x}_{j}\\right)+\\frac{1}{n^{2}} \\sum_{r=1}^{n} \\sum_{s=1}^{n} k\\left(\\boldsymbol{x}_{r}, \\boldsymbol{x}_{s}\\right)\\tag{2.7}\n$$\nfor $\\tilde{k}(x)$\n$$\nk\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}\\right)-\\frac{1}{n} \\sum_{r} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{r}\\right)-\\frac{1}{n} \\sum_{s} k\\left(\\boldsymbol{x}, \\boldsymbol{x}_{s}\\right)+\\frac{1}{n^{2}} \\sum_{r} \\sum_{s} k\\left(\\boldsymbol{x}_{r}, \\boldsymbol{x}_{s}\\right) \\tag{2.8}\n$$\nThus, the final KRR(w/ offset) predictor is \n$$\n\\widehat{f}(\\boldsymbol{x})=\\bar{y}+\\tilde{\\boldsymbol{y}}^{T}(\\tilde{\\boldsymbol{K}}+n \\lambda \\boldsymbol{I})^{-1} \\tilde{\\boldsymbol{k}}(\\boldsymbol{x}) \\tag{2.9}\n$$\nNote  from eq. 2.4 it is very tempting to attempt to kernelize this method by replacing dot products $\\langle\\tilde{x},\\tilde{x^{\\prime}}\\rangle$ with $k(\\tilde{x},\\tilde{x}^{\\prime})$, but this is incorrect. Because we should let $\\Phi$ works for orginal $x$ \n\nfeature space.\n\nTo see it , we can introduce the notation \n$$\n\\tilde{\\Phi}(\\boldsymbol{x}):=\\Phi(\\boldsymbol{x})-\\frac{1}{n} \\sum_{i} \\Phi\\left(\\boldsymbol{x}_{i}\\right)\\tag{2.10}\n$$\n-- point is $\\Phi(\\tilde{x}) \\neq \\tilde{\\Phi}(x)$, so this approach make no sense.\n\nHere we refer to $\\tilde{\\Phi}$ as the *the centered feature map*, $\\tilde{k}(x,x^{\\prime}):=\\langle\\tilde{\\Phi}(x),\\tilde{\\Phi}(x^{\\prime})\\rangle$  as the centered kernel, and $\\tilde{K}$ as the *centered kernel matrix* associated to the training data set. \n\n","tags":["Machine learning","Kernel trick","Kernel logistic regression","feature map"],"categories":["Machine learning","Kernel trick"]},{"title":"Moment generating func&Characteristic func","url":"/2021/10/17/Moment-generating-func-Characteristic-func/","content":"\n![img](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYtsAcrE8dz6m4ClTgYHiL0Wtv3QbJaM9o8Q&usqp=CAU)\n\nA generating function is a clothesline on which we hang up a sequence of numbers for display ----Herbert Wlif\n\n<!-- more -->\n\n## Moment generating function\n\n### Definition of MGF\n\nwe can use moment generating function rather than PMF to define or describe a random variable.\n$$\nM(t)=\\mathbb{E}\\left[e^{t X}\\right]=\\int e^{t x} f_{X}(x) d x=\\int e^{t x} d F_{X}(x)\n$$\nWhere $X$ is a random variable of interest, $t$ is one parameter of tunction. And according to the **LOTUS** we can expand the expectation formula.\n\n----\n\nIf $M(t) \\leq \\infin$ on some interval containing the origin, then \n\n(a) $E(X)=M'(0)$    (b) $E[X^k]=M^{(k)}(0)$\n\ne.g.\n$$\n\\begin{aligned}\n&M^{\\prime}(0)=\\left.\\int x e^{t x} f_{X}(x) d x\\right|_{t=0}=\\int x f_{X}(x) d x=\\mathbb{E}[X] \\\\\n&M^{\\prime \\prime}(0)=\\left.\\int x^{2} e^{t x} f_{X}(x) d x\\right|_{t=0}=\\int x^{2} f_{X}(x) d x=\\mathbb{E}\\left[X^{2}\\right]\n\\end{aligned}\n$$\n**Notice that $M$ is function of t!!!**  and actually this structure is well-designed, everytime we take a derivative of this formula we generate a factor $x$, and we take $t =0 $  to eliminate the exponential item thus we get moment generating function.\n\n---\n\n### Property of MGF\n\n- MGF of the sum of independent random variables. \n  $$\n  M_{X+Y}(t)=\\mathbb{E}\\left[e^{t (X+Y)}\\right]=\\mathbb{E}\\left[e^{t X} e^{t Y}\\right]\n  $$\n\n- If $X,Y$ are independent then it equals to $\\mathbb{E}\\left[e^{t X} e^{t Y}\\right]=\\mathbb{E}\\left[e^{t X}]\\mathbb{E}[ e^{t Y}\\right]=M_X(t)M_Y(t)$\n\n- thus when $X_1....,X_n$ Are independently and identically distributed \n  $$\n  M_{W}(t)=\\left(M_{X}(t)\\right)^{n}\n  $$\n\nwhere $W=\\sum_{i=1}^mX_{i}$\n\n---\n\n## Characteristic function\n\nIt's very similar to the MGF but more powerful since for some random varaibles MGF doesn't exist( such as given certain $t$ MGF goes infinity). \n\n----\n\n$$\n\\begin{aligned}\n\\phi(t) &=\\mathbb{E}\\left[e^{i t X}\\right](i=\\sqrt{-1}) \\\\\n&=\\int e^{i t x} f_{X}(x) d x=\\int(\\cos t x+i \\cdot \\sin t x) f_{X}(x) d x \\\\\n&=\\mathbb{E}[\\cos t X]+i \\mathbb{E}[\\sin t X]\n\\end{aligned}\n$$\n\nProperties:\n- $\\phi(0)=\\mathbb{E}\\left[e^{i 0 X}\\right]=\\mathbb{E}[1]=1$\n- $|\\phi(t)| \\leq \\int\\left|e^{i t x}\\right| f_{X}(x) d x=\\int f_{X}(x) d x=1 .$ So $\\phi(t)$ exists while $M_{t}$ may not.\n- If $X$ and $Y$ are independent $\\phi_{X+Y}(t)=\\mathbb{E}\\left[e^{i t(X+Y)}\\right]=\\mathbb{E} e^{i t X} e^{i t Y}=\\phi_{X}(t) \\phi_{Y}(t)$\n- $Y=a X+b, a, b \\in[R]$\n\n$$\n\\phi_{Y}(t)=\\mathbb{E}\\left[e^{i t(a X+b)}\\right]=\\mathbb{E}\\left[e^{i t b} e^{i a t X}\\right]=e^{i t b} \\mathbb{E}\\left[e^{i a t X}\\right]=e^{i t b} \\phi_{X}(a t)\n$$\n\n---\n\n### Charateristic function for gaussian\n\n\n\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"Transformations(functions)","url":"/2021/10/08/Transformations-functions/","content":"\n<!-- more -->\n\n## Inverse function\n\n![img](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Inverse_Function.png/220px-Inverse_Function.png)\n\nIn mathematics, an **inverse function** (or **anti-function**) is a function  that \"reverses\" another function: if the function *f* applied to an input *x* gives a result of *y*, then applying its inverse function *g* to *y* gives the result *x*, i.e., *g*(*y*) = *x* if and only if *f*(*x*) = *y*.The inverse function of *f* is also denoted as ${\\displaystyle f^{-1}}$\n\n----\n\n## Transformations (functions)\n\nLet $X$ be a random variable and $Y=g(X)$.  Given the **PDF** of $X$, find the **PDF** of **Y** .\n\n**REMARK**: The general way to find the **PDF** is to first find the **CDF** and then differentiate it to obtain the **PDF**. \n\n### Monotonous case\n\n#### 1 strictly increasing\n\nSay $g$ is **strictly increasing** and its inverse function is $h$ i.e., $h=g^{-1}$ which is also **increasing**.\n\nThe first step is to know the **CDF** of **R.V**. $Y$ and plug-in existing conditions,\n$$\nF_{Y}(y)=P(Y \\leq y)=P(h(Y) \\leq h(y))=P(X \\leq h(y))=F_{X}(h(y))\n$$\n**REMARK**:the second equation we apply $h$ function on both side thus we get $h(Y)=X$ and according to the monotone increasing of $h$ the inequality sign remains unchanged. Then with the defination of **CDF** we get $F_{X}(h(y))$\n\nThus, we can **differentiate** CDF to get PDF where chain rule will be used:\n$$\nf_{Y}(y)=\\frac{d}{d y} F_{Y}(y)=\\frac{d}{d y} F_{X}(h(y))=f_{X}(h(y)) \\frac{d h(y)}{d y}\n$$\n\n#### 2 strictly decreasing\n\nwhen it comes to decreasing case, we have minor change, see\n$$\nF_{Y}(y)=P(Y \\leq y)=P(h(Y) \\geq h(y))=1-P(X \\leq h(y))=1-F_{X}(h(y))\n$$\n\n$$\nf_{Y}(y)=\\frac{d}{d y} F_{Y}(y)=-\\frac{d}{d y} F_{X}(h(y))=-f_{X}(h(y)) \\frac{d h(y)}{d y}\n$$\n\n### No Monotonous case \n\nSay we have $Y=X^2$, $X-U(-1,1) $ \n\nobviously function is not monitonous so we can not use the conclusion aforementioned. \n\nTo solve this, firstly get the possible range of Y i.e. $Support(Y) = [0,1]$, so we assume $y \\in Y$\n$$\nF_{Y}(y)=P(Y \\leq y)=P\\left(X^{2} \\leq y\\right)=P\\left(\\left\\{x: x^{2} \\leq y\\right\\}\\right)=P(X \\in[-\\sqrt{y}, \\sqrt{y}])\n$$\nNote $X^2 \\leq y$ actually returns a set i.e. $\\set{x:x^2 \\leq y}$, \n$$\nP(X \\in[-\\sqrt{y}, \\sqrt{y}])=\\int_{-\\sqrt{y}}^{\\sqrt{y}} f_{X}(x) d x=\\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{2} d x=\\sqrt{y}\n$$\nso\n$$\nf_{Y}(y)= \\begin{cases}F_{Y}^{\\prime}(y)=\\frac{1}{2 \\sqrt{y}} & y \\in[0,1] \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\n### Find the mapping\n\n\n\nLet $X$ and $Y$ be two random variables with PDFs $f_X$ and $f_Y$ respectively. Find a transformation $g$ such that $Y=g(X)$.\n\n**REMARK**: Since **uniform distribution** can generated any distributions , so if we can find such a function $g$ we are able to sample in the $X$ without have simulation of $Y$\n\n- Uniqueness can be guaranteed only if we assume g to be **monotone non-decreasing** or **monotone non-increasing.**\n\n--\n\n#### Example \n\n$$\n\\text { Example: } X \\sim \\mathcal{U}[0,1] \\text { and } Y \\sim \\text { Exponential }(\\lambda)\n$$\n\nFirstly, we can know $F_{X}=x$,  so $F_X(h(y))=h(y)$, \n\n\n\nSince we have known from the previous conclusion that $F_X(h(y))=F_Y(y)$ , hence we get \n$$\nh(y)=F_Y(y)=\\int_{0}^{y} \\lambda \\exp (-\\lambda t) d t=1-\\exp (-\\lambda y)\n$$\nagain $g(X)=h^-1(X)$, so to get the function $g$  we need to calculate inverse of function $h$, thus we get \n$$\ny=-\\frac{1}{\\lambda} \\log (1-h(y)): g(X)=\n\\frac{-1}{\\lambda} \\log (1-X)\n$$\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"Variance&Conditional Variance","url":"/2021/10/02/EECS501-Notes2/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n## Variance&Conditional Variance\n\n### LECTURE 6-7 Discrete Random Variables\n\n\n\n### Variance\n\nVariance of a random variable is defined as follows:\n$$\n\\operatorname{Var}(X)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^{2}\\right]\n$$\n**REMARK:** $E(X)$ is one way to summarize the PMF $P_X$, but it connot capture **randomness/uncertainty** thus we need variance.\n\n#### Alternative expresion for variance\n\n$$\n\\begin{aligned}\n\\operatorname{Var}(X) &=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^{2}\\right] \\\\\n&=\\mathbb{E}\\left[X^{2}-2 X \\mathbb{E}[X]+\\mathbb{E}[X]^{2}\\right] \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}[2 X \\cdot \\mathbb{E}[X]]+\\mathbb{E}\\left[(\\mathbb{E}[X])^{2}\\right] \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-2 \\mathbb{E}[X] \\mathbb{E}[X]+(\\mathbb{E}[X])^{2} \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-(\\mathbb{E}[X])^{2} \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}^{2}[X]\n\\end{aligned}\n$$\n\nNote that $Var(X)\\geq 0$ and it equals 0 if and only if $X$ is a **constant function** \n\n### Conditional variance\n\nconditional variance is analogous to conditional expectation. Let $A$ be some event and $X,Y$ be some random variables.\n\n1. $\\operatorname{Var}(X \\mid A)=\\mathbb{E}\\left[(X-\\mathbb{E}[X \\mid A])^{2} \\mid A\\right]=\\sum_{x}(x-\\mathbb{E}[X \\mid A])^{2} \\cdot P_{X \\mid A}(x)$\n\n   $X|A$ is random variable of $X$\n\n   Alternatively, $\\operatorname{Var}(X \\mid A)=\\mathbb{E}\\left[X^{2} \\mid A\\right]-\\mathbb{E}^{2}[X \\mid A]$\n\n   Similarly, $\\operatorname{Var}(X \\mid Y=y)=\\operatorname{Var}(X \\mid\\{Y=y\\})=\\mathbb{E}\\left[(X-\\mathbb{E}[X \\mid Y=y])^{2} \\mid Y=y\\right]$\n\n2. $\\operatorname{Var}(X \\mid Y)(y)=\\operatorname{Var}(X \\mid Y=y)$\n\n   $X|Y$ is a random variable of $Y$ thus a function of $Y$ \n\n   $\\operatorname{Var}(X \\mid Y)=\\mathbb{E}\\left[X^{2} \\mid Y\\right]-\\mathbb{E}^{2}[X \\mid Y]$\n\n### Law of total variance ***\n\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]+\\operatorname{Var}(\\mathbb{E}[X \\mid Y])\n$$\n\n- why we need the second item?\n\n  consider $Var(X|X)$ , it is a function of $X$ , so $Var(X|X)_(x)=Var(X|X=x)=Var(X=x)=0$, for any $x$ the function give the zero output, so we say $Var(X|X)=0$ is a function of constant, the $E[Var(X)]=0$] obviously, if we don't have the second item the equation above will not make sense.\n\n  ---\n\n  **PROVE**\n  $$\n  \\begin{aligned}\n  \\mathbb{E}[\\operatorname{Var}(X \\mid Y)] &=\\mathbb{E}\\left[\\mathbb{E}\\left[X^{2} \\mid Y\\right]-\\mathbb{E}^{2}[X \\mid Y]\\right] \\\\\n  &=\\mathbb{E}\\left[\\mathbb{E}\\left[X^{2} \\mid Y\\right]\\right]-\\mathbb{E}\\left[\\mathbb{E}^{2}[X \\mid Y]\\right] \\\\\n  &=\\mathbb{E}\\left[X^{2}\\right]-\\left(\\operatorname{Var}(Z)+\\mathbb{E}^{2}[Z]\\right) \\\\\n  &=\\mathbb{E}\\left[X^{2}\\right]-(\\mathbb{E}[\\mathbb{E}[X \\mid Y]])^{2}-\\operatorname{Var}(Z) \\\\\n  &=\\mathbb{E}\\left[X^{2}\\right]-(\\mathbb{E}[X])^{2}-\\operatorname{Var}(Z)\n  \\end{aligned}\n  $$\n  Thus, $\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]+\\operatorname{Var}(\\mathbb{E}[X \\mid Y])=\\operatorname{Var}(X)$\n\n---\n\n- The **first step** we just expand the variance and **the second** we use the Linearity of expectation.\n- Then to be concise, let $Z$ denote $X|Y$,  the first item **in third step** become $E[X^2]$ because the property of **smoothing**.\n- using smoothing again we get fourth step\n- then let $Var(X)$ substitute the first two items in **step 5**, we then get all we want. \n\n--------\n\n### Small practice\n\n- We have two bins {1; 2} and each bin has three types of balls {0; 2; 4}: A bin is\n\nrandomly selected â€€first and then a ball is drawn from the bin. The fraction of each\n\ntype of balls in each bin is shown in the following table.\n\n| -    | 0    | 2    | 4    |\n| ---- | ---- | ---- | ---- |\n| Bin1 | 0.6  | 0.3  | 0.1  |\n| Bin2 | 0.1  | 0.3  | 0.6  |\n\nLet X denote the type of the ball selected. Calculate Var (X)\n\n-----\n\n**analysis**: we can solve it by calculating the pmf of $X$, but we should use the marginal distribution to calculate it from the distribution of joint $X,Y$, it will not be trivial since this is just a discrete case. \n\nbut the purpose of this small practice is to use **LOTV**, thus string all the concept together. \n\n---\n\n**Solution**:\n\nSo let us first expand $Var(X)$ by **LOTV**:\n$$\nVar(X)=\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]+\\operatorname{Var}(\\mathbb{E}[X \\mid Y]) \\tag{1}\n$$\nNote that $E[X|Y]$  and $Var[X|Y]$ are both **function of Y** thus both **random variables**. and to be convise we denote them $Z1~~Z2$ respectively. \n$$\nVar(X)=\\mathbb{E}[\\operatorname{Z2}]+\\operatorname{Var}(\\mathbb{E}[Z1]) \\tag{2}\n$$\nWhen calculate the **expectation and variance** of a random variable we should know its all valid **real value and correspondent pmf**. \n\n---\n\nfor $Z1$, since its function of Y and Y is a discrete  R.V.  with only two valid value hence we can get two \n\n**conditional expectation**\n$$\n\\begin{aligned}\n&\\{Z 1=1\\}=\\{E[X \\mid Y=1]\\}=\\sum_{x \\in v a l(x)} x * p(x \\mid y=1)=1 \\\\\n&\\ldots \\quad p(Z 1=1)=\\sum_{x} p(x \\mid y=1)=p(Y=1) \\\\\n&\\{Z 1=2\\}=\\{E[X \\mid Y=2]\\}=\\sum_{x \\in v a l(x)} x * p(x \\mid y=2)=3 \\\\\n&\\cdots \\quad p(Z 1=2)=\\sum_{x} p(x \\mid y=2)=p(Y=2)\n\\end{aligned}\n$$\n**Note** $X|Y=1$ is function of X, when we calculate its expectation, actually we are just calculating a conditional expectation (**which just change the pmf to conditional pmf given conditions**) .\n\nsimilarly, we have **conditional variance**\n\n$$\n\\begin{gathered}\n\\{Z 2=1\\}=\\operatorname{Var}[X \\mid Y=1]=\\sum_{x}(x-E[X \\mid Y=1])^{2} p(x \\mid y=1) \\\\\n=(0-1)^{2} * 0.6+(2-1)^{2} * 0.3+(4-1)^{2} * 0.1=1.8 \\\\\n\\cdots \\quad p(Z 2=1)=P(Y=1)=1 / 2 \\\\\n\\{Z 2=2\\}=\\operatorname{Var}[X \\mid Y=2]=\\sum_{x}(x-E[X \\mid Y=2])^{2} p(x \\mid y=1) \\\\\n=(0-3)^{2} * 0.6+(2-3)^{2} * 0.3+(4-3)^{2} * 0.1=1.8 \\\\\n \\cdots \\quad p(Z 2=2)=P(Y=2)=1 / 2\n\\end{gathered}\n$$\n\nSo we have get all the information need to calculate the **expectation** of R.V. $Z2$ and **variance** of R.V. $Z1$. \n\nthe former is $E[Z2]=\\sum_{z_{2}}z_{2}*P(z_{2})=1.8$ and the latter is $\\operatorname{Var}[Z 1]=\\sum_{z_{1}}\\left(z_{1}-E[Z 1]\\right)^{2} * p\\left(z_{1}\\right)=1$ where $E(Z1)=2$, so we get the answer **2.8**\n\n\n\n**REMARK**:  when using **LOTV**, we should firstly specify  **two R.Vs** and calculating all their **real value-pdf** when calculate their value, note that we are calculate the **conditional expectation** or conditional variance of another random variable i.e. the $X$ in $X|Y$ which result a real value. and the pmf\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"Conditional PMF&Expectation","url":"/2021/10/01/EECS501-Notes1/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n\n## Conditional PMF&Expectation\n### LECTURE 6-7 Discrete Random Variables\n\n\nFor the **conditional PMF** we have\n$$\nP_{X \\mid Y}(x \\mid y)=P(\\{X=x\\} \\mid Y=y)=\\frac{P(\\{X=x\\} \\cap\\{Y=y\\})}{P(\\{Y=y\\})}=\\frac{P_{X Y}(x y)}{P_{Y}(y)}\n$$\nAs for the **conditional Expectation**, we first talk about the case \n$$\n\\mathbb{E}[X \\mid A]=\\sum_{x} x P_{X \\mid A}(x)\n$$\n$X|A$ can be regarded as a **R.V. conditioned on event A**(a bunch of sets from sample space), the difference between $E[X] ~and~E[X|A]$  is the latter changed the **PMF $P_X~to~P_{X|A}$** when calculating.\n\nThen we elaborate on the case\n$$\ng(y)=\\mathbb{E}[X \\mid Y](y \\mid)=\\mathbb{E}[X \\mid Y=y]\n$$\nwhere **event A** is substituted by the **R.V.** $Y$ , note that in this circumstance the expectation should be a **function of Y** which means **function of function** mapping the event $\\set{Y=y_i}$ to real numbers, and hence it is also a R.V.\n\n\n\n### Independence of a R.V. from a event\n\n####  independence of a R.V. from a event\n\n$$\nP_{X \\mid A}(x)=P_{X}(x) \\quad \\forall x\n$$\n\n####  independence of two R.V. \n\n$$\n\\begin{gathered}\nP_{X Y}(x, y)=P_{X}(x) P_{Y}(y) \\quad \\forall x, y \\\\\n\\Rightarrow P_{X \\mid Y}(x \\mid y)=P_{X}(x)\n\\end{gathered}\n$$\n\n#### independence of several R.V. \n\n$$\nP_{X Y Z}(x, y, z)=P_{X}(x) P_{Y}(y) P_{Z}(z), \\forall x, y, z\n$$\n\nNote that it seems different form the **definition** of **independent of three events**, but actually they are **essentially equivalent.** \n\nReason is we can derive the following form in three events just by applying the marginal distribution. \n$$\n\\begin{aligned}\nP_{X Y}(x, y) &=\\sum_{z} P_{X Y}(x, y, z) \\\\\n&=\\sum_{z} P_{X}(x) P_{Y}(y) P_{Z}(z)=P_{X}(x) P_{Y}(y)\n\\end{aligned}\n$$\n\n\n### LOTE: Law of total expectation\n\nEnsure that we have the probability space: $(\\Omega, \\mathcal{F}, P)$ with $B_1,B_2...B_n$ be a **partition** of the $\\Omega$. Then let $X$ be a R.V. on $\\Omega$ with PMF $P_X$, Then we can have \n$$\n\\mathbb{E}[X]=\\sum_{i=1}^{n} P\\left(B_{i}\\right) \\mathbb{E}\\left[X \\mid B_{i}\\right]\n$$\n\n### Some important property of expectation of R.V.\n\n#### 1. Smoothing/law of iterated expectation $\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\mathbb{E}[Y]$\n\nWhen we are asked to calculate $E[Y]$ but we find it a little bit difficult, we can try to **calculate left hand side equation instead**\n\n**emphasis again**: $E[Y|X]$ is function of R.V. $X$ , so when calculating the $E[E[Y|X]]$, we need to know all the potential value of R.V. $Z=g(X)$ also the corresponding probability value of $X$\n\n e.g. $\\mathbb{E}[Y \\mid X=1]=\\sum_{y} x P_{Y \\mid X}(y \\mid 1)$ is one of the value of R.V. and its corresponding pmf is $P(X=1)$\n\n#### 2. $\\mathbb{E}[h(X) \\mid X]=h(X)$\n\nwe can just remember this equation by: when we know $X$ then $h(X)$ become real value and the expectation of constant is itself.\n\n#### 3. Substitution $\\mathbb{E}[g(X, Y) \\mid X=x] \\mid=\\mathbb{E}[g(x, Y) \\mid X=x]$\n\n**PROVEï¼š**\n$$\n\\begin{gathered}\n\\mathbb{E}[Z \\mid X=x]=\\sum_{y} \\mathbb{E}[Z \\mid X=x, Y=y] P(Y=y \\mid X=x) \\\\\n=\\sum_{y} g(x, y) P(Y=y \\mid X=x)\\\\ \\text{[LOTE]} \\\\\n\n\\mathbb{E}\\left[Z_{x} \\mid X=x\\right]=\\sum_{y} g(x, y) P(Y=y \\mid X=x)\\\\\n\\text{[LOTUS]}\n\\end{gathered}\n$$\n#### 4. $\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X]$\n\n\n\n#### 5. Towering $\\mathbb{E}[\\mathbb{E}[X \\mid Y, Z] \\mid Z]=\\mathbb{E}[X \\mid Z]$\n\nA little trivial, but just remember two things:\n\n1. $\\mathbb{E}_{X \\mid Y, Z}[X \\mid Y, Z]$ is a function of two random variables Y and Z\n\n2. $\\mathbb{E}_{Y \\mid Z}\\left[\\mathbb{E}_{X \\mid Y, Z}[X \\mid Y, Z] \\mid Z\\right]=\\mathbb{E}_{Y \\mid Z}[g(Y, Z) \\mid Z]$ is a function of Z\n\n---\n\n**My intuition: when we calculate the expectation of function of multiple r.v. it can work as flatten which have the same sense when we reduce dimention in matrix** \n\n","tags":["Math","Probability and Random Processes","Conditional PMF&Expectation"],"categories":["Math","Probability and Random Processes"]},{"title":"èµ«å°”æ›¼Â·é»‘å¡ã€Šé›¾ä¸­ã€‹","url":"/2021/08/07/é›¾ä¸­/","content":"\n<img src=\"/images/fog.jpg\" width=\"500\">\n\n## é›¾ä¸­\n\nåœ¨é›¾ä¸­æ•£æ­¥çœŸæ˜¯å¥‡å¦™ï¼\n\nä¸€æœ¨ä¸€çŸ³éƒ½å¾ˆå­¤ç‹¬ï¼Œ\n\næ²¡æœ‰ä¸€æ£µæ ‘çœ‹åˆ°åˆ«æ£µæ ‘ï¼Œ\n\næ£µæ£µéƒ½å¾ˆå­¤ç‹¬ã€‚\n\nå½“æˆ‘ç”Ÿæ´»å¾—å¼€æœ—ä¹‹æ—¶ï¼Œ\n\næˆ‘åœ¨ä¸–ä¸Šæœ‰å¾ˆå¤šå‹äººï¼›\n\nå¦‚ä»Šï¼Œ\n\nç”±äºå¤§é›¾å¼¥æ¼«ï¼Œ\n\nå†ä¹Ÿçœ‹ä¸åˆ°ä»»ä½•äººã€‚\n\nç¡®å®ï¼Œ\n\nä¸è®¤è¯†é»‘æš—çš„äººï¼Œ\n\nå†³ä¸èƒ½ç§°ä¸ºæ˜æ™ºä¹‹å£«ï¼Œ\n\néš¾æ‘†è„±çš„é»‘æš—æ‚„æ‚„åœ°\n\næŠŠä»–è·Ÿä¸€åˆ‡äººéš”ç¦»ã€‚\n\nåœ¨é›¾ä¸­æ•£æ­¥çœŸæ˜¯å¥‡å¦™ï¼\n\näººç”Ÿå°±æ˜¯å­‘ç„¶å­¤ç‹¬çš„æ ·å­ã€‚\n\nç‹¬å¤„ã€‚\n\næ²¡æœ‰ä¸€ä¸ªäººäº†è§£åˆ«äººï¼Œ\n\näººäººéƒ½å¾ˆå­¤ç‹¬ã€‚\n\n### Im Nebel\n\nSeltsam, im Nebel zu wandern!\nEinsam ist jeder Busch und Stein,\nKein Baum sieht den andern,\nJeder ist allein.\n\nVoll von Freunden war mir die Welt,\nAls noch mein Leben licht war;\nNun, da der Nebel fÃ¤llt,\nIst keiner mehr sichtbar.\n\nWahrlich, keiner ist weise,\nDer nicht das Dunkel kennt,\nDas unentrinnbar und leise\nVon allen ihn trennt.\n\nSeltsam, im Nebel zu wandern!\nLeben ist Einsamsein.\nKein Mensch kennt den andern,\nJeder ist allein.\n\n---\n\n{% meting \"1859874\" \"netease\" \"song\" \"autoplay\"%}\n\n\n\n","tags":["Life","Poem"],"categories":["Life","Poem"]},{"title":"çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆå››ï¼‰","url":"/2021/07/30/çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆå››ï¼‰/","content":"\n<!-- more -->\n\n**MIT 18.06 P7-8**\n\n7. Solving Ax = 0: Pivot Variables, Special Solutions\n\n8.  Solving Ax = b: Row Reduced Form R\n\n{% asset_img image.png %}\n\n> p6ä¸­ä»‹ç»äº†ä¸¤ç§æ„æˆçŸ©é˜µAå­ç©ºé—´çš„æ–¹æ³•ï¼Œp7,p8åˆ™ä»‹ç»äº†å…·ä½“æ±‚è§£Ax=0,Ax=bçš„æ–¹æ³•\n\n###  æ±‚è§£Ax=0ï¼Œä¸»å˜é‡ï¼Œç‰¹è§£\n\n>æ˜ç¡®ä¸»å˜é‡å’Œè‡ªç”±å˜é‡çš„æ¦‚å¿µåï¼Œå¯ä»¥å®šä¹‰ä¸»åˆ—å’Œè‡ªç”±åˆ—ï¼Œå¯¹å…¶æ“ä½œå¯ä»¥å¾—åˆ°Ax=0çš„ç‰¹è§£ï¼Œç‰¹è§£çš„linear combinationå¯ä»¥å¾—åˆ°null spaceï¼Œreduced row  echelon formåˆ™å¯ä»¥è¿›ä¸€æ­¥å½¢å¼åŒ–è§£çš„è¡¨å¾ã€‚\n\n$$\nGiven \\qquad A=\\left[\\begin{array}{cccc}\n1 & 2 & 2 & 2 \\\\\n2 & 4 & 6 & 8 \\\\\n3 & 6 & 8 & 10\n\\end{array}\\right]\n$$\n\n#### AX=0çš„ç‰¹è§£\n\n$$\n\\mathrm{A}=\\left[\\begin{array}{rrrr}\n1 & 2 & 2 & 2 \\\\\n2 & 4 & 6 & 8 \\\\\n3 & 6 & 8 & 10\n\\end{array}\\right] \\stackrel{\\text {elimination }}\\longrightarrow\\left[\\begin{array}{llll}\n\\underline{1} & 2 & 2 & 2 \\\\\n0 & 0 & \\underline{2} & 4 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]=\\mathrm{U}\n$$\n\næ¶ˆå…ƒå®Œæˆåï¼Œå¾—åˆ°ä¸¤ä¸ª**ä¸»å˜é‡ï¼ˆpivot variable, ç”¨ä¸‹åˆ’çº¿æ ‡å‡ºï¼‰**,ä»£è¡¨**çŸ©é˜µAçš„ç§©(rank)**ä¸º2, å³r=2. \n\nä¸»å˜é‡æ‰€åœ¨çš„åˆ—ä¸º**ä¸»åˆ—(pivot column)**,å…¶ä½™åˆ—ä¸º**è‡ªç”±åˆ—(free column)**. è‡ªç”±åˆ—ä¸­çš„å˜é‡ä¸º**è‡ªç”±å˜é‡(free variable)**. å¯¹äºä¸€ä¸ªm*nçš„çŸ©é˜µ, **è‡ªç”±å˜é‡çš„ä¸ªæ•°ä¸ºn-r**,å¯¹äºçŸ©é˜µAä¸º4-2=2. \n\næ±‚è§£æ—¶ï¼Œå¸¸èµ‹å€¼ç»™è‡ªç”±åˆ—ï¼Œéšåæ±‚è§£ä¸»å˜é‡çš„å€¼ã€‚æ±‚ç‰¹è§£æ—¶å°†å…¶ä¸­ä¸€ä¸ªè‡ªç”±åˆ—èµ‹å€¼ä¸º1å…¶ä»–ä¸º0ï¼Œéšåé€šè¿‡back substitutionæ±‚è§£æ–¹ç¨‹ç»„, ä»¥æ­¤éå†æ‰€æœ‰è‡ªç”±åˆ—ï¼Œ**å¾—åˆ°n-rä¸ªç‰¹è§£**ã€‚\n\nä¾‹å¦‚ï¼š\n$$\nUx=0\\rightarrow\\left\\{\\begin{array}{ll}\n x_{1}&+2x_{2}&+2x_{3} &+2x_{4}   =0 \\\\\n \t\t\t&\t\t\t\t&+2x_{3} &+4x_{4}   =0 \\\\\n\n\\end{array}\\right.\n$$\nè§‚å¯Ÿæ¶ˆå…ƒçŸ©é˜µUå¯ä»¥å¾—åˆ°col2,col4ä¸ºè‡ªç”±åˆ—ï¼Œå› æ­¤$x_{2},  x_{4}$â€‹â€‹â€‹â€‹ä¸ºè‡ªç”±å˜é‡ï¼Œä»¤$x_{2}=1,  x_{4}=0$â€‹â€‹â€‹â€‹,æˆ‘ä»¬æœ‰**ç‰¹è§£1**=$[-2~~1~~0~~0]^T$â€‹â€‹â€‹â€‹ , ä»¤$x_{2}=0,  x_{4}=1$â€‹â€‹â€‹â€‹æˆ‘ä»¬æœ‰**ç‰¹è§£2**=$[2~~0~-2~~1]^T$â€‹â€‹â€‹â€‹â€‹â€‹â€‹\n\n\n\n(å› ä¸ºrow3æ˜¯row1å’Œrow2çš„linear combination, æ‰€æœ‰æ¶ˆå…ƒçš„è¿‡ç¨‹ä¸­row3å˜æˆäº†zero vector, åœ¨back substitutionçš„è¿‡ç¨‹ä¸­ä½“ç°ä¸ºå¯ä»¥çœå»æ±‚è§£å†—ä½™çš„æ–¹ç¨‹å¼)\n\n#### ç‰¹è§£æ„æˆé›¶ç©ºé—´null space\n\nä¸ºäº†æ›´é«˜æ•ˆçš„æ±‚è§£null space, è¿™é‡Œåº”ç”¨åˆ°äº†ç‰¹è§£ï¼Œå› ä¸ºç‰¹è§£æ„æˆçš„æ–¹å¼å†³å®šäº†å®ƒä»¬éƒ½æ˜¯independentçš„, æ‰€ä»¥å®ƒä»¬çš„linear combinationå¯ä»¥é«˜æ•ˆçš„è§£å¾—null space.å› æ­¤ç»“åˆä¸Šä¾‹å³\n$$\na\\left[\\begin{array}{c}\n2 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n\\end{array}\\right]+b\n\\left[\\begin{array}{c}\n2 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{array}\\right]\n$$\n\n\n#### è§„èŒƒçš„é˜¶æ¢¯å½¢çŸ©é˜µ\n\n$$\n\\mathrm{U}=\\left[\\begin{array}{llll}\n\\underline{1} & 2 & 2 & 2 \\\\\n0 & 0 & \\underline{2} & 4 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\longrightarrow\\left[\\begin{array}{cccc}\n\\underline{1} & 2 & 0 & -2 \\\\\n0 & 0 & \\underline{1} & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]=\\mathrm{R}\n$$\n\nå°†çŸ©é˜µUåŒ–ç®€ä¸º**RçŸ©é˜µ(Reduced row echelon form)**ï¼Œä¸»å…ƒä¸º1å…¶ä¸Šä¸‹çš„å…ƒç´ ä¸º0. å¯¹çŸ©é˜µRè¿›è¡Œåˆ—äº¤æ¢å¾—åˆ°ä»¥ä¸‹çš„å½¢å¼\n$$\n\\mathrm{R}=\\left[\\begin{array}{cccc}\n\\underline{1} & 2 & 0 & -2 \\\\\n0 & 0 & \\underline{1} & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\stackrel{\\text {col exchange}}\\longrightarrow\n\\left[\\begin{array}{cc|cc}\n1 & 0 & 2 & -2 \\\\\n0 & 1 & 0 & 2 \\\\\n\\hline 0 & 0 & 0 & 0\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\mathrm{I} & \\mathrm{F} \\\\\n0 & 0\n\\end{array}\\right]=R_{col}\n$$\n**I(identity)ä¸ºå•ä½çŸ©é˜µ**ç”±ç®€åŒ–åpivot columnç»„æˆï¼Œ**F(free)ä¸ºfree columnç»„æˆçš„çŸ©é˜µ**. æˆ‘ä»¬è¦è®¡ç®—\n\néšåè®¡ç®—$R_{col}N=0$â€‹â€‹â€‹â€‹ï¼Œå¯ä»¥å¾—åˆ°\n$$\nN=\\begin{bmatrix}-F\\\\I\\end{bmatrix}=\\begin{bmatrix}-2&2\\\\0&-2\\\\1&0\\\\0&1\\end{bmatrix} \\quad \\text{where order of x component is }\\begin{bmatrix}x_{1}\\\\x_{3}\\\\x_{2}\\\\x_{4}\\end{bmatrix}\n$$\n**P.S.** è¿™é‡Œæ•™æˆè®²çš„æ—¶å€™ä¸€å¼€å§‹ç»•çš„å¾ˆæ™•ï¼Œçªç„¶æå‡ºä¸€ä¸ªRåˆçªç„¶åˆ†å—çŸ©é˜µï¼Œåæ¥æƒ³æ˜ç™½è¿™é‡Œç›¸å½“äºæ˜¯ä¸€ç§ä¼˜é›…çš„è®¡ç®—æ–¹å¼ï¼Œæœ€åå¯ä»¥ä»¥ç®€æ´çš„å½¢å¼è¡¨å¾æ±‚å¾—çš„è§£ï¼Œå®ƒæŠŠåŸæœ¬éœ€è¦é€šè¿‡back substitutionæ¥å®Œæˆçš„æ“ä½œï¼ŒæŠ½è±¡åˆ°matrixè¿ç®—ä¸­ï¼Œæ­¤å¤–æ•™æˆæåˆ°MATLABä¸­ç‰¹è§£ä¹Ÿæ˜¯è¿™ç§æ–¹æ³•æ±‚çš„ï¼Œé‚£ä¹ˆæˆ‘ä¼°æ‘¸ç€è¿™ç§æ–¹æ³•åº”è¯¥ä¹Ÿèƒ½èŠ‚çœè®¡ç®—å¤æ‚åº¦ã€‚ï¼ˆè¿˜æœ‰æ¯”è¾ƒå®¹æ˜“æ··æ·†çš„åœ°æ–¹æ˜¯ï¼Œè¿™è¾¹æ•™æˆè¯´è¿™é‡Œçš„å‡ ä¸ªçŸ©é˜µå˜åŒ–ä¸ä¼šæ”¹å˜ç‰¹è§£ï¼Œä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯col exchangeä»¥åï¼Œå¯¹åº”xå‘é‡ä¸­componentæ˜¯è¦è·Ÿç€å˜æ¢ä½ç½®çš„ï¼Œæ‰€ä»¥æ±‚å¾—çš„næ˜¯æ›´æ¢äº†componentä½ç½®çš„) . BTW col exchangeæ˜¯è¦è®©free columnå¹¶åœ¨ä¸€èµ·å½¢æˆFï¼Œæ–¹ä¾¿æŠŠassignçš„æ“ä½œç”¨å•ä½çŸ©é˜µæ›¿æ‰ï¼ŒæŠŠé—®é¢˜ç®€åŒ–æˆ$I*?+F*I=0$â€‹â€‹â€‹, è¿™é‡Œä¹Ÿèƒ½ä½“ç°å‡ºä¹‹å‰æ±‚Rçš„æ„ä¹‰ï¼Œç›¸å½“äºæå‰æŠŠç­”æ¡ˆå½’ä¸€åŒ–â€”â€”å¦™å°±å®Œäº‹äº†ã€‚\n\n----\n\n### æ±‚è§£Ax=b å¯è§£æ€§ï¼Œè§£çš„ç»“æ„\n\n>åœ¨Ax=0è§£çš„åŸºç¡€ä¸Šï¼Œå¯ä»¥ç»§ç»­æ¢è®¨äº†è§£Ax=bçš„å¯è§£æ€§ï¼Œä»¥åŠè§£çš„ç»“æ„ã€‚\n\n#### Ax=bçš„å¯è§£æ€§\n\n$$\nGiven~3\\times4~matrix \\qquad A=\\left[\\begin{array}{cccc}\n1 & 2 & 2 & 2 \\\\\n2 & 4 & 6 & 8 \\\\\n3 & 6 & 8 & 10\n\\end{array}\\right] \\\\assume ~~~~~~~~~b= \\begin{bmatrix}1\\\\5\\\\6\\end{bmatrix}\n$$\n\næ±‚Ax=bçš„ç‰¹è§£ï¼Œå¯å…ˆå†™å‡ºå…¶å¢å¹¿çŸ©é˜µ[A|b]çš„å½¢å¼ï¼Œå¹¶è¿›è¡Œæ¶ˆå…ƒ\n$$\n\\left[\\begin{array}{cccc|c}\n1 & 2 & 2 & 2 & \\mathrm{~b}_{1} \\\\\n2 & 4 & 6 & 8 & \\mathrm{~b}_{2} \\\\\n3 & 6 & 8 & 10 & \\mathrm{~b}_{3}\n\\end{array}\\right]\\stackrel{\\text{elimination}}\\longrightarrow \\left[\\begin{array}{cccc|c}\n1 & 2 & 2 & 2 & \\mathrm{~b}_{1} \\\\\n0 & 0 & 2 & 4 & \\mathrm{~b}_{2}-2 \\mathrm{~b}_{1} \\\\\n0 & 0 & 0 & 0 & \\mathrm{~b}_{3}-\\mathrm{b}_{2}-\\mathrm{b}_{1}\n\\end{array}\\right]\n$$\nåœ¨ä¹‹å‰å·²ç»æ˜ç¡®äº†å½“**$b\\in C(A)$â€‹(bå±äºAçš„åˆ—ç©ºé—´)**æ—¶,å…¶å¿…æœ‰è§£ã€‚\n\nè¿™é‡Œä»è¡Œçš„è§’åº¦æ¥çœ‹ï¼Œ0è¡Œæ˜¯ç”±å„è¡Œçš„linear combinationå¾—åˆ°, å› æ­¤æœ‰è§£çš„æ¡ä»¶(**solvability condition on b**)ç­‰ä»·äºå‘é‡bæ»¡è¶³$b_{3}-b_{2}-b_{1}=0$â€‹â€‹â€‹â€‹â€‹â€‹ã€‚\n\n**P.S. å®ƒä»¬æ˜¯Ax=bçš„å¿…è¦æ¡ä»¶, è¿™é‡Œæ•™æˆæ²¡æœ‰ç»†è®²ï¼Œè€Œæ˜¯æ¥ç€è®²è§£çš„ç»“æ„**\n\n#### Ax=bè§£çš„ç»“æ„\n\n##### Ax=bçš„ç‰¹è§£\n\nä»¤çŸ©é˜µAä¸­æ‰€æœ‰è‡ªç”±å˜é‡ä¸º0ï¼Œæ±‚è§£æ–¹ç¨‹å…³äºä¸»å˜é‡çš„è§£\n$$\n\\begin{cases} x_1 & + & 2x_3 & = & 1 \\\\ & & 2x_3 & = & 3 \\\\ \\end{cases}\n$$\nè§£å¾—\n$$\n\\begin{cases} x_1 & = & -2 \\\\ x_3 & = & \\frac{3}{2} \\\\ \\end{cases}â†’x_{p}=\\begin{bmatrix}-2\\\\0\\\\ \\frac{3}{2}\\\\0\\end{bmatrix}\n$$\n\n##### Ax=bçš„è§£é›†\n\n**Ax=bçš„è§£é›†ç”±å…¶ç‰¹è§£å’ŒAçš„null spaceç»„æˆ**ï¼ŒAçš„null spaceç”±Ax=0ç‰¹è§£çš„linear combinationç»„æˆ, åœ¨æœ¬æ–‡ç¬¬ä¸€é˜¶æ®µå·²ç»ç»™å‡ºã€‚å› æ­¤å¯¹äºçŸ©é˜µå®ä¾‹A, å…¶è§£é›†å¯ä»¥è¡¨ç¤ºä¸º\n$$\n\\mathrm{x}_{\\text {complete }}=\\left[\\begin{array}{c}\n-2 \\\\\n0 \\\\\n\\frac{3}{2} \\\\\n0\n\\end{array}\\right]+\\mathrm{c}_{1}\\left[\\begin{array}{c}\n-2 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{array}\\right]+\\mathrm{c}_{2}\\left[\\begin{array}{c}\n2 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{array}\\right]\n$$\nå³$X_c=X_p+nullspace(A)$â€‹â€‹\n\n---\n\nP.S. è¿™ä¸€éƒ¨åˆ†æ•™æˆä½œå›¾è®²äº†è¿™ä¸ª$X_c$â€‹â€‹,  å¾—åˆ°çš„è§£ç³»$X_{complete}$â€‹ç›¸å½“äºæŠŠåœ¨å››ç»´ç©ºé—´ä¸­çš„å­ç©ºé—´null spaceç§»åŠ¨åˆ°$X_{p}$â€‹(å‘é‡ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹)çš„ä½ç½®ã€‚æ˜¾ç„¶$X_{p}$ä¸ä¸º0å‘é‡, å› æ­¤$X_{c}$â€‹â€‹ä¸å†ç»è¿‡åŸç‚¹ï¼Œå› æ­¤å…¶ä¸ä¸ºsubspace, è¿™ä¹Ÿæ˜¯ä¹‹å‰æ•™æˆè®²Ax=bä¸­xæ‰€æ„æˆçš„ç©ºé—´ä¸æ„æˆå­ç©ºé—´çš„å…·ä½“åŸå› .\n\n##### çŸ©é˜µç§©ä¸Ax=bè§£ \n\n$$\n\\begin{array}{c|c|c|c}\n\\mathrm{r}=\\mathrm{m}=\\mathrm{n} & \\mathrm{r}=\\mathrm{n}<\\mathrm{m} & \\mathrm{r}=\\mathrm{m}<\\mathrm{n} & \\mathrm{r}<\\mathrm{m}, \\mathrm{r}<\\mathrm{n} \\\\\n\\mathrm{R}=\\mathrm{I} & \\mathrm{R}=\\left[\\begin{array}{l}\n\\mathrm{I} \\\\\n0\n\\end{array}\\right] & \\mathrm{R}=\\left[\\begin{array}{ll}\n\\mathrm{I} & \\mathrm{F}\n\\end{array}\\right] & \\mathrm{R}=\\left[\\begin{array}{cc}\n\\mathrm{I} & \\mathrm{F} \\\\\n0 & 0\n\\end{array}\\right] \\\\\n\\text { 1 solution } & 0 \\text { or } 1 \\text { solution } & \\infty \\text { solution } & 0 \\text { or } \\infty \\text { solution }\n\\end{array}\n$$\n\nå¯¹äºä»»æ„$m\\times n$â€‹â€‹â€‹â€‹çŸ©é˜µA, å…¶r(rank,ç§©)æ»¡è¶³$r\\leq min(m,n)$â€‹â€‹\n\n- å½“åˆ—æ»¡ç§©å³r=n<mæ—¶(case 2), æ²¡æœ‰free variable å› æ­¤nullspaceé‡Œåªæœ‰zero vector, é€šè§£å°±æ˜¯ç‰¹è§£åŠ ä¸Šzero vector, å› æ­¤å½“$b\\in  C(A)$â€‹æ—¶æœ‰å”¯ä¸€è§£ï¼ˆæˆ–è€…è¯´æ¶ˆå…ƒåå› ä¸ºæœ‰zero rowçš„å­˜åœ¨éœ€è¦combination of rowå¯¹åº”çš„bçš„å¼å­=0ï¼‰, ä¹Ÿå³è§£å¾—çš„$X_{p}$â€‹, å¦åˆ™æ— è§£ã€‚\n\n- å½“è¡Œæ»¡ç§©å³r=m<næ—¶(case 3), æœ‰n-rä¸ªfree variable , å› æ­¤æœ‰n-rä¸ªç‰¹è§£ï¼Œæ„æˆnullspaceåŠ ä¸Šç‰¹è§£å› æ­¤æœ‰æ— æ•°ä¸ªã€‚\n\n  P.S. è¿™é‡Œå› ä¸ºæ²¡æœ‰zero row æ‰€ä»¥bæ²¡æœ‰ä»»ä½•é™åˆ¶æ‰€ä»¥ä¸€å®šæœ‰è§£ï¼Œæˆ‘çš„ç†è§£æ˜¯å› ä¸ºcol rank=m, å› æ­¤$C(A)=R^m$, è€Œbä¸€å®šæ˜¯å±äº$R^m$çš„ï¼Œæ‰€ä»¥åŸæœ¬çš„å‰æåœ¨è¿™ä¸ªæƒ…å†µä¸‹ä¸€å®šæˆç«‹ã€‚\n\n- å½“è¡Œåˆ—æ»¡ç§©æ—¶r=m=n(case1), ç»“åˆè€ƒè™‘case2å’Œcase3, å› ä¸ºæ²¡æœ‰free variableæ‰€ä»¥nullspace é‡Œåªæœ‰zero vectorï¼Œé€šè§£å³Ax=bçš„ç‰¹è§£ï¼Œåˆå› ä¸ºr=måˆ™$b \\in C(A)$â€‹â€‹, æ‰€ä»¥ä¸€å®šæœ‰è§£ã€‚å› æ­¤åªæœ‰å”¯ä¸€è§£ã€‚\n\n- å½“è¡Œåˆ—å‡ä¸æ»¡ç§©æ—¶r<m<næ—¶, å…¶å½“$b \\in C(A)$ æ—¶ç­‰ä»·äºcase3çš„æƒ…å†µï¼Œè€Œå½“$b \\in C(A)$â€‹æ—¶åˆ™æ— è§£ã€‚\n\n","tags":["Math","Linear Algebra"],"categories":["Math","Linear Algebra"]},{"title":"çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆä¸‰ï¼‰","url":"/2021/07/13/çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆä¸‰ï¼‰/","content":"\n<!-- more -->\n\n**MIT 18.06 P5-6**\n\n5. Transposes, Permutations, Spaces R^n\n6. Column Space and Nullspace\n\n----\n\n## -è½¬ç½®, ç½®æ¢, ç©ºé—´\n\n### ç½®æ¢çŸ©é˜µ\n\nä¸ŠèŠ‚è¯¾è®²åˆ°çš„LUåˆ†è§£æœ‰è¦æ±‚åœ¨æ¶ˆå…ƒè¿‡ç¨‹ä¸­æ˜¯ä¸èƒ½å‡ºç°è¡Œäº¤æ¢çš„ã€‚å› æ­¤æ•™æˆç»™å‡º$PA=LU$æ¥è¡¨ç¤ºåŒ…å«è¡Œäº¤æ¢çš„æ¶ˆå…ƒè¿‡ç¨‹ã€‚å…¶ä¸­Pç”¨æ¥äº¤æ¢è¡Œçš„ä½ç½®ä»¥é¿å…å‡ºä¸»å…ƒ(pivot)ä½ç½®ä¸º0çš„æƒ…å†µã€‚\n\næ›´å…·ä½“åœ°ï¼ŒPä»£è¡¨**ç½®æ¢çŸ©é˜µ(Permutation Matrix)**, å®ƒå¯ä»¥ç”±å•ä½çŸ©é˜µè¿›è¡Œè¡Œé‡ç»„è€Œå¾—åˆ°ã€‚éœ€è¦æ³¨æ„çš„æ˜¯å•ä½çŸ©é˜µå±äºç½®æ¢çŸ©é˜µä½†æ˜¯å¹¶ä¸è¿›è¡Œä»»ä½•è¡Œäº¤æ¢ã€‚ä¸€ä¸ªné˜¶æ–¹é˜µæœ‰n!ç§å¯èƒ½çš„ç½®æ¢çŸ©é˜µã€‚\n\nå¹¶ä¸”$P$æœ‰ä¸€ä¸ªæ€§è´¨å³$P^{-1}=P^{T}$ï¼ˆå…¶å®å°±æ˜¯**æ­£äº¤çŸ©é˜µ**), å› æ­¤$P^{T}P=I$.\n\n### è½¬ç½®\n\n$\\begin{bmatrix}1&3\\\\2&3\\\\4&1\\end{bmatrix}^T=\\begin{bmatrix}1&2&4\\\\3&3&1\\end{bmatrix}$\n\nè½¬ç½®å³çŸ©é˜µä¸­æ‰€æœ‰è¡Œå‘é‡å˜æˆåˆ—å‘é‡ï¼Œå¯ä»¥ç”±å¼$(A^T)_{ij}=A_{ji}$è¡¨ç¤º\n\n### å¯¹ç§°çŸ©é˜µ\n\n**å¯¹ç§°çŸ©é˜µ(Symmetric Matrix)** ,å³$A^{T}=A$, è½¬ç½®åå¾—åˆ°çš„ä»ä¸ºåŸæ¥çš„çŸ©é˜µã€‚æ­¤å¤–ä»»ä½•çŸ©é˜µçš„è½¬ç½®ä¹˜ä¸Šè¯¥çŸ©é˜µå¾—åˆ°çš„å¿…å®šæ˜¯å¯¹ç§°çŸ©é˜µ, å› ä¸º$(P^{T}P)^T=P^TP\\\\$\n\n### å‘é‡ç©ºé—´\n\nå‘é‡ç©ºé—´(Vector Spaces)è¦æ»¡è¶³å…¶ä¸­çš„componentç»è¿‡ä¸€å®šçš„operation(æ ¹æ®å®šä¹‰çš„rules)ä»¥åè¿˜åœ¨åŸæ¥çš„å‘é‡ç©ºé—´ä¸­ï¼Œä¹Ÿå³éœ€è¦æ˜¯å°é—­çš„ã€‚ æ¯”å¦‚äºŒç»´å‘é‡ç©ºé—´$R^{2}$, å®ƒä»£è¡¨æ‰€æœ‰çš„äºŒç»´Real Vectorsçš„é›†åˆã€‚æ¯”å¦‚x-y planeçš„ç¬¬ä¸€è±¡é™å°±ä¸æ˜¯vector spaces, å› ä¸ºå…¶ä¸æ»¡è¶³**æ•°ä¹˜å°é—­**(ä¹˜è´Ÿçš„scalarï¼ŒåŒç†ä¸åŒ…æ‹¬åŸç‚¹çš„spaceä¹ŸåŒæ ·ä¸æ»¡è¶³é›¶ä¹˜å°é—­)\n\n### å­ç©ºé—´\n\nå­ç©ºé—´æ˜¯å‘é‡ç©ºé—´ä¸­çš„å‘é‡ç©ºé—´, æ¯”å¦‚$R^{2}$çš„å­ç©ºé—´(subspace)æœ‰1. $R^2$æœ¬èº« 2. æ‰€æœ‰ç»è¿‡[0,0]çš„ç›´çº¿ 3. $Z$é›¶å‘é‡ã€‚\n\n#### å¦‚ä½•åˆ›é€ å­ç©ºé—´\n\n$R^{n}$çš„å­ç©ºé—´å¯ä»¥ç”±ç©ºé—´ä¸­æŸä¸ªçŸ©é˜µå®ä¾‹Aæ‰€æœ‰åˆ—å‘é‡çš„Linear combinationsæ¥ç»„æˆï¼Œè¯¥ç©ºé—´è¢«ç§°ä¸ºColumn space, è®°ä½œ$C(A)$ã€‚ä½†æ˜¯å½¢æˆçš„ç©ºé—´$R^{m}$çš„må–å†³äºé‚£äº›åˆ—å‘é‡çš„æ€§è´¨, ä½†è‡³å°‘```m<=len(row(A))```ã€‚\n\n\n\n## -åˆ—ç©ºé—´å’Œé›¶ç©ºé—´\n\næ•™æˆç»§ç»­æ¥ç€å­ç©ºé—´çš„è¯é¢˜, $R^{3}$ä¸¤ä¸ªå­ç©ºé—´På’ŒLçš„å¹¶é›†$P\\cup L$ï¼ˆå¹³é¢å’Œçº¿ï¼‰å¹¶ä¸æ˜¯å­ç©ºé—´,å› ä¸ºå½“Pçš„å®ä¾‹åŠ ä¸ŠLçš„å®ä¾‹å¾—åˆ°çš„ç»“æœä¸åœ¨å¹¶é›†ä¸­ï¼Œä¹Ÿå³ä¸å°é—­ã€‚ä½†æ˜¯$P\\cap L$ åˆ™å±äº$R^{3}$å­ç©ºé—´ã€‚\n\n----\n\n$$\ngiven\\qquad A=\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}_{4 \\times3}\n$$\n\nè¿™é‡Œä¸‰ä¸ªåˆ—å‘é‡éƒ½æ˜¯çŸ©é˜µAæ‰€å±çš„$R^{4}$çš„å­ç©ºé—´, å®ƒçš„åˆ—ç©ºé—´ $C(A)$ç”±ä¸‰ä¸ªåˆ—å‘é‡æ‰€æœ‰çš„Linear combinationç»„æˆ,ã€‚è¿™é‡Œç”±äºåªæœ‰ä¸‰ä¸ªåˆ—å‘é‡ï¼Œå› æ­¤$C(A)$æ— æ³•å¾—åˆ°full space $R^{4}$ï¼ˆå…·ä½“åŸå› ä¸‹å°èŠ‚é˜è¿°ï¼‰ã€‚\n\nä¸ºäº†è¿›ä¸€æ­¥é˜è¿°åŸå› ï¼Œæ•™æˆé¦–å…ˆå°†é—®é¢˜æç‚¼ä¸ºåˆ—ç©ºé—´æ˜¯å¦å¯ä»¥è¦†ç›–åŸæ¥çš„å‘é‡ç©ºé—´$R^{4}$ ?(Does linear combination of C(A) build $R^{4}$  ), ä¸ºäº†è§£ç­”è¿™ä¸ªé—®é¢˜æ•™æˆç»“åˆå‡ èŠ‚è¯¾å‰çš„linear combination, å°†ä¸Šå¼å¯ä»¥æ”¹å†™ä¸º\n$$\n\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=\\begin{bmatrix}b_{1}\\\\b_{2}\\\\b_{3}\\\\b_{4}\\end{bmatrix}\n$$\näºæ˜¯é—®é¢˜å°±å¯ä»¥è½¬åŒ–ä¸ºAx=bæ˜¯å¦æ€»æœ‰å¯¹äºä»»æ„å››ç»´å‘é‡bçš„è§£?(Does Ax=b have a solution for every b)ã€‚ å¾ˆæ˜¾ç„¶å››ä¸ªæ–¹ç¨‹ä¸‰ä¸ªæœªçŸ¥æ•°ä¸ä¸€å®šæœ‰è§£ã€‚äºæ˜¯æ•™å¸ˆåˆå¼•ç”³å‡ºäº†ä¸¤ä¸ªæ–°çš„é—®é¢˜ï¼Œ\n\n1. æ€ä¹ˆæ ·çš„bå‘é‡å¯ä»¥æœ‰è§£?\n2. è¿™äº›bæ»¡è¶³ä»€ä¹ˆæ€§è´¨?\n\nå…³äº1ï¼Œæ˜¾ç„¶é›¶å‘é‡$\\begin{bmatrix}0& 0& 0\\end{bmatrix}^T$æ˜¾ç„¶æ˜¯ä¸€ä¸ªè§£, å› ä¸ºAx=0ä¸€å®šæœ‰è§£, å…¶æ¬¡bè‹¥æ˜¯ä¸‰ä¸ªåˆ—å‘é‡ä¸­çš„ä»»æ„ä¸€ä¸ªä¹Ÿä¸€å®šæœ‰è§£(è¿™ç‚¹ä»åˆ—å‘é‡çš„linear combinationæ¥ç†è§£å°±å¾ˆç›´è§‚, å°±æ˜¯01çš„æ‘†æ”¾)ï¼Œä»è¿™ç‚¹æ¨å¹¿å¼€æ¥ï¼Œåªè¦$b\\in C(A)$é‚£ä¹ˆå°±ä¸€å®šæœ‰è§£ï¼Œå› ä¸ºè¿™ä¸ªbå°±æ˜¯é€šè¿‡çº¿æ€§ç»„åˆå¾—åˆ°ï¼Œæ±‚è§£åªæ˜¯ä¸€ä¸ªåæ¨çš„è¿‡ç¨‹ã€‚\n\nå…³äº2ï¼Œtarget bæ»¡è¶³ $b\\in C(A)$\n\n---\n\nè€Œå¦‚æœç›´æ¥ä»çŸ©é˜µAçš„åˆ—å‘é‡å…¥æ‰‹ï¼Œæˆ‘ä»¬å‘ç°col1 col2 col3å¹¶ä¸ç‹¬ç«‹ï¼Œå®ƒä»¬çº¿æ€§ç›¸å…³ï¼Œcol1 å’Œ col2æ„æˆå¹³é¢, col3æ˜¯å¹³é¢å†…çš„ä¸€ä¸ªå‘é‡ï¼Œå› æ­¤$C(A)=R^{2}$.\n\n### é›¶ç©ºé—´\n\n\n$$\n\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}\n$$\n\nçŸ©é˜µAçš„é›¶ç©ºé—´(Null Space)å³æ‰€æœ‰æ»¡è¶³Ax=0ä¸­xç»„æˆçš„ç©ºé—´ï¼Œä¸Šå¼çš„ä¾‹å­ä¸­Açš„é›¶ç©ºé—´$N(A)$å±äº$R^{3}$. éšåæ•™æˆç®€å•è¯´æ˜äº†ä¸‹**é›¶ç©ºé—´ä¸€å®šæ„æˆå­ç©ºé—´**ï¼Œå¹¶ç»™å‡ºäº†ç®€å•ç›´è§‰ä¸Šçš„è¯æ˜ï¼š\n\nxçš„ä»»æ„ä¸¤ä¸ªå®ä¾‹ï¼Œ$Av=0$ $Aw=0$ ï¼Œæ˜¾ç„¶$A(v+w)=0$ ï¼ˆåˆ†é…ç‡ï¼‰, å› æ­¤æ»¡è¶³åŠ æ³•å°é—­, æ­¤å¤–æ˜¾ç„¶ä¹Ÿæ»¡è¶³æ•°ä¹˜åˆ†å¸ƒã€‚\n\næœ«å°¾æ•™æˆä¸ºäº†å¼ºè°ƒä¸ºä»€ä¹ˆå³ä¾§æ˜¯é›¶å‘é‡ä»¥åŠå…¶æ„ä¹‰,ç»™äº†å¦‚ä¸‹çš„ä¾‹å­\n\n$$\n\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\end{bmatrix}\n$$\n\n----\n\nè¿™æ—¶å€™æ˜¾ç„¶xå¹¶ä¸æ„æˆå­ç©ºé—´, å› ä¸ºé›¶å‘é‡[0 0 0]å¹¶ä¸åœ¨xæ„æˆçš„ç©ºé—´å†…ï¼Œè¿™è¿èƒŒäº†å‘é‡ç©ºé—´çš„å®šä¹‰ã€‚\n\n## æ€»ç»“\n\nè¿™ä¸¤èŠ‚æ•™æˆä¸»è¦ä»‹ç»äº†ä¸¤ç§æ„æˆçŸ©é˜µAçš„full spaceå­ç©ºé—´çš„æ–¹å¼ï¼š\n\n1. ä»Ax=bçš„Aå…¥æ‰‹, æ»¡è¶³Ax=bå§‹ç»ˆæœ‰è§£çš„bå‘é‡æ‰€æ„æˆçš„ç©ºé—´ï¼Œä¹Ÿå³AçŸ©é˜µåˆ—å‘é‡çš„linear combination(åˆ—ç©ºé—´, column space)ã€‚\n2. ä»Ax=bçš„bå…¥æ‰‹, ä»¤b=0, å³æ»¡è¶³Ax=0çš„xå‘é‡æ‰€æ„æˆçš„ç©ºé—´ã€‚\n\næœ¬è´¨ä¸Šï¼Œåœ¨æ»¡è¶³å‘é‡ç©ºé—´ç‰¹æ€§çš„è¦æ±‚ä»¥å¤–æœ€é‡è¦çš„ä¸€ç‚¹æ˜¯ä¸¤ç§æ–¹å¼éƒ½ç¡®ä¿äº†é›¶å‘é‡ä¸€å®šå­˜åœ¨äºè§£å¾—çš„ç©ºé—´ä¸­ã€‚\n\n","tags":["Math","Linear Algebra"],"categories":["Math","Linear Algebra"]},{"title":"çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆäºŒï¼‰","url":"/2021/07/06/çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆäºŒï¼‰/","content":"\n<!-- more -->\n\n**MIT 18.06 P3-4**\n\n3. Multiplication and Inverse Matrices \n\n4. Factorization into A = LU\n\n## çŸ©é˜µä¹˜æ³•å’Œé€†çŸ©é˜µ\n\n###  çŸ©é˜µä¹˜æ³•\n\næ•™æˆè®²è§£äº†å››ç§æ–¹æ³•æ¥çœ‹å¾…ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜ï¼Œå‡è®¾çŸ©é˜µA(m$\\times$p), çŸ©é˜µB(p$\\times$n),çŸ©é˜µè¿ç®—çš„å‰ææ˜¯å·¦çŸ©é˜µçš„è¡Œæ•°ç­‰äºå³çŸ©é˜µçš„åˆ—æ•°,å¾—åˆ°å·¦çŸ©é˜µè¡Œæ•°ä¹˜ä¸Šå³çŸ©é˜µåˆ—æ•°è§„æ¨¡çš„æ–°çŸ©é˜µC=A$\\times$B(m$\\times$n).\n\n#### 1-å…ƒç´ çš„è§’åº¦-è¡Œåˆ—å†…ç§¯\n\nè¡Œåˆ—å†…ç§¯æ˜ç¡®äº†CçŸ©é˜µä¸­æ¯ä¸ªå…ƒç´ (entry)çš„ç”±æ¥ï¼Œå¦‚å…¬å¼\n\n$$\nc_{ij}=row_{i}\\sdot column_{j}=\\sum_{k=1}^{p}e_{ik}e_{kj}\n$$\n\n#### 2-å‘é‡çº¿æ€§ç»„åˆè§’åº¦-è¡Œå‘é‡çš„è§’åº¦\n$$\n{\n\\begin{bmatrix} % matrix A\n.&.&&.\\\\\n.&.&&.\\\\\n\na_{k1}&a_{k2}&...&a_{kp}\\\\\n.&.&&.\\\\ \n.&.&&.\\\\ \n\\end{bmatrix} \n\\begin{bmatrix} % matrix B\nB_{row1}\\\\B_{row2}\\\\\n...\\\\\n...\\\\B_{rowp}\n\\\\\n\\end{bmatrix} =  % matrix C\n\\begin{bmatrix}\n...\\\\\n...\\\\\na_{k1}B_{row1}+a_{k2}B_{row2}+...a_{kp}B_{rowp}\n\\\\...\n\\\\\n...\n\\end{bmatrix}\n}\n$$\nå¦‚æœæˆ‘ä»¬ä»è¡Œçš„è§’åº¦æ¥çœ‹ï¼ŒæŠŠCçŸ©é˜µçœ‹ä½œæ•°ä¸ªè¡Œå‘é‡ï¼Œ**CçŸ©é˜µä¸­ä»»æ„ç¬¬kè¡Œå‘é‡å¯ä»¥çœ‹ä½œæ˜¯çŸ©é˜µBè¡Œå‘é‡å…³äºçŸ©é˜µAç¬¬iè¡Œå…ƒç´ çš„çº¿æ€§ç»„åˆ**ã€‚è¿™å¾ˆæœ‰ç”¨ï¼**æ¶ˆå…ƒè¿ç®—**ä¸­ï¼Œè¯¾å ‚çš„ä¾‹å­æ•™æˆéƒ½æ˜¯ä»¥è¡Œå‘é‡çš„çº¿æ€§ç»„åˆè§’åº¦æ¥å£ç®—å’Œé˜è¿°é—®é¢˜çš„ã€‚\n\n#### 3-å‘é‡çº¿æ€§ç»„åˆè§’åº¦-åˆ—å‘é‡çš„è§’åº¦\n$$\n{\n\\begin{bmatrix} % matrix A\nA_{col1}&\nA_{col2}&\n...&\nA_{colp}\n\\\\\n\\end{bmatrix}\n\\begin{bmatrix} % matrix B\n.&.&&b_{1k}&.&\\\\\n.&.&&b_{2k}&.\\\\\n\n.&.&...&.&.\\\\\n.&.&&.&.\\\\ \n.&.&&b_{pk}&.\\\\ \n\\end{bmatrix} \n\n =  % matrix C\n\n\\begin{bmatrix} % matrix A\n...&\n...&\nC_{colk}&\n...&\n...\n\\\\\n\\end{bmatrix}\n}\n\\\\\nwhere \\qquad C_{col_{k}}= b_{1k}A_{col1}+b_{2k}A_{col2}+\n...+\n...+b_{pk}A_{colp}\n$$\nå¦‚æœä»åˆ—çš„è§’åº¦çœ‹ï¼ŒCçŸ©é˜µçœ‹ä½œæ•°ä¸ªåˆ—å‘é‡ï¼Œ**CçŸ©é˜µä¸­ä»»æ„ç¬¬kåˆ—éƒ½å¯ä»¥çœ‹ä½œæ˜¯çŸ©é˜µAåˆ—å‘é‡å…³äºçŸ©é˜µBç¬¬kåˆ—å…ƒç´ çš„çº¿æ€§ç»„åˆ**ã€‚\n\n#### 4-**åˆ—å‘é‡ä¹˜è¡Œå‘é‡çš„è§’åº¦**\n$$\n\\begin{bmatrix} % matrix A\nA_{col1}&\nA_{col2}&\n...&\nA_{colp}\n\\\\\n\\end{bmatrix}\n\\begin{bmatrix} % matrix B\nB_{row1}\\\\B_{row2}\\\\\n...\\\\\n...\\\\B_{rowp}\n\\\\\n\\end{bmatrix}\n=A_{col1}B_{row1}+A_{col2}B_{row2}+...A_{colp}B_{rowp}\n$$\n**P.S.** è®²åˆ°è¿™é‡Œæ•™æˆæäº†ä¸‹$A_{coli}B_{rowi}$çš„å¤§å°ï¼Œ å³(m,1)$\\times$(1,n)â†’(m,n)ï¼Œè¿™ä¸æœ€ç»ˆå¾—åˆ°çš„çŸ©é˜µçš„å¤§å°æ˜¯ç›¸ç­‰çš„ï¼Œå› æ­¤æ›´å¤šè¡Œåˆ—å‘é‡ä¹˜çš„ç´¯åŠ è®©å¾—åˆ°çš„çŸ©é˜µæ›´åŠ é€¼è¿‘æœ€åçš„ç»“æœã€‚\n\n---\n\nç„¶åæ•™æˆåˆæåˆ°äº†åˆ†å—çŸ©é˜µï¼Œä¹‹å‰å¬è¯¾çš„æ—¶å€™è¿˜çº³é—·ä¸ºå•¥ä¸€ä¸‹å­æåˆ°åˆ†å—çŸ©é˜µï¼Œåæ¥æ•´ç†çš„æ—¶å€™å‘ç°ä¸Šé¢çš„è¡Œåˆ—ç›¸ä¹˜ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯åˆ†å—çŸ©é˜µçš„ä¸€ä¸ªç‰¹ä¾‹ã€‚\n\nåˆ†å—çŸ©é˜µä¹˜æ³•å¦‚ä¸‹ï¼š\n$$\n\\begin{array}{l}\n\\left[\\begin{array}{l|l} %vline\n\\mathrm{A}_{1} & \\mathrm{~A}_{2} \\\\\n\\hline \\mathrm{A}_{3} & \\mathrm{~A}_{4} %hline\n\\end{array}\\right]\\left[\\begin{array}{c|c}\n\\mathrm{B}_{1} & \\mathrm{~B}_{2} \\\\\n\\hline \\mathrm{B}_{3} & \\mathrm{~B}_{4}\n\\end{array}\\right]=\\left[\\begin{array}{c|c}\n\\mathrm{A}_{1} \\mathrm{~B}_{1}+\\mathrm{A}_{2} \\mathrm{~B}_{3} & \\mathrm{~A}_{1} \\mathrm{~B}_{2}+\\mathrm{A}_{2} \\mathrm{~B}_{4} \\\\\n\\hline \\mathrm{A}_{3} \\mathrm{~B}_{1}+\\mathrm{A}_{4} \\mathrm{~B}_{3} & \\mathrm{~A}_{3} \\mathrm{~B}_{2}+\\mathrm{A}_{4} \\mathrm{~B}_{4}\n\\end{array}\\right]\\\\\n\\end{array}\n$$\n### é€†ï¼ˆæ–¹é˜µï¼‰\n\nå¹¶éæ‰€æœ‰æ–¹é˜µéƒ½æœ‰é€†ï¼›è€Œå¦‚æœé€†å­˜åœ¨ï¼Œåˆ™$A^{-1}A=I=AA^{-1}$ã€‚å¯¹äºæ–¹é˜µå·¦é€†ç­‰äºå³é€†ï¼Œä½†æ˜¯å¯¹äºçŸ©é˜µè€Œè¨€å·¦é€†ä¸ä¸€å®šç­‰äºå³é€†ã€‚\n\næœ‰é€†çš„çŸ©é˜µè¢«ç§°ä¸ºå¯é€†çš„æˆ–éå¥‡å¼‚(non-singular)çš„,ä¸å¯é€†çš„å¦‚$\\begin{bmatrix}2 &4\\\\1&2\\end{bmatrix}$, ä¹‹å‰æåˆ°è¿‡ä¹Ÿå³å†—ä½™çš„æƒ…å†µã€‚å®ƒä»¬åŒæ ·ä¹Ÿæ»¡è¶³determinant(A)=0ã€‚\n\n---\n\néšåæ•™æˆä»‹ç»äº†é«˜æ–¯-è‹¥å°”å½“ï¼ˆGauss-Jordanï¼‰æ–¹æ³•æ¥è®¡ç®—çŸ©é˜µçš„é€†ï¼Œ\n\nå…¶åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡å°†$\\begin{array}{l|l}A&I\\end{array}$é€šè¿‡æ¶ˆå…ƒæ³•å°†å·¦ä¾§çš„Aå˜åŒ–æˆ$I$ æ¥è‡ªåŠ¨å°†å³ä¾§çš„å•ä½çŸ©é˜µå˜æˆAçš„é€†çŸ©é˜µï¼Œä¹Ÿå³$\\begin{array}{l|l}I&A^{-1}\\end{array}$ã€‚\n\n\n\n## LUåˆ†è§£\n\næ•™æˆå…ˆç»™äº†ä¸€äº›ç­‰å¼ï¼Œ\n\nABçš„é€†çŸ©é˜µï¼š\n\n$$AA^{-1}=I=A^{-1}A\\\\(AB)Â·(B^{-1}A^{-1})$$\n\nå› æ­¤ABçš„é€†çŸ©é˜µæ˜¯$B^{-1}A^{-1}$,è¿™é‡Œæ•™æˆè®²äº†ä¸ªç¬‘è¯ï¼Œè¯´è¿™å°±åƒç©¿å…ˆå¾—ç©¿è¢œå­å†ç©¿é‹å­,è€Œè„±çš„æ—¶å€™åˆ™å…ˆè„±é‹åè„±è¢œã€‚æˆ‘çš„ç†è§£æœ¬è´¨ä¸Šè¿˜æ˜¯çŸ©é˜µä¹˜æ³•ä¸æ»¡è¶³äº¤æ¢å¾‹å¯¼è‡´çš„ã€‚\n\nç”±æ­¤æ•™æˆåˆæŠ›å‡ºäº†$A^{T}$é€†çŸ©é˜µçš„é—®é¢˜,é€šè¿‡ä¸‹åˆ—ç­‰å¼,\n$$\n\\begin{array}{l}\n\\begin{array}{l}\n\\left(\\mathrm{A} \\cdot \\mathrm{A}^{-1}\\right)^{\\mathrm{T}}=\\mathrm{I}^{\\mathrm{T}} \\\\\n\\left(\\mathrm{A}^{-1}\\right)^{\\mathrm{T}} \\cdot \\mathrm{A}^{\\mathrm{T}}=\\mathrm{I}\n\\end{array}\\\\\n\\end{array}\n$$\n$A^{T}$çš„é€†çŸ©é˜µä¸º$(A^{-1})^{T}$\n\n### LUåˆ†è§£\n\nLUåˆ†è§£ä¹Ÿå³ä¸‰è§’åˆ†è§£ï¼ŒLä»£è¡¨ä¸‹ä¸‰è§’çŸ©é˜µ(lower triangle matrix), Uä»£è¡¨(upper triangle matrix), å®ƒæ˜¯é«˜æ–¯-è‹¥å°”å½“æ¶ˆå…ƒæ³•çš„ä¸€ç§è¡¨è¾¾å½¢å¼ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯å·¦ä¹˜Açš„æ¶ˆå…ƒçŸ©é˜µçš„é€†çŸ©é˜µï¼Œä½¿å¾—ç­‰å¼å·¦è¾¹åªå‰©ä¸‹Aè€Œå³è¾¹UçŸ©é˜µåˆ™å·¦ä¹˜ä¸Šå˜åŒ–çŸ©é˜µçš„é€†çŸ©é˜µã€‚\n$$\n\\begin{array}{l}\nE_{p} \\cdot \\ldots \\cdot E_{3} \\cdot E_{2} \\cdot E_{1} \\cdot A=U \n\\\\\nE_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots \\cdot E_{p}^{-1} \\cdot E_{p} \\cdot \\ldots \\cdot E_{3} \\cdot E_{2} \\cdot E_{1} \\cdot A=E_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots \\cdot E_{p}^{-1} \\cdot U \\\\\nI \\cdot A=\\left(E_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots \\cdot E_{p}^{-1}\\right) \\cdot U\n\\end{array}\n$$\nå…¶ä¸­\n$$\nA=L \\cdot U \\\\ L=E_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots E_{p}^{-1} \n$$\n### LUåˆ†è§£çš„å‰æ\n\n1. çŸ©é˜µæ˜¯æ–¹é˜µ\n\n2. çŸ©é˜µæ˜¯å¯é€†çš„\n3. æ¶ˆå…ƒè¿‡ç¨‹ä¸­æ²¡æœ‰0ä¸»å…ƒå‡ºç°, å³ä¸èƒ½è¡Œäº¤æ¢\n\n----\n\n### LUåˆ†è§£çš„è®¡ç®—é‡\n$$\n\\left[\\begin{array}{cccc}\n\\mathrm{a}_{11} & \\mathrm{a}_{12} & \\cdots & \\mathrm{a}_{1 \\mathrm{n}} \\\\\n\\mathrm{a}_{21} & \\mathrm{a}_{22} & \\cdots & \\mathrm{a}_{2 \\mathrm{n}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathrm{a}_{\\mathrm{n} 1} & \\mathrm{a}_{\\mathrm{n} 2} & \\cdots & \\mathrm{a}_{\\mathrm{nn}}\n\\end{array}\\right] \\stackrel{\\text { æ¶ˆå…ƒ }}{\\longrightarrow}\\left[\\begin{array}{cccc}\n\\mathrm{a}_{11} & \\mathrm{a}_{12} & \\cdots & \\mathrm{a}_{1 \\mathrm{n}} \\\\\n0 & \\mathrm{a}_{22} & \\cdots & \\mathrm{a}_{2 \\mathrm{n}} \\\\\n0 & \\vdots & \\ddots & \\vdots \\\\\n0 & \\mathrm{a}_{\\mathrm{n} 2} & \\cdots & \\mathrm{a}_{\\mathrm{nn}}\n\\end{array}\\right]\n$$\nè¿™æ˜¯æ¶ˆå…ƒçš„ç¬¬ä¸€æ­¥,å…¶è¿ç®—æ“ä½œä¸»è¦ä¸º<multiply,subtract>,å› æ­¤å¯¹äºä¸»å…ƒa11å…¶éœ€è¦1æ¬¡ç›¸ä¹˜å’Œ99æ¬¡ç›¸å‡(æ³¨æ„:LUåˆ†è§£ä¸­æ²¡æœ‰è¡Œäº¤æ¢),åˆå› ä¸ºæ¯è¡Œå‘é‡æœ‰100ä¸ªå…ƒç´ ï¼Œæ‰€ä»¥ç¬¬ä¸€æ¬¡æ“ä½œçš„è¿ç®—ä¸º100*100,éšåä¸€æ¬¡é€’å‡ã€‚å¯ä»¥å¾—åˆ°å…¶æ€»çš„è¿ç®—é‡ä¸º$O(n^{2}+(n-1)^{2}+\\cdots+2^{2}+1^{2})$ï¼Œå³$O(\\frac{n^3}{3})$\n\n**P.S.** \n$$\n\\sum_{k=1}^{n} k^{2}=1^{2}+2^{2}+3^{2}+\\cdots+n^{2}=\\frac{n^{3}}{3}+\\frac{n^{2}}{2}+\\frac{n}{6}=\\frac{n(n+1)(2 n+1)}{6}\n$$\n\n\n### ç½®æ¢çŸ©é˜µ\n\næœ€åæ•™æˆä¸ºä¸‹èŠ‚è¯¾é“ºå«æåˆ°äº†ç½®æ¢çŸ©é˜µ(permutation matrix),  né˜¶ç½®æ¢çŸ©é˜µæœ‰$\\left(\\begin{array}{c}\n\\mathrm{n} \\\\1\\end{array}\\right)$ä¸ª,å³n!\n\n","tags":["Math","Linear Algebra"],"categories":["Math","Linear Algebra"]},{"title":"çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆä¸€ï¼‰","url":"/2021/07/03/çº¿æ€§ä»£æ•°ç¬”è®°ï¼ˆä¸€ï¼‰/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n**MIT 18.06 P1-2**\n\n1. The Geometry of Linear Equations\n2. Elimination with Matrices \n\n## P1 æ–¹ç¨‹ç»„çš„å‡ ä½•è§£é‡Š\n\næ•™æˆä»æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„$\\left\\{\\begin{array}{ll}\n2 x&-y   =0 \\\\\n-x & +2y =3\n\\end{array}\\right.$å¼€å§‹, \n\néšåç»™å‡ºå…¶çŸ©é˜µå½¢å¼$\\left[\\begin{array}{cc}\n2 & -1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\mathrm{x} \\\\\n\\mathrm{y}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n3\n\\end{array}\\right]$ è¿™é‡Œç¬¬ä¸€ä¸ªçŸ©é˜µè¢«ç§°ä¸ºç³»æ•°çŸ©é˜µA, ç¬¬äºŒä¸ªåˆ™ç§°ä¸ºå‘é‡xè€Œç¬¬ä¸‰ä¸ªä¸ºå‘é‡b,å› æ­¤çº¿æ€§æ–¹ç¨‹ç»„å¯ä»¥è¡¨ç¤ºä¸ºAx=b.\n\n----\n\n\n\nä»ä¼ ç»Ÿå‡ ä½•æ„ä¹‰ä¸Šçœ‹ï¼Œæ±‚è§£æ–¹ç¨‹ç»„å³æ±‚è§£ä¸¤æ¡ç›´çº¿çš„äº¤ç‚¹ï¼Œä½†æ˜¯æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†ä¸Šå¼çœ‹ä½œå‘é‡çš„**çº¿æ€§ç»„åˆï¼ˆlinear combinationï¼‰**ï¼Œä¹Ÿå³$\\mathrm{x}\\left[\\begin{array}{c}\n2 \\\\\n-1\n\\end{array}\\right]+\\mathrm{y}\\left[\\begin{array}{c}\n-1 \\\\\n2\n\\end{array}\\right]=\\left[\\begin{array}{c}\n0 \\\\\n3\n\\end{array}\\right]$, é€šè¿‡ç»„åˆçŸ©é˜µAçš„ä¸¤ä¸ªåˆ—å‘é‡æ¥å¾—åˆ°çŸ©é˜µb,\n\næ— è®ºæ˜¯ä»¥å“ªç§è§’åº¦, åŒæ ·çš„, æˆ‘ä»¬éƒ½å¯ä»¥å¾—åˆ°(x,y)=(1,2)ã€‚\n\n---\n\næ•™æˆåœ¨è¿™ä¸ªç¯èŠ‚å¯ç¤ºå¤§å®¶ï¼Œè¯•æƒ³ä¸¤ä¸ªåˆ—å‘é‡æ‰€æœ‰çš„çº¿æ€§ç»„åˆçš„é›†åˆæ˜¯ä»€ä¹ˆ----æ˜¯å¹³é¢ã€‚é‚£ä¿©åˆ—å‘é‡ä»€ä¹ˆæƒ…å†µä¸‹ï¼Œè¿™äº›çº¿æ€§ç»„åˆæ— æ³•æ„æˆä¸€ä¸ªå¹³é¢ï¼ˆè§£æ–¹ç¨‹çš„è§’åº¦çš„è¯å³æ— æ³•æ±‚å¾—å”¯ä¸€è§£ï¼‰----ä¸¤ä¸ªåˆ—å‘é‡åœ¨ä¸€æ¡ç›´çº¿ä¸Šçš„æ—¶å€™ï¼Œä¹Ÿå³å®ƒä»¬ä¹‹é—´å­˜åœ¨å€æ•°å…³ç³»ã€‚\n\næ˜¾ç„¶ï¼Œè¿™æ ·ä¿©å‘é‡æ— è®ºå¦‚ä½•çº¿æ€§ç»„åˆéƒ½ä¸€å®šæ˜¯åœ¨ç›´çº¿ä¸Šï¼Œå› æ­¤æ— æ³•æ„æˆä¸€ä¸ªå¹³é¢ã€‚æ¢å¥è¯æ¥è¯´å‘é‡ä¹‹ä¸­å­˜åœ¨å†—ä½™ã€‚è¿™é‡Œä»…ä»…æ¢è®¨äº†äºŒç»´çš„æƒ…å†µï¼Œè¿™ä¸ªç°è±¡å¯ä»¥æ¨å¹¿åˆ°é«˜ç»´ä¸Šã€‚æœºå™¨å­¦ä¹ ä¸­ç‰¹å¾çš„å†—ä½™æœ¬è´¨ä¸Šå°±æ˜¯è¿™ä¸ªæƒ…å†µçš„å…·è±¡ã€‚å¹¶ä¸”åœ¨çº¿æ€§ä»£æ•°ä¸­ä¼šæœ‰æ›´ä¸“ä¸šçš„åè¯å»æè¿°è¿™ä¸€ç°è±¡ï¼Œä¹Ÿå³ä¹‹åæ¶‰åŠåˆ°çš„çŸ©é˜µæ˜¯å¦**å¥‡å¼‚ï¼ˆsingularï¼‰ã€å¯é€†ï¼ˆinverseï¼‰**çš„æ€§è´¨ã€‚\n\n-----\n\næœ€åæ•™æˆå»ºè®®ï¼Œå¯¹äºçŸ©é˜µç›¸ä¹˜\n$\n\\mathrm{A}=\\left[\\begin{array}{ll}\n2 & 5 \\\\\n1 & 3\n\\end{array}\\right], \\mathrm{x}=\\left[\\begin{array}{l}\n1 \\\\\n2\n\\end{array}\\right]\n$\næ¯”èµ·ä¼ ç»Ÿçš„ä½¿ç”¨å‘é‡å†…ç§¯ï¼ˆinner productï¼‰\n\n$\\left[\\begin{array}{ll}2 & 5\\end{array}\\right] \\cdot\\left[\\begin{array}{ll}1 & 2\\end{array}\\right]^{\\mathrm{T}}=12,\\left[\\begin{array}{ll}1 & 3\\end{array}\\right] \\cdot\\left[\\begin{array}{ll}1 & 2\\end{array}\\right]^{\\mathrm{T}}=7$\n\nä»–æ›´å»ºè®®ä»¥ä¸‹å¼åˆ—å‘é‡çš„çº¿æ€§ç»„åˆçš„æ–¹å¼æ¥çœ‹ã€‚\n\n$\\left[\\begin{array}{ll}2 & 5 \\\\ 1 & 3\\end{array}\\right]\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right]=1\\left[\\begin{array}{l}2 \\\\ 1\\end{array}\\right]+2\\left[\\begin{array}{l}5 \\\\ 3\\end{array}\\right]=\\left[\\begin{array}{c}12 \\\\ 7\\end{array}\\right]$\n\n----\n\n## P2 çŸ©é˜µæ¶ˆå…ƒ\n\n### æ¶ˆå…ƒæ–¹æ³•\n\nç»™å‡ºä¸‰å…ƒæ–¹ç¨‹ç»„$\\begin{cases}\n-x &+2y&+z=2\\\\\n3x &+8y&+z=12\\\\\n&+4y&+z=2\n\\end{cases}$ ,\n\nå¯¹åº”çš„çŸ©é˜µå½¢å¼ä¸º$Ax=bâ†’\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n x\\\\\n y\\\\\n z\\\\\n\\end{bmatrix}\n=\\begin{bmatrix}\n 2\\\\\n 12\\\\\n 2\\\\\n\\end{bmatrix}$\n\n\n\næ¶ˆå…ƒçš„æ“ä½œæ•™æˆç»™å‡ºäº†è¯¦ç»†çš„è®²è§£ï¼Œæ“ä½œå¦‚ä¸‹\n\n$\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}\\stackrel{\\text{row2-3row1}}{\\longrightarrow}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}\\stackrel{\\text{row3-2row2}}{\\longrightarrow}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&0&5\\\\\\end{bmatrix}$\n\nè¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯bçŸ©é˜µéœ€è¦å’ŒAçŸ©é˜µè¿›è¡Œç›¸åŒçš„å˜æ¢ï¼Œå› æ­¤åœ¨æˆ‘ä»¬å®é™…è¿ç®—ä¸­ï¼Œæˆ‘ä»¬åˆ™ä¼šæŠŠbçŸ©é˜µå¹¶è¡Œæ”¾ç½®äºAçŸ©é˜µåé¢å†™æˆå¢å¹¿çŸ©é˜µ(augmented matrix)çš„å½¢å¼ï¼Œå¦‚ä¸‹\n\n$\n\\left[\\begin{array}{ll}\n\\mathrm{A} \\mid \\mathrm{b}\n\\end{array}\\right]=\\left[\\begin{array}{ccc|c}\n1 & 2 & 1 & 2 \\\\\n3 & 8 & 1 & 12 \\\\\n0 & 4 & 1 & 2\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{ccc|c}\n1 & 2 & 1 & 2 \\\\\n0 & 2 & -2 & 6 \\\\\n0 & 4 & 1 & 2\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{ccc|c}\n1 & 2 & 1 & 2 \\\\\n0 & 2 & -2 & 6 \\\\\n0 & 0 & 5 & -10\n\\end{array}\\right]\n$\n\néšåå›ä»£(back substitution)å¾—åˆ°æ–¹ç¨‹ç»„$\\begin{cases}x &+2y&+z&=2\\\\\n& 2y &-2z&=6\\\\\n&&5z&=-10\\end{cases}$å³å¯æ±‚è§£ã€‚\n\n---\n\n### æ¶ˆå…ƒçŸ©é˜µ\n\næ­¤è¯¾ç¬¬äºŒéƒ¨åˆ†æ•™æˆä»‹ç»äº†æ¶ˆå…ƒçŸ©é˜µï¼ŒåŸå§‹çŸ©é˜µä¹˜ä»¥å¯¹åº”çš„æ¶ˆå…ƒçŸ©é˜µå¯ä»¥å®Œæˆä¸Šè¿°row2-3row1,row3-2row2æ“ä½œã€‚\n\nè¿™é‡Œä½œä¸ºé“ºå«ï¼Œæ•™æˆæåˆ°äº†ä¹‹å‰æ‰€è¯´çš„ä»¥linear combinationçš„è§’åº¦æ¥çœ‹çŸ©é˜µç›¸ä¹˜ä¹Ÿå³$\\begin{bmatrix}v_1 &v_2&v_3\\end{bmatrix}\\begin{bmatrix}3 \\\\4\\\\5\\end{bmatrix}=3v1+4v2+5v3$ \n\nè¿™é‡Œæ˜¾è€Œæ˜“è§ï¼Œå³ä¾§çš„çŸ©é˜µæ˜¯å¯¹å·¦ä¾§çš„çŸ©é˜µè¿›è¡Œåˆ—çš„çº¿æ€§ç»„åˆã€‚ä½†äº‹å®ä¸Šåœ¨æ¶ˆå…ƒçš„è¿‡ç¨‹ä¸­æˆ‘ä»¬æ˜¯å¯¹çŸ©é˜µçš„è¡Œè¿›è¡Œæ“ä½œã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å°†å…¶çœ‹ä½œæ˜¯å·¦ä¾§çŸ©é˜µå¯¹å³ä¾§çš„è¡Œå‘é‡è¿›è¡Œçº¿æ€§ç»„åˆã€‚\n\n$\\begin{bmatrix}3 &4&5\\end{bmatrix}\\begin{bmatrix}row1 \\\\row2\\\\row3\\end{bmatrix}=3*row1+4*row2+5*row3$\n\n\n\nç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å·¦ä¹˜æ¶ˆå…ƒçŸ©é˜µæ¥å¯¹åŸå§‹çŸ©é˜µè¿›è¡Œæ¶ˆå…ƒæ“ä½œã€‚å¯¹äºç¬¬ä¸€æ­¥row2-3row1ä¸»éœ€è¦åœ¨æœ€åˆçš„çŸ©é˜µAå·¦ä¾§ä¹˜ä¸Šæ¶ˆå…ƒçŸ©é˜µ$E_{21}$ä¹Ÿå³$E_{21}A=Wâ†’\\begin{bmatrix}1&0&0\\\\\n -3&1&0\\\\\n 0&0&1\\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}$\n\n----\n\n**P.S.** è¿™é‡Œå…³äºå¦‚ä½•å£ç®—$E_{21}$,å¯ä»¥ä»å³ä¾§è¡Œå‘é‡çš„çº¿æ€§ç»„åˆæ¥ç†è§£ï¼ŒWçŸ©é˜µçš„ç¬¬ä¸€è¡Œæ˜¯ç”±AçŸ©é˜µä¸­æ¯ä¸€è¡Œå…³äº$E_{21}$çŸ©é˜µçš„ç¬¬ä¸€è¡Œå¯¹åº”å…ƒç´ çš„çº¿æ€§ç»„åˆã€‚å› ä¸ºçŸ©é˜µç›¸ä¹˜æ—¶å·¦ä¾§çŸ©é˜µçš„è¡Œæ•°å’Œå³ä¾§çŸ©é˜µçš„åˆ—æ•°æ€»æ˜¯ç›¸ç­‰çš„ï¼Œå› æ­¤$E_{21}$ç¬¬ä¸€è¡Œå…ƒç´ çš„åˆ—ç´¢å¼•ä¸å…¶ç›¸ä¹˜çš„è¡Œå‘é‡çš„è¡Œç´¢å¼•çš„æ˜¯ç›¸ç­‰çš„ã€‚è¿ç®—æ—¶åªéœ€è¦è¦éå†ç´¢å¼•ç„¶åç›¸åŠ å³å¯å¾—åˆ°ç»“æœã€‚æ¯”å¦‚ç¬¬ä¸€è¡Œå³ä¸º$1*[1\\quad2 \\quad1]+0*[3\\quad 8\\quad1]+0*[3\\quad 8\\quad1]$å¾—åˆ°$[1\\quad 2\\quad1]$ã€‚\n\nè¿™é‡Œå¾ˆè‡ªç„¶çš„ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å¦‚æœ$E_{21}$ä¸­æŸä¸€è¡Œåªæœ‰å¯¹è§’å…ƒç´ ä¸º1å…¶ä½™ä¸º0ï¼Œé‚£ä¹ˆç›®æ ‡çŸ©é˜µWä¸­å¯¹åº”çš„é‚£è¡Œä¸åŸæ¥AçŸ©é˜µçš„é‚£è¡Œç›¸åŒï¼Œä¹Ÿå³ä¸å‘ç”Ÿå˜åŒ–ã€‚å› ä¸ºè¡Œå‘é‡çš„çº¿æ€§ç»„åˆä¸­åªæœ‰åŸæœ¬çš„é‚£è¡Œä¸”ç³»æ•°ä¸º1ã€‚è€Œè‹¥åŸå§‹çŸ©é˜µä¹˜ä¸Šä¸€ä¸ªåªæœ‰å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ä¸º1å…¶ä½™çš†ä¸º0çš„çŸ©é˜µæ—¶ï¼ŒåŸå§‹çŸ©é˜µä¸å‘ç”Ÿä»»ä½•å˜åŒ–ã€‚è€Œè¿™æ ·çš„çŸ©é˜µåœ¨çº¿æ€§ä»£æ•°ä¸­è¢«å®šä¹‰ä¸º**å•ä½çŸ©é˜µ(Identity Matrix)**  $I=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}$\n\n---\n\nç±»ä¼¼çš„, æˆ‘ä»¬å¯ä»¥å¾—åˆ°$E_{32}$, å¦‚ä¸‹æ‰€ç¤º\n\n$E_{32}W=Uâ†’\\begin{bmatrix}1&0&0\\\\\n 0&1&0\\\\\n 0&-2&1\\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&0&5\\\\\\end{bmatrix}$ æ‰€ä»¥æ•´ä¸ªæ¶ˆå…ƒçš„è¿‡ç¨‹å³ä¸º$E_{32}(E_{21}A)=U$,è¿™é‡Œç”±äºçŸ©é˜µè¿ç®—ä¸­ç»“åˆå¾‹æ˜¯å¹¶è¯æ˜é€‚ç”¨çš„ï¼Œæˆ‘ä»¬å¯ä»¥å†™æˆ$(E_{32}E_{21})A=U$,å› æ­¤æ¶ˆå…ƒçš„è¿‡ç¨‹å…¶å®æ˜¯å¯ä»¥ä¸€æ­¥åˆ°ä½çš„.\n\n-----\n\n**P.S. **å•ä½çŸ©é˜µ$I=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}$æ˜¯ä¸»å¯¹è§’çº¿å…ƒç´ ä¸º1å…¶ä½™ä¸º0 ï¼Œè€Œè‹¥æŸä¸€çŸ©é˜µæ˜¯å‰¯å¯¹è§’çº¿çš„å…ƒç´ çš†ä¸º1å…¶ä½™ä¸º0$\\begin{bmatrix}0&0&1\\\\0&1&0\\\\1&0&0\\end{bmatrix}$åˆæœ‰ä»€ä¹ˆå«ä¹‰å—----è¿™å…¶å®æ˜¯**ç½®æ¢çŸ©é˜µ(Permutation Matrix)**ï¼Œå¯ç”¨äºäº¤æ¢ç›®æ ‡çŸ©é˜µçš„è¡Œå‘é‡\n\n$\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}=\\begin{bmatrix}c&d\\\\a&b\\end{bmatrix}$ï¼Œä¹Ÿå¯äº¤æ¢åˆ—å‘é‡$\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}=\\begin{bmatrix}b&a\\\\d&c\\end{bmatrix}$\n\n----\n\n### é€†è¿ç®—\n\nåˆ°ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†Aé€šè¿‡ä¹˜ä¸Šå˜æ¢çŸ©é˜µå˜åˆ°Uï¼Œé‚£ä¹ˆä»Uå˜å›Aï¼Œå³æ¶ˆå…ƒçš„é€†è¿ç®—åˆæ˜¯æ€ä¹ˆæ ·çš„å‘¢ï¼Ÿæ˜¯ä¸æ˜¯å¯¹äºä»»ä½•Uéƒ½æœ‰é€†å˜æ¢å¯ä»¥å›åˆ°Aï¼Ÿ\n\nä»¥$E_{21}$ä¸ºä¾‹, $E_{21}A=Wâ†’\\begin{bmatrix}1&0&0\\\\\n -3&1&0\\\\\n 0&0&1\\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}$,ä¸ºäº†è®©Wå˜å›Aï¼Œä¹Ÿå³æ±‚è§£\n\n$?W=Aâ†’\\begin{bmatrix}\\\\\n &?&\\\\\n \\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}$. æ˜¾ç„¶ç”±äºåŸæ¥çš„æ¶ˆå…ƒæ“ä½œä¸ºrow2-3row1å› æ­¤æˆ‘ä»¬åªéœ€è¦å°†row2è¢«å‡å»çš„3row1åŠ å›å»å°±å¯ä»¥å¾—åˆ°åŸå§‹çš„Aï¼Œä¹Ÿå³å–æ¶ˆå˜åŒ–çŸ©é˜µ$E_{21}$å¸¦æ¥çš„æ”¹å˜ã€‚å› æ­¤å°†å…¶çœ‹ä½œè¡Œå‘é‡çš„çº¿æ€§ç»„åˆï¼Œrow1å’Œrow3å…ƒç´ ä¸å˜ï¼Œrow2åŠ å›row1çš„ä¸‰å€ï¼Œå¾—åˆ°$\\begin{bmatrix}1&0&0\\\\3&1&0\\\\0&0&1\\end{bmatrix}$, è€Œè¯¥çŸ©é˜µä¹Ÿå³ä¸º$E_{21}$çš„é€†çŸ©é˜µ,ç”¨$E_{21}^{-1}$è¡¨ç¤ºï¼Œå®ƒä»¬æ»¡è¶³$E_{21}^{-1}E_{21}=I$ã€‚å› æ­¤ï¼Œè¿™é‡Œå¼•å‡ºäº†çº¿æ€§ä»£æ•°ä¸­é€†çŸ©é˜µçš„æ¦‚å¿µï¼šçŸ©é˜µ$E$çš„çŸ©é˜µç€è®°ä½œ$E^{-1}$ï¼Œå¹¶ä¸”æœ‰$E^{-1}E=I$.\n\n\n\n","tags":["Math","MIT 18.06"],"categories":["Math","Linear Algebra"]},{"title":"Pythonè¡¥åŸºç¡€ï¼ˆä¸€ï¼‰","url":"/2021/06/16/Pythonä¸­çš„å¼•ç”¨/","content":"\n<!-- more -->\n\nä¸€äº›Pythonçš„åŸºç¡€çŸ¥è¯†ï¼Œæ¶‰åŠå˜é‡ã€å¯¹è±¡ã€å¼•ç”¨ã€è®¡æ•°ã€æ‹·è´\n\n## ä¸€äº›æŠ„ä¸‹æ¥çš„åºŸè¯\n\n-Pythonä¸º**åŠ¨æ€è§£é‡Šæ€§**è¯­è¨€ï¼Œèµ‹å€¼ä¸éœ€è¦äº‹å…ˆ**å£°æ˜å˜é‡**ï¼Œç±»å‹æ˜¯è¿è¡Œè¿‡ç¨‹ä¸­è‡ªåŠ¨å†³å®šçš„\n\n-Pythonä¸­éƒ½æ˜¯**å¼•ç”¨**\n\n-Pythonä¸­**å˜é‡**å’Œ**å¯¹è±¡**çš„å…³ç³»æ˜¯**å¼•ç”¨**\n\n## Pythonä¸­çš„å˜é‡ã€å¯¹è±¡ã€å¼•ç”¨\n\n### å˜é‡ï¼ˆvariableï¼‰\n\n- ç¬¬ä¸€æ¬¡èµ‹å€¼å³åˆ›å»ºï¼Œå†æ¬¡'èµ‹å€¼'ä¼šæ”¹å˜æ”¹å˜çš„å€¼\n\n- å˜é‡åæœ¬èº«æ˜¯æ— ç±»å‹çš„ï¼Œå¯¹è±¡æ‰æœ‰ç±»å‹ï¼Œå˜é‡åªæ˜¯å¼•ç”¨äº†å¯¹è±¡\n- å˜é‡éœ€è¦åœ¨ä½¿ç”¨å‰èµ‹å€¼ï¼Œå¦åˆ™æŠ¥é”™\n\n### å¯¹è±¡ï¼ˆObjectï¼‰\n\n- å¯¹è±¡æœ‰ç±»å‹\n\n- å¯¹è±¡ç”Ÿæˆæ—¶ä¼šå¾—åˆ°ä¸€å—å†…å­˜ç©ºé—´æ¥å­˜å‚¨å…¶å€¼\n\n- æ¯ä¸ªå¯¹è±¡æœ‰ä¸¤ä¸ªæ ‡å‡†çš„å¤´éƒ¨ä¿¡æ¯\n\n  - ç±»å‹æ ‡è¯†ç¬¦ï¼šæ ‡è¯†å¯¹è±¡çš„ç±»å‹\n  - å¼•ç”¨è®¡æ•°å™¨ï¼šå†³å®šæ˜¯å¦å›æ”¶å¯¹è±¡\n\n### å¼•ç”¨ï¼ˆReferenceï¼‰\n\n- Pythonä¸­**å˜é‡**åˆ°**å¯¹è±¡**çš„è¿æ¥å«**å¼•ç”¨**\n- **å¼•ç”¨**æ˜¯ä¸€ç§å…³ç³»ï¼Œé€šè¿‡**å†…å­˜**ä¸­çš„**æŒ‡é’ˆ**å½¢å¼å®ç°\n- èµ‹å€¼æ“ä½œæ—¶ï¼Œè‡ªåŠ¨å»ºç«‹**å˜é‡**å’Œ**å¯¹è±¡**ä¹‹é—´çš„å…³ç³»ï¼Œå³å¼•ç”¨\n---\n\n**ä¸¾ä¸ªæ —å­**ï¼š\n\n  ```python\n  str='hello world'\n  print('type:',type(str),'\\n','value:','\\n',str,'id:',id(str))\n  ```\n\n  å¾—åˆ°ç»“æœ\n\n  ```shell\n  type: <class 'str'> \n  value: \n  hello world id: 140225804592048\n  ```\n\n>typeæŸ¥çœ‹å¯¹è±¡strçš„ç±»å‹æ ‡è¯†ç¬¦ï¼Œç»“æœä¸ºstrå³å­—ç¬¦ä¸²ç±»å‹ï¼ŒvalueæŸ¥çœ‹å¯¹è±¡strçš„å€¼ä¸º'hello world',idç”¨æ¥æŸ¥çœ‹å¯¹è±¡çš„å†…å­˜åœ°å€ã€‚\n>\n>èµ‹å€¼è¯­å¥ ```str='hello world'``` å®é™…ä¸Šæ˜¯åšçš„æ˜¯ï¼Œ1 -æ›¿å€¼ä¸ºhello worldçš„å­—ç¬¦ä¸²å¯¹è±¡å¼€è¾Ÿå†…å­˜åœ°å€ 2 -å˜é‡strå¼•ç”¨å¯¹è±¡çš„åœ°å€ï¼Œç›¸å½“äºæŒ‡é’ˆæŒ‡å‘åœ°å€ã€‚\n\n## Pythonæ ‡å‡†æ•°æ®ç±»å‹\n\nç®€å•ä»‹ç»äº†å˜é‡ï¼Œå¯¹è±¡å’Œå¼•ç”¨çš„åŸºç¡€çŸ¥è¯†ï¼Œæ¥ä¸‹æ¥ä»‹ç»ä¸‹Pythonä¸­äº”å¤§æ ‡å‡†æ•°æ®ç±»å‹\n\nNumbers \n\nString\t\n\nList\n\nTuple\n\nDictionary\n\n- åœ¨pythonä¸­, String tuple numberæ˜¯ä¸å¯å˜æ•°æ®ç±»å‹è€Œ list dictæ˜¯å¯å˜æ•°æ®ç±»å‹\n\n## å¯å˜å¯¹è±¡å’Œä¸å¯å˜å¯¹è±¡\n\nå¯å˜å¯¹è±¡åˆ›å»ºä¹‹åä»å¯ç»§ç»­ä¿®æ”¹ï¼Œä¸å¯å˜å¯¹è±¡åˆ™ä¸å¯ä¿®æ”¹ã€‚å…·ä½“ä¸€ç‚¹ï¼š\n\n- å¯¹è±¡æ‰€æŒ‡å‘çš„å†…å­˜ä¸­çš„å€¼å¯ä»¥è¢«æ”¹å˜ã€‚å˜é‡ï¼ˆå‡†ç¡®çš„è¯´æ˜¯å¼•ç”¨ï¼‰æ”¹å˜åï¼Œå®é™…ä¸Šæ˜¯å…¶æ‰€æŒ‡çš„å€¼ç›´æ¥å‘ç”Ÿæ”¹å˜ï¼Œå¹¶æ²¡æœ‰å‘ç”Ÿå¤åˆ¶è¡Œä¸ºï¼Œä¹Ÿæ²¡æœ‰å¼€è¾Ÿæ–°çš„åœ°å€ï¼Œé€šä¿—ç‚¹è¯´å°±æ˜¯åŸåœ°æ”¹å˜\n\n- ä¸å¯å˜å¯¹è±¡æ‰€æŒ‡å‘çš„å†…å­˜ä¸­çš„å€¼ä¸èƒ½è¢«æ”¹å˜ã€‚å½“æ”¹å˜æŸä¸ªå˜é‡æ—¶å€™ï¼Œç”±äºå…¶æ‰€æŒ‡çš„å€¼ä¸èƒ½è¢«æ”¹å˜ï¼Œç›¸å½“äºæŠŠåŸæ¥çš„å€¼å¤åˆ¶ä¸€ä»½åå†æ”¹å˜ï¼Œè¿™ä¼šå¼€è¾Ÿä¸€ä¸ªæ–°çš„åœ°å€ï¼Œå˜é‡å†æŒ‡å‘è¿™ä¸ªæ–°çš„åœ°å€\n\n  \n\n## èµ‹å€¼ã€æµ…æ‹·è´ã€æ·±æ‹·è´\n\nèµ‹å€¼ï¼šå¯¹è±¡çš„å¼•ç”¨\n\næµ…æ‹·è´ï¼šæ‹·è´çˆ¶å¯¹è±¡ï¼Œä¸ä¼šæ‹·è´å¯¹è±¡å†…éƒ¨çš„å­å¯¹è±¡\n\næ·±æ‹·è´ï¼šå®Œå…¨æ‹·è´å¯¹è±¡\n\n----\n\nå…ˆå ä¸ªå‘ï¼Œæ˜å¤©ä¸‹è¯¾å›æ¥ç»§ç»­æ›´ã€‚\n","tags":["Python"],"categories":["Programming","Python"]},{"title":"ç»ˆç‚¹å’Œæ„ä¹‰","url":"/2021/05/22/ç»ˆç‚¹å’Œæ„ä¹‰/","content":"\n<!-- more -->\n\n{% meting \"5265370\" \"netease\" \"song\" %}\n\næ„ä¹‰å›°æ‰°äº†æˆ‘å¾ˆå¤šå¹´ï¼Œç¬¬ä¸€æ¬¡æ„è¯†åˆ°äººç”Ÿç»ˆç‚¹é‚£ä¼šï¼Œå¿ƒé‡ŒçœŸçš„å¾ˆå®³æ€•ï¼Œä¼šææƒ§ä¹ŸæŠ—æ‹’ï¼Œé‚£ç§å‡è§†è™šç©ºçš„æ„Ÿè§‰è®©æˆ‘æ„Ÿå—åˆ°å­¤ç‹¬æ— åŠ©ã€‚æˆ‘ç”šè‡³ä¼šåœ¨è„‘æµ·é‡Œæƒ³è±¡è‡³äº²ç¦»ä¸–æ—¶æ‰‹æ¡ç€ä»–ä»¬è‹ç™½çš„æ‰‹äº¦æˆ–æ˜¯è‡ªå·±å¹´è¿ˆè¡°è€æ¿’æ­»ä¹‹é™…è‡ªå·±çš„å­©å­åœ¨ç—…åºŠå‰å®ˆå€™çš„åœºæ™¯ï¼Œå°±åƒç”µå½±ã€Šè¿”è€è¿˜ç«¥ã€‹å¼€å¤´æ¸²æŸ“é‚£ä¸€èˆ¬ã€‚é«˜è€ƒå‰çš„é‚£æ®µæ—¶é—´ä»¥åŠå¤§å­¦çš„å‰å‡ å¹´å¯¹ç»ˆç‚¹çš„ææƒ§éƒ½è®©æˆ‘ååˆ†æŒ£æ‰å°¤å…¶æ˜¯åœ¨ç‹¬è‡ªä¸€äººçš„éš¾çœ ä¹‹å¤œã€‚\n\n\n\næˆ‘è§‰å¾—è¿™æ˜¯ä¸€ä»¶å¾ˆæ®‹é…·çš„äº‹æƒ…ï¼Œå› ä¸ºæ— è®ºæˆ‘åœ¨äººç”Ÿè·‘é“ä¸Šæ¼«æ­¥æˆ–æ˜¯å¥”è·‘é¢†å…ˆæˆ–æ˜¯è½åï¼Œæœ€åéƒ½æœ‰é‚£ä¹ˆä¸€ä¸ªç»ˆç‚¹ç­‰ç€æˆ‘ï¼Œåˆ°é‚£æ—¶æ— è®ºå¿ƒä¸­è¿˜æœ‰å¤šå¤§çš„æ‰§å¿µæˆ‘ä¹Ÿåªèƒ½æ”¾æ‰‹ã€‚å¯æˆ‘æƒ³ä¸€ç›´è·‘æˆ‘å¹¶ä¸æƒ³ç»“æŸæˆ‘ä¹Ÿå®³æ€•ç»“æŸï¼Œç”šè‡³è´ªå©ªçš„æˆ‘è¿˜æƒ³å’Œæˆ‘çš„æœ‹å‹äº²äººä»¬ä¸€ç›´åœ¨ä¸€èµ·ã€‚ä½†æ˜¯ç»ˆç‚¹å°±åœ¨é‚£å¹¶ä¸”ä¸æ¯«ä¸å—ä»»ä½•ä¸»è§‚çš„å½±å“ç”šè‡³ä¸å­˜åœ¨å®¢è§‚ä¸Šçš„æ”¹å˜ã€‚\n\n\n\nå°±åƒç¾å‰§ã€ŠTrue Detectiveã€‹ä¸­Rustçš„é‚£æ®µè¯:â€œæˆ‘è®¤ä¸ºäººç±»çš„æ„è¯†æ˜¯è¿›åŒ–è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªå¯æ‚²çš„é”™è¯¯ï¼Œè¿™è®©æˆ‘ä»¬å˜å¾—å¤ªæœ‰è‡ªæˆ‘æ„è¯†äº†ã€‚è‡ªç„¶ä»è‡ªèº«ä¸­æŠ½ç¦»å‡ºä¸€éƒ¨åˆ†åˆåŒ–ä¸ºè‡ªç„¶ï¼Œä½†ä»è‡ªç„¶æ³•åˆ™æ¥è¯´æˆ‘ä»¬æ˜¯ä¸è¯¥å­˜åœ¨çš„ç”Ÿç‰©ï¼Œæˆ‘ä»¬è¢«â€œæ‹¥æœ‰è‡ªæˆ‘â€è¿™ä¸€å¹»è§‰ç»™å¥´å½¹äº†ã€‚å› ä¸ºæ„Ÿå®˜ä½“éªŒå’Œæ„Ÿè§‰ç›¸ç»“åˆï¼Œè¢«è®¾å®šæˆè®©æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬æ¯ä¸ªäººéƒ½æ˜¯æŸä¸ªäººï¼Œå¯äº‹å®ä¸Šæˆ‘ä»¬è°éƒ½ä¸æ˜¯ã€‚æˆ‘è®¤ä¸ºå¯¹äºæ‰€æœ‰ç‰©ç§æ¥è¯´ï¼Œæœ€å´‡é«˜çš„äº‹æƒ…å°±æ˜¯æ‹’ç»è¢«è®¾å®šã€åœæ­¢ç¹æ®–ã€æ‰‹ç‰µæ‰‹èµ°å‘ç­äº¡ã€‚â€\n\n\n\nä¸ªä½“æ‹¥æœ‰å¼ºçƒˆçš„è‡ªæˆ‘æ„è¯†çš„ç¡®æ˜¯è’è°¬çš„ï¼Œè¿™ç§å¼ºçƒˆçš„è‡ªæˆ‘æ„è¯†è®©æˆ‘å¾ˆéš¾è¿‡çš„æ´’è„±ï¼Œä¹Ÿå¾ˆéš¾ç§¯ææŠ•å…¥åˆ°ç°å®ä¸­ï¼Œæˆ‘æ˜¯å¤šæƒ³åƒã€Šç‰›æ°“ã€‹ä¸­æè¿°çš„é‚£æ ·â€œæ— è®ºæˆ‘æ´»ç€ï¼Œè¿˜æ˜¯æ­»å»ï¼Œæˆ‘éƒ½æ˜¯ä¸€åªç‰›æ°“ï¼Œå¿«ä¹åœ°é£æ¥é£å»ã€‚â€å°±é‚£ä¹ˆæ¼«æ— ç›®çš„çš„é£æ¥é£å»å¤šå¥½å•Šï¼\n\n\n\nå¯¹ç»ˆç‚¹çš„æ€è€ƒä¼¼ä¹å¹¶ä¸åƒæ˜¯çªç„¶åœ¨æˆ‘è„‘æµ·ä¸­è¹¦å‡ºï¼Œæˆ‘æƒ³è¿™ä¸€åˆ‡åº”è¯¥æ˜¯å½’ç»“äºé‚£æ—¶å€™è‡ªèº«æ‹’ç»ç¤¾äº¤æŠŠè‡ªå·±å°é—­åœ¨è‡ªå·±ç‹­å°çš„èˆ’é€‚åœˆä¸­ï¼Œå› ä¸ºæ›¾ç»åœ¨å¤šæ¬¡ç›®ç¹äººæ€§çš„æ¶æ¯’åï¼Œç²¾ç–²åŠ›å°½ï¼Œå¼€å§‹å¯¹äººç¾¤å’Œé›†ä½“æœ‰äº†è«åçš„ææƒ§ï¼Œæˆ‘ä¸»åŠ¨æŠŠè‡ªå·±è¾¹ç¼˜åŒ–äº†ï¼ŒæŠŠè‡ªå·±ä»é›†ä½“ä¸­æŠ½ç¦»å‡ºæ¥ã€‚ä¹Ÿå› æ­¤æˆ‘å¯¹ä¸€ç³»åˆ—ç¤¾ä¼šæ´»åŠ¨ä¸å†ä¸Šå¿ƒï¼Œäºæ˜¯ä¹é‚£å‡ å¹´æ„Ÿè§‰è‡ªå·±æµ‘æµ‘å™©å™©çš„,é æ¸¸æˆå’ŒåŠ¨æ¼«éº»é†‰è‡ªå·±ã€‚æ„ä¹‰çš„ç¼ºå¤±è®©æˆ‘æ„Ÿåˆ°è¿·èŒ«ï¼Œå¯¹äºå¤§å®¶çƒ­è¡·çš„GPAï¼Œç§‘ç ”ï¼Œç«èµ›éƒ½è®©æˆ‘æä¸èµ·å…´è¶£æ¥ã€‚æˆ‘åªæ˜¯æŒ‰éƒ¨å°±ç­å®Œæˆå·²ç»å˜æˆä¹ æƒ¯çš„æ—¥å¸¸ã€‚\n\n\n\né‚£æ®µæ—¶é—´è§‚å¯Ÿèº«è¾¹çš„äººæˆä¸ºäº†å”¯ä¸€çš„ä¹è¶£ï¼Œäººç¾¤ä¸­æœ‰ä¸ºäº†å¡«è¡¥å¿ƒä¸­çš„è‡ªå‘æ„Ÿç”˜æ„¿è¢«è™šè£å¥´å½¹çš„ï¼Œä¹Ÿæœ‰ä¸ºäº†è‡å¤´å°åˆ©ç»å°½è„‘æ±å»åˆ©ç”¨åˆ«äººå¹¶ä¸ºæ­¤æ²¾æ²¾è‡ªå–œçš„ï¼Œè¿˜æœ‰ç¬‘é‡Œè—åˆ€æå°½æ¶æ¯’çš„è™šä¼ªä¹‹äºº......å½“ç„¶ä¹Ÿä¸ä¹çœŸè¯šæ­£ç›´çš„ï¼Œå–„è‰¯æ— æ‰€æ±‚çœŸå¿ƒå¸®åŠ©é¼“åŠ±ä»–äººçš„ã€‚ä½†æ˜¯æ— è®ºå¦‚ä½•ï¼Œå¤§å®¶å¥½åƒéƒ½å¾ˆå¿™ç¢Œï¼Œä¼¼ä¹èº«è¾¹çš„äººéƒ½æ²¡æœ‰å»åœ¨æ„æ—¢å®šçš„ç»ˆç‚¹ã€‚\n\n\n\nä¸€æ—¶é—´æ„Ÿè§‰è‡ªå·±å°±åƒæ˜¯ä¸€ä¸ªåœ¨äººæ¥äººå¾€çš„åå­—è¡—å£çš„å¹½çµï¼Œç«™åœ¨å²”è·¯ä¸­å¤®ï¼Œæ¥æ¥å¾€å¾€çš„è¡Œäººä»æˆ‘èº«ä¸Šç©¿è¿‡ï¼Œæ¼«æ— ç›®çš„æ¸¸è¡çš„å´ä¼¼ä¹åªæœ‰æˆ‘ä¸€ä¸ªã€‚æˆ‘æƒ³æˆ‘æ˜¯æˆ‘è¿·å¤±äº†,åœ¨è¿™è’è¯çš„äººç”Ÿä¸­å¯»æ‰¾æ„ä¹‰å°±åƒæ˜¯ç¼˜æœ¨æ±‚é±¼ã€‚\n\n\n\nç›´åˆ°ç»å†äº†å¾ˆå¤šäº‹æƒ…åæˆ‘æ‰æ˜ç™½ï¼Œå°±åƒè‡§å…‹å®¶çš„é‚£å¥â€œäººç”Ÿæ°¸è¿œè¿½é€å¹»å…‰ï¼Œè°æŠŠå¹»å…‰çœ‹ä½œå¹»å…‰ï¼Œè°ä¾¿æ²‰å…¥æ— è¾¹è‹¦æµ·â€ ï¼Œä¹Ÿè®¸æ„ä¹‰ä»æ¥ä¸åœ¨äºæ„ä¹‰æœ¬èº«ã€‚\n\n\n\nç°å®çš„æ‹·æ‰“æ˜¯æˆ‘æ‘†è„±è¿™ä¸ªæ¡æ¢çš„é‡å¤§å¥‘æœºï¼Œå› ä¸ºæˆ‘ä»æ¥ä¸æ˜¯ä¸€ä¸ªå¯ä»¥ä»æŠ½è±¡æ¦‚å¿µä¹‹ä¸­å»ºç«‹èµ·æ›´é«˜å±‚è®¤çŸ¥çš„äººï¼Œæœ¬èƒ½çš„æ€€ç–‘è®©æˆ‘æ— æ³•ä¸“æ³¨äºå…¶ä¸­ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘æ˜¯é‚£ç§éœ€è¦ç©ºé—´å’Œå…·è±¡çš„äººï¼Œæˆ‘æ›´æ“…é•¿ä»å®ä¾‹ä¸­å»æŠ½è±¡è§£æ„æ¦‚å¿µç„¶åå†å»æ„å»ºæ–°çš„æ¦‚å¿µã€‚ä¹Ÿå› æ­¤æ³¨å®šäº†ç°å®å’Œå®è·µæ‰æ˜¯æˆ‘æ‘†è„±è¿™ä¸€ç‰¢ç¬¼çš„å…³é”®ã€‚\n\n\n\nè¿˜è®°å¾—é‚£æ®µæ—¶é—´æ¥è¿çš„æƒ…æ„Ÿä¸Šçš„æŒ«è´¥ï¼Œæœ‹å‹çš„åˆ©ç”¨å’ŒèƒŒå›ä»¥åŠç›´é¢æ®‹é…·çš„ç”Ÿæ­»ç¦»åˆ«è®©æˆ‘æ„Ÿå—åˆ°å·¨å¤§çš„ç—›è‹¦ã€‚æ¶ˆææƒ…ç»ªå°†æˆ‘åå™¬ï¼Œé‚£æ®µæ—¶é—´ååˆ†å—œç¡ï¼Œå› ä¸ºæ¢¦ä¸­çš„ä¸–ç•Œè¿˜æ²¡å˜çš„ä¸€å¡Œç³Šæ¶‚ã€‚å¯æ˜¯ä¸€æ—¦æ¢¦åˆé†’äº†ï¼Œä¸€åˆ‡è¿˜æ˜¯ç…§æ—§ã€‚\n\n\n\nå¹¸è¿çš„æ˜¯ï¼Œå‡æœŸé‡Œæˆ‘æ”¶åˆ°äº†æœ‹å‹çš„ä¸€æ¬¡æ—…è¡Œé‚€çº¦ã€‚æ˜¯å»æ–°è¥¿å…°ï¼Œå—åŠçƒçš„ä¸€ä¸ªå°å²›å›½ï¼Œä¹Ÿæ˜¯åœ¨é‚£é‡Œæˆ‘è§åˆ°äº†æ›´å¤§çš„ä¸–ç•Œï¼Œä½“éªŒåˆ°äº†æ›´å¤šçš„æ–°å¥‡ä¹‹ç‰©ã€‚\n\n\n\né‚£æ—¶å€™ï¼Œå‡ºå»çœ‹ä¸€çœ‹çš„æƒ³æ³•å¼€å§‹èŒç”Ÿï¼Œä¹Ÿæ˜¯ä»é‚£æ—¶å€™æˆ‘æ‰çœŸæ­£å¼€å§‹å¯¹äººç”Ÿå¼€å§‹äº†æ¨¡ç³Šçš„è§„åˆ’ï¼Œå°±åƒæ˜¯æ¨¡ç”µä¸­çš„æ­£åé¦ˆã€‚ä¹‹åä¸€åˆ‡éƒ½å¼€å§‹æ‚„ç„¶æ”¹å˜ï¼Œä¸æ–­åœ°çªç ´ä¸æ–­åœ°å’Œæ–°çš„äººæ¥è§¦ï¼Œæˆ‘ä½“éªŒåˆ°äº†æ›´å¤šï¼Œè§‰å¾—å……å®ã€‚å¼€å§‹æ…¢æ…¢çš„ä¸å»æ€è€ƒäººç”Ÿçš„æ„ä¹‰ï¼Œä¹Ÿä¸å†å»ç•æƒ§ç»ˆç‚¹ã€‚æˆ‘æ‰¾åˆ°äº†è‡ªå·±çš„è·¯ï¼Œå¹¶ä¸”åªæƒ³åšå®šçš„èµ°ä¸‹å»ã€‚\n\n\n\nå”æœ¬åè¯´ç”Ÿå‘½æ˜¯ä¸€å›¢æ¬²æœ›ï¼Œæ¬²æœ›å¾—ä¸åˆ°æ»¡è¶³å°±ç—›è‹¦ï¼Œæ»¡è¶³ä¾¿æ— èŠï¼Œäººç”Ÿåœ¨ç—›è‹¦å’Œæ— èŠä¹‹é—´æ‘‡æ‘†ã€‚è€Œæˆ‘çš„æ¬²æœ›ä»…ä»…æ˜¯ä¸æ–­å»ä½“éªŒå»çªç ´äººç”Ÿçš„è¾¹é™…ç½¢äº†ã€‚\n\n{% asset_img Newzealand.jpeg %}\n\n","tags":["Life","Meaning"],"categories":["Life","Record"]},{"title":"å®éªŒå®¤LinuxæœåŠ¡å™¨ç¯å¢ƒéƒ¨ç½²(ä¸€)","url":"/2021/04/27/å®éªŒå®¤LinuxæœåŠ¡å™¨ç¯å¢ƒéƒ¨ç½²ï¼ˆä¸€)/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\nå› ä¸ºå¹¶éCSç§‘ç­ï¼Œä¹‹å‰å°‘æœ‰æœºä¼šæ¥è§¦è¿™æ–¹é¢çš„å†…å®¹ï¼Œå¯¹äºè®¡ç®—æœºçš„åº•å±‚æ¶æ„å’ŒLinuxéƒ½æ˜¯æ¯”è¾ƒé™Œç”Ÿçš„ï¼Œå› æ­¤æ­¤ç¯‡åªæµ…æ˜¾ä»‹ç»ä¸‹å¤§è‡´åŸç†ä»¥åŠminicondaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶é€šè¿‡æœ¬åœ°jupyterè¿æ¥è¿œç¨‹æœåŠ¡å™¨çš„æµç¨‹ã€‚\n\n----\n\n## ä¸€ã€å…³äºæœåŠ¡å™¨\n\næœåŠ¡å™¨ç¡¬ä»¶é…ç½®å¦‚ä¸‹ï¼š\n\n| CPU      | XEON 5115*2                    |\n| -------- | ------------------------------ |\n| å†…å­˜     | DDR4 2666 16G*8                |\n| RAIDå¡   | 2GB SAS RAIDå¡                 |\n| GPUå¡    | NV 2080TI*6                    |\n| å›ºæ€ç¡¬ç›˜ | 480G 2.5 SATA 6Gb R SSD        |\n| æœºæ¢°ç¡¬ç›˜ | 1.8TB 2.5å¯¸ 10K 12Gb SASç¡¬ç›˜*5 |\n| ç”µæºæ¨¡å— | 2000W ç”µæºæ¨¡å—X4               |\n\n æ“ä½œç³»ç»Ÿï¼š**Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-128-generic x86_64)**\n\næ¯ä¸ªå›¢é˜Ÿéƒ½åˆ†é…åˆ°äº†è´¦å·ï¼Œç®¡ç†å‘˜åœ¨åˆ›å»ºç›®å½•çš„æ—¶å€™ç»™æ¯ä¸ªå›¢é˜Ÿå»ºç«‹äº†ä¸€ä¸ªä¸»ç›®å½•ï¼Œé€šå¸¸åœ¨/homeä¸‹ï¼Œå›¢é˜Ÿå¯¹è‡ªå·±ä¸»ç›®å½•çš„æ–‡ä»¶æ‹¥æœ‰æ‰€æœ‰æƒï¼Œå¯ä»¥ç”¨äºè¿›è¡Œå„ç§æ“ä½œã€‚ \n\n```shell\nssh teamn@x.x.x.x\n```\n\nè¿™ä¸€æ­¥æ˜¯é€šè¿‡sshè¿œç¨‹è¿æ¥æœåŠ¡å™¨ï¼Œå…¶ä¸­x.x.x.xæ˜¯æœåŠ¡å™¨çš„åœ°å€ï¼Œteamnåˆ™æ˜¯åˆ†é…åˆ°çš„è´¦å·ã€‚ç´§æ¥ç€ä¼šè®©ä½ è¾“å…¥å¯†ç (å…³äºsshçš„è¯¦ç»†ä¿¡æ¯å¯å‚è€ƒ[[1](https://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html)]\n\n```shell\nteamn@x.x.x.x's password: \n```\n\nè¿™é‡Œæ˜¯çœ‹ä¸è§è¾“å…¥çš„å¯†ç é•¿åº¦çš„ï¼Œåªéœ€è¦åœ¨é”®ç›˜ä¸Šç›²æ‰“ç„¶åæŒ‰å›è½¦å°±è¡Œã€‚å¯†ç æ­£ç¡®åä¼šæ˜¾ç¤ºç™»å½•æœåŠ¡å™¨åå’Œè´¦å·ã€‚å¯ä»¥è¾“å…¥å‘½ä»¤**ls**æ¥çœ‹å½“å‰ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶ï¼Œè¿æ¥æ—¶æˆ‘ä»¬å¤„äº/home/teamnçš„ç”¨æˆ·ä¸»ç›®å½•ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨å‘½ä»¤**cd ..**è¿”å›ä¸Šçº§ç›®å½•æˆ–è€…**cd file_name** (file_nameå½“å‰ç›®å½•ä¸‹æŸä¸ªæ–‡ä»¶å¤¹çš„åå­—)æ¥è¿›å…¥åˆ°å­ç›®å½•ä¸­ã€‚\n\n```shell\nnvidia-smi\n```\n\nBTWä¸Šè¿°å‘½ä»¤å¯ä»¥æŸ¥çœ‹GPUçŠ¶æ€\n\n## äºŒã€åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…Linuxç‰ˆAnaconda\n\nè¿™é‡Œæˆ‘ç›´æ¥æœ¬åœ°ä¸‹è½½ç„¶åï¼Œä¸Šä¼ åˆ°æœåŠ¡å™¨çš„ç›®å½•ï¼Œç„¶åæ‰§è¡Œ\n\n```shell\nbash Anaconda3-2020.11-Linux-x86_64.sh\n```\n\nè¿›è¡Œå®‰è£…ï¼Œå®‰è£…å®Œä»¥åå¯ä»¥è¾“å…¥condaè¿›è¡ŒéªŒè¯ã€‚\n\n##  ä¸‰ã€åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\n\n### -è™šæ‹Ÿç¯å¢ƒç®€å•ä»‹ç»\n\nç›®å‰æˆ‘ç†è§£çš„åˆ›å»ºè™šæ‹Ÿç¯å¢ƒçš„åŠ¨æœºä¸»è¦æ˜¯ 1.ä¸åŒé¡¹ç›®æˆ–è€…åº“çš„ä¾èµ–ä¸åŒï¼ŒåŒä¸€ä¸ªåŒ…åœ¨ä¸åŒé¡¹ç›®ä¸­å¯¹åº”ç‰ˆæœ¬çš„ä¸åŒï¼Œå› æ­¤å¾ˆéš¾å…¼å®¹ï¼Œé¢‘ç¹çš„å»upgradeæˆ–è€…downgradeæ˜æ˜¾è¿‡äºç¹çã€‚ 2. ä¸€ä¸ªå›¢é˜Ÿå¤§å®¶å…±ç”¨ä¸€ä¸ªè´¦å·ï¼Œå¦‚æœå…±ç”¨ä¸€ä¸ªç¯å¢ƒæ˜¾ç„¶ä¼šå˜å¾—æ›´åŠ æ··ä¹±ã€‚å› æ­¤åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå°±ååˆ†æœ‰å¿…è¦ï¼Œä½¿ç”¨æ—¶æ¿€æ´»ï¼Œç¯å¢ƒé…ç½®éƒ½åœ¨æ¿€æ´»çš„ç¯å¢ƒä¸­è¿›è¡Œï¼Œç¯å¢ƒä¹‹é—´äº’ä¸å½±å“ï¼Œå¹¶ä¸”å¯ä»¥æœ¬åœ°æˆ–è€…è¿œç¨‹cloneåˆ«äººçš„ç¯å¢ƒæ¥è¿›è¡Œè‡ªå·±çš„éƒ¨ç½²å¼€å‘ã€‚ \n\näº‹å®ä¸Šï¼Œç¯å¢ƒç®¡ç†çš„å·¥å…·å¾ˆå¤šï¼Œæœ‰virtualenv, Pipenv, conda,dockerç­‰ã€‚æˆ‘ä»¬ç†ŸçŸ¥çš„pipæ˜¯åŒ…ç®¡ç†å·¥å…·, virtualenvå¯ä»¥ç®¡ç†ç¯å¢ƒã€‚è€Œcondaä¸¤è€…å…¼æœ‰ï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨conda,condaè¿˜æœ‰minicondaï¼Œå…¶ä¸­åè€…æ›´åŠ è½»ä¾¿ã€‚\n\n### -åˆ›å»ºç¯å¢ƒ\n\nè¿™é‡Œåªä»‹ç»å‡ ç§ç®€å•ç”¨æ³•ï¼Œå…·ä½“å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£[[2](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands)]\n\nåˆ›å»ºåä¸ºmyenvçš„ç¯å¢ƒ\n```shell\nconda create --name myenv\n```\nåˆ›å»ºé»˜è®¤ç¯å¢ƒå¸¦pythonä¸º3.6ç‰ˆæœ¬\n```shell\nconda create -n myenv python=3.6\n```\nåˆ›å»ºå¸¦æœ‰scipyåº“çš„é»˜è®¤ç¯å¢ƒ\n```shell\nconda create -n myenv scipy\nor\nconda create -n myenv python\nconda install -n myenv scipy\n```\n\n### -å¸¸ç”¨å‘½ä»¤\n\n```conda list```\n\nç±»ä¼¼äºpip list,å¯ä»¥æŸ¥çœ‹å½“å‰ç¯å¢ƒç›®å½•å®‰è£…çš„æ‰€æœ‰åº“\n\n```conda info -e```\n\næŸ¥çœ‹åˆ›å»ºçš„æ‰€æœ‰ç¯å¢ƒï¼Œä¼šæ˜¾ç¤ºæ‰€æœ‰ç¯å¢ƒçš„åå­—\n\n```conda activate env_name```\n\næ¿€æ´»ç¯å¢ƒ, env_nameæ˜¯ç¯å¢ƒçš„åå­—\n\n```conda deactivate env_name```\n\nå…³é—­ç¯å¢ƒ\n\n## å››ã€æœ¬åœ°è¿æ¥æœåŠ¡å™¨ç«¯Jupyter notebook\n\nè¿™é‡Œç®€è¿°å®ç°æµç¨‹ï¼Œå…·ä½“å†…å®¹ä¹Ÿå¯å‚è€ƒå®˜æ–¹æ–‡æ¡£[[3](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html)]\n\n### - åœ¨æœåŠ¡å™¨ç«¯é…ç½®jupyter configuration\n\n**ç¬¬ä¸€æ­¥,**ç”Ÿæˆjupyterç¬”è®°æœ¬çš„é»˜è®¤é…ç½®æ–‡ä»¶ï¼Œå¦‚æœå·²ç»å­˜åœ¨ä¼šæç¤ºä½ æ˜¯å¦è¦åˆå§‹åŒ–ã€‚\n\n```shell\njupyter notebook --generate-config\n```\n\n**ç¬¬äºŒæ­¥ï¼Œ**è®¾ç½®ç¬”è®°æœ¬çš„ç™»å½•å¯†ç (å…å¯†ç™»å½•çš„è®¾ç½®å‚è€ƒå®˜æ–¹æ–‡æ¡£)\n\n```shell\njupyter notebook password\n---------------------------\nEnter password:  ****\nVerify password: ****\n[NotebookPasswordApp] Wrote hashed password to /Users/you/.jupyter/jupyter_notebook_config.json\n```\n\n**ç¬¬ä¸‰æ­¥ï¼Œ**ä¿®æ”¹jupyterç¬”è®°æœ¬é…ç½®æ–‡ä»¶ï¼Œç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€jupyter_notebook_config.pyæ–‡ä»¶ï¼Œå°†å…¶ä¸­å¯¹åº”è¡Œæ³¨é‡Šç¬¦#å»æ‰ä¿®æ”¹å­—æ®µæˆ–è€…ç›´æ¥åœ¨æ–‡ä»¶ä»»ä¸€ä½ç½®æ·»åŠ :\n\n```shell\nc.NotebookApp.ip = '*'\nc.NotebookApp.open_browser = False\n```\n\nè¿™é‡Œæ–‡æœ¬ç¼–è¾‘å™¨ä½¿ç”¨vim.å…³äºvimçš„æ“ä½œå¯ä»¥å‚è€ƒ[4](https://www.ruanyifeng.com/blog/2018/09/vimrc.html)\n\n### - åœ¨æœåŠ¡å™¨ç«¯å¼€å¯jupyter server\n\nÂ å®ŒæˆåŸºæœ¬è®¾ç½®å¥½ï¼Œç„¶ååœ¨ä¸­bashè¾“å…¥\n\n```jupyter notebook  ```\n\né»˜è®¤çš„ç«¯å£ä¸º8888ï¼Œä½†æ˜¯æœ‰æ—¶å€™å·²ç»æœ‰äººä½¿ç”¨äº†è¯¥ç«¯å£ï¼Œä¸ºäº†é¿å…å†²çªå¯ä»¥æŒ‡å®šå¼€å¯ç«¯å£å·\n\n```shell\njupyter notebook --port YYYY(å››ä½ä»»æ„)\n```\n\nå¦‚æœåœ¨æœ¬æœºè¿™æ ·æ“ä½œä¼šç›´æ¥åœ¨æµè§ˆå™¨è·³å‡ºjupyter webé¡µé¢ï¼Œåœ¨æœåŠ¡å™¨ç«¯å› ä¸ºæ²¡æœ‰GUIå¹¶ä¸”åœ¨configä¸­è®¾ç½®äº† no browserï¼Œå› æ­¤  bash ä¼šæç¤ºserver è¿è¡Œåœ¨Bç«¯å£ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥æ”¾ç€å…ˆä¸ç®¡äº†ã€‚\n\n### -åˆ›å»ºæœ¬åœ°åˆ°æœåŠ¡å™¨ç«¯çš„æ˜ å°„\n\näº‹å®ä¸Šä¹Ÿå¯ä»¥æ”¹jupyter configçš„ä¸€äº›é…ç½®ï¼Œç›´æ¥è¿œç¨‹è®¿é—®è¯¥jupyterï¼ˆremote address:YYYYï¼‰ï¼Œè¿™é‡Œç”±äºæŸäº›åŸå› æˆ‘å¤±è´¥äº†æš‚æ—¶é€‰æ‹©äº†å¦‚ä¸‹æ–¹æ³•ã€‚\n\nåœ¨æœ¬åœ°ç»ˆç«¯è¾“å…¥:\n\n```ssh -N -f -L localhost:XXXX:localhost:YYYY  team6@192.168.156.31```\n\n-N å‘Šè¯‰SSHæ²¡æœ‰å‘½ä»¤è¦è¢«è¿œç¨‹æ‰§è¡Œ\n\n-f å‘Šè¯‰SSHåœ¨åå°æ‰§è¡Œ\n\n-L æŒ‡å®šport forwardingçš„é…ç½® è¿œç¨‹ç«¯å£æ˜¯YYYY æœ¬åœ°æ˜¯XXXX\n\nç›¸å½“äºä¸€ä¸ªæ˜ å°„ï¼ŒæŠŠæœåŠ¡å™¨åœ°å€æ˜ å°„åˆ°Â localhostçš„ä¸€ä¸ªç«¯å£ã€‚\n\n---\n\néšåæ‰“å¼€ä»»æ„æµè§ˆå™¨åœ°å€æ è¾“å…¥localhost:XXXXå¹¶è¾“å…¥å¯†ç  xxxxï¼ˆè¿™æ˜¯æˆ‘ä¹‹å‰åœ¨jupyter configé‡Œè®¾ç½®çš„ï¼‰,å³å¯è¿›å…¥ ã€‚\n\nè¿™é‡Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨jupyterä¸Šè¿è¡ŒShellå‘½ä»¤ï¼Œåªéœ€è¦åœ¨ä»£ç è¿è¡Œçš„cellå‰é¢åŠ ! ,   å¦‚æœéœ€è¦äº¤äº’ å¯ä»¥åœ¨æœ«å°¾åŠ --yes æˆ–è€…-- yes *\n\n## äº”ã€ä¸Šä¼ ä¸‹è½½æ–‡ä»¶\n\nå¦‚æœæ˜¯ä½¿ç”¨äº†è¿œç¨‹ç»ˆç«¯è½¯ä»¶ï¼Œå›¾å½¢ç•Œé¢å†…è‡ªå¤‡äº†å¯è§†åŒ–æ¥å£å°±å¯æ–¹ä¾¿ä¸Šä¼ ä¸‹è½½æ–‡ä»¶ã€‚åœ¨bashä¸­æˆ‘ä»¬å¯ä»¥ä½¿ç”¨shellå‘½ä»¤scpè¿›è¡Œæ“ä½œã€‚\n\nscp local_dir teamn@192.168.x.x:remote_dir\n\n## References\n\n[1]https://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html\n\n[2]https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands\n\n[3]https://jupyter-notebook.readthedocs.io/en/stable/public_server.html\n\n[4]https://www.ruanyifeng.com/blog/2018/09/vimrc.html","tags":["Linux","Server","Environment"],"categories":["Linux","Server"]},{"title":"Lambda function","url":"/2021/04/23/Lambda-function/","content":"\n<!-- more -->\n\n## Basic idea\n\nç¬¬ä¸€æ¬¡æ¥è§¦åˆ°åŒ¿åå‡½æ•°è¿˜æ˜¯åœ¨å­¦JAVAçš„æ—¶å€™ï¼Œå®ƒèƒ½è®©ä»£ç å˜å¾—ç®€æ´(pithy anonymity )ã€‚åœ¨Pythonä¸­å…¶ä¸»è¦æ˜¯ç”±ä¿ç•™å­—(keyword)ä¸­çš„Lambdaå®ç°çš„ã€‚å®ƒçš„åŸç†å¾ˆç®€å•ï¼Œä½†æ˜¯æœ‰å¾ˆå¤šç”¨æ³•ï¼Œé…åˆå…¶ä»–çš„è¯­æ³•å¾€å¾€æœ‰å¥‡æ•ˆã€‚\n\n\n\näº‹å®ä¸Šï¼Œåœ¨ä¹‹å‰çš„åšæ–‡{% post_link 'Lloyd-Max Quantizer' %}ä¸­ä»£ç å®ç°éƒ¨åˆ†çš„ç¬¬7ï¼Œ8è¡Œå°±ä½¿ç”¨åˆ°äº†è¿™ä¸ªè¡¨è¾¾ï¼Œå½“æ—¶çš„contextæ˜¯è¦å¯¹ä¸€ä¸ªæœä»æ­£æ€åˆ†å¸ƒçš„éšæœºå˜é‡è¿›è¡Œç§¯åˆ†ï¼Œè€Œç§¯åˆ†å‡½æ•°quadéœ€è¦è¾“å…¥ç§¯åˆ†å½¢å¼çš„å‚æ•°ä»¥è¿›è¡Œç§¯åˆ†ã€‚æ˜¾ç„¶ï¼Œlambdaè¡¨è¾¾å¼è®©ä»£ç å˜å¾—ååˆ†ç®€æ´æ˜“è¯»ã€‚\n\n---\n\n{% asset_img lambda.png %}\n\nä¸Šå›¾å®˜æ–¹docå¯¹Lambdaçš„ä»‹ç»ï¼Œlambda_exprç”¨æ¥å£°æ˜åŒ¿åå‡½æ•°ï¼Œç„¶å```lambda parameters: expression```è¯­å¥ä¸ºå…¶ç”Ÿæˆä¸€ä¸ªå‡½æ•°å¯¹è±¡ï¼Œå®ƒä¸ä¸Šå›¾ä¸­defå‡½æ•°ä½œç”¨ç›¸åŒã€‚\n\n## Usage\n\nä¸‹é¢ç®€å•ä»‹ç»ä¸‹å¸¸è§çš„ç”¨æ³•\n\n## 1. å•ä¸€å‚æ•°\n\nå®ç°2*x+1çš„è¿ç®—\n\n```python\ndef f(x):\n  return x*2+1\nprint(f)\nprint(lambda x: x*2+1)\nprint('def way:',f(2))\nprint('lambda way',(lambda x: x*2+1)(2))\ng=lambda x: x*2+1\nprint(g(2))\n-----------------------------------------------\n# output\n'<function f at 0x7f4cbe4d2050>'\n'<function <lambda> at 0x7f4cbe4d2440>'\n'def way: 5'\n'lambda way: 5'\n'5'\n\n```\n\né€šè¿‡ä¸Šå›¾ï¼Œå¯ä»¥çœ‹åˆ°lambdaè¡¨è¾¾å¼å¼€è¾Ÿäº†ä¸€ä¸ªå‡½æ•°ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©èµ‹äºˆå…¶ä¸€ä¸ªå‡½æ•°åæ¯”å¦‚ä¸Šé¢çš„gï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨å¦‚è¡¨è¾¾å¼```(lambda x: x*2+1)(2)```\n\n---\n\n## 2. å¤šå‚æ•°æˆ–è€…æ— å‚æ•°\n\n### å¤šå‚æ•°\n\nå®ç°åå’Œå§“çš„åˆå¹¶è¾“å‡º\n\n```python\nname=lambda fn,ln: fn.strip().title()+\" \"+ln.strip().title()\nprint(name('LAST  ','   XUAN'),'\\n',name('shiny  ','   ruo'))\n-----------------------------------------------\n# output\n'Last Xuan'\n'Shiny Ruo'\n```\n\nå…¶ä¸­stripå’Œtitleæ˜¯ä¸ºäº†è‡ªåŠ¨çº æ­£ä¸è§„èŒƒè¾“å…¥çš„å‡½æ•°ï¼Œå‰è€…å»æ‰å†…å®¹æ”¶å°¾å†—ä½™çš„ç©ºæ ¼åè€…è®©é¦–å­—æ¯å¤§å†™å…¶ä½™å°å†™ã€‚\n\n### æ— å‚æ•°\n\nlambdaè¡¨è¾¾å¼ä¸æ·»åŠ å‚æ•°ï¼Œç›¸å½“äºè¿‡ç¨‹å‡½æ•°ï¼Œå‡½æ•°æ‰§è¡Œå‡½æ•°ä½“å†…çš„å¥å­ä½†ä¸è¿”å›ä»»ä½•å€¼ã€‚\n\n```python\nprocess_func=lambda : print('nothing to return')\nprocess_func()\n-----------------------------------------------\n# output\n'nothing to return'\n```\n\n### å†…åµŒäºå…¶ä»–å‡½æ•°\n\näº‹å®ä¸Šå‡½æ•°ä¹Ÿå¯ä»¥ä½œä¸ºå‡½æ•°çš„å‚æ•°ï¼Œä¸€å¼€å§‹quadå‡½æ•°å°±æ˜¯è¿™ç§æƒ…å†µçš„ä¸€ä¸ªå®ä¾‹ã€‚è¿™ç§æƒ…å†µä¸‹æ‰€éœ€çš„å‡½æ•°å¾€å¾€å¹¶ä¸æ˜¯å¾ˆå¤æ‚ï¼Œä½†æ˜¯åˆéœ€è¦æœ‰ä¸€å®šçš„çµæ´»æ€§ï¼Œé‚£ä¹ˆlambdaå‡½æ•°å°±æ˜¾å¾—å¾ˆä¾¿æ·ã€‚å¦‚å¸¸ç”¨çš„sort,filter,map,reduceç­‰ï¼Œå…·ä½“çš„ç”¨æ³•åœ¨ä¸‹ä¸€æ¬¡æ›´æ–°~\n\n\n\n","tags":["Python","Lambda function"],"categories":["Programming","Python","Lambda function"]},{"title":"Interpretable Machine Learning(LIME-1)","url":"/2021/04/15/Interpretable Machine Learning/","content":"\n## å…³äºLIME\nå› ä¸ºç ”ç©¶éœ€è¦ï¼Œå¾—å¼„æ‡‚kernel SHAPæ‰€ä»¥å…ˆå¾—å¼„æ˜ç™½LIMEï¼Œä¸æƒ³è¿™ä¸œè¥¿è¿˜æŒºæœ‰æ„æ€çš„ï¼Œè¯¥ç®—æ³•å‘è¡¨åœ¨2016çš„KDDä¸Šï¼Œå…ˆæŒ‚ä¸ªä»‹ç»è§†é¢‘å§ã€‚\n\n{% youtube hUnRCxnydCc %}\nè§†é¢‘ç®€å•å½¢è±¡ä»‹ç»äº†LIMEä»¥åŠè¯¥ç®—æ³•çš„motivationå’Œintuition. æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨LIMEå»1) åœ¨å‡ ä¸ªæ——é¼“ç›¸å½“ï¼ˆæ€§èƒ½ç›¸ä¼¼ï¼‰çš„æ¨¡å‹ä¸­åšé€‰æ‹©ã€‚2ï¼‰å»é‰´åˆ«ä¸å€¼å¾—ä¿¡ä»»çš„æ¨¡å‹å¹¶æ”¹å–„ã€‚3ï¼‰ä»æ¨¡å‹ä¸­å¾—åˆ°æ–°çš„å‘ç°çµæ„Ÿã€‚\n\nå…·ä½“ä¸€ç‚¹ï¼Œ\n1ï¼‰çš„åº”ç”¨ä¸»è¦æ˜¯åœ¨æ»¡è¶³metricéœ€è¦çš„æ¨¡å‹ä¹‹é—´æ‰¾åˆ°æ›´é€‚åˆéœ€æ±‚çš„æ¨¡å‹ï¼Œæ¯”å¦‚æœ‰çš„æ¨¡å‹è™½ç„¶perform wellä½†æ˜¯è§£é‡Šæ€§ä¸€å›¢ç³Ÿï¼Œåˆæ¯”å¦‚æœ‰çš„è¯­è¨€æ¨¡å‹æ¶‰å«Œç§æ—æ­§è§†â€¦â€¦\n\n2ï¼‰æœ‰ä¸€äº›æ¨¡å‹perform beyond expectationï¼Œæœ‰å¾ˆå¤§å«Œç–‘å‘ç”Ÿäº†data leakageï¼ˆæˆ‘å°±è¢«è¿™ä¸ªå‘æƒ¨äº†ï¼‰ï¼Œæ¯”å¦‚è¯´ç”¨æ¥é‰´åˆ«å­¦ç”Ÿå±äºå“ªä¸ªç­çº§ï¼Œæ¨¡å‹å°†å­¦ç”ŸIDä½œä¸ºç‰¹å¾ï¼Œè€Œç”±IDå¯ç›´æ¥æ¨å‡ºå­¦ç”Ÿç­çº§ã€‚åˆæ¯”å¦‚å›¾åƒé¢†åŸŸï¼Œè¯†åˆ«åŒ—æç†Šå’Œæ£•ç†Šï¼Œæ¨¡å‹å°†é›ªåœ°èƒŒæ™¯ä½œä¸ºåˆ¤åˆ«image æ˜¯å¦ä¸ºåŒ—æç†Šçš„é‡è¦ç‰¹å¾ã€‚è¿™äº›æ¨¡å‹è™½ç„¶è¡¨ç°çš„å¾ˆå¥½ä½†æ˜¯å´æ¯«æ— æ„ä¹‰ï¼ˆæœ¬è´¨ä¸ºè¿‡æ‹Ÿåˆï¼‰ï¼Œåœ¨éƒ¨ç½²ä¸Šçº¿åä¼šå˜å¾—ä¸€å¡Œç³Šæ¶‚ã€‚\n\n3ï¼‰è¿™æ–¹é¢åº”ç”¨å°±æ¯”è¾ƒçµæ´»äº†ï¼Œå¯ä»¥ç”¨äºå¼‚å¸¸æ£€æµ‹ï¼Œä¹Ÿå¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©æˆ–è€…æ„å»ºæ–°çš„powerful feature....\n\n## LIMEç®—æ³•\n### IDEA\nLIMEï¼ˆLocal Interpretable Model-Agnostic Explanations )å±äºå±€éƒ¨ä»£ç†æ¨¡å‹ï¼Œæ˜¯ä¸€ç§å¯è§£é‡Šçš„æ¨¡å‹ç”¨äºè§£é‡Šé»‘ç›’æœºå™¨æ¨¡å‹å¯¹å•ä¸ªå®ä¾‹ï¼ˆindividualï¼‰çš„é¢„æµ‹ã€‚å®ƒçš„æƒ³æ³•éå¸¸ç›´è§‰ï¼Œé¦–å…ˆæˆ‘ä»¬ä»…ä¿ç•™è®­ç»ƒå¥½çš„é»‘ç›’æ¨¡å‹ï¼Œç„¶åæ‰°åŠ¨æ•°æ®ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Œé€šè¿‡é»‘ç›’æ¨¡å‹å¾—åˆ°è¿™äº›æ ·æœ¬çš„é¢„æµ‹å€¼ä½œä¸ºLIME explainerçš„labelï¼Œè®­ç»ƒLIME explainerï¼Œç”±äºexplainerå¯¹æ¯”åŸæ¥çš„é»‘ç›’æ¨¡å‹æ›´åŠ ç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®ƒä½œä¸ºåŸå§‹é»‘ç›’æ¨¡å‹çš„ä»£ç†å¯¹æ„Ÿå…´è¶£çš„æ ·æœ¬ç‚¹è¿›è¡Œè§£é‡Šå’Œåˆ†æã€‚\n\nå®é™…ä¸Šï¼Œexplainerå¯ä»¥æ˜¯ä»»ä½•æ¨¡å‹ï¼Œä½†æ˜¯å› ä¸ºå¤æ‚åº¦çš„å› ç´ ï¼ŒLassoï¼ˆlinear regression with L1ï¼‰å’Œdecision treeé€šå¸¸è¢«é€‰ä½œexplainer.\n\n### Mathematics \n\næ•°å­¦ä¸Šï¼Œå¸¦æœ‰æ¨¡å‹å¤æ‚åº¦ï¼ˆå¯è§£é‡Šæ€§ï¼‰æ­£åˆ™é¡¹é™åˆ¶çš„å±€éƒ¨ä»£ç†æ¨¡å‹\n$$\n\\operatorname{explanation}(x)=\\arg \\min _{g \\in G} L\\left(f, g, \\pi_{x}\\right)+\\Omega(g)\n$$\nå…¶ä¸­få‡½æ•°ä»£è¡¨å¾…è§£é‡Šçš„black-box model, gå‡½æ•°åˆ™æ˜¯åœ¨Gå‡½æ•°ç©ºé—´ä¸­çš„ä¸€ä¸ªè§£é‡Šæ€§æ¨¡å‹ï¼Œ$Ï€_x$ä»£è¡¨æ„Ÿå…´è¶£æ ·æœ¬xçš„é‚»æ ·æœ¬èŒƒå›´çš„å¤§å°ã€‚\n\n\n\næ˜¾ç„¶ï¼Œ$Ï€_x$å¦‚æœè¶Šå¤§ï¼Œåˆ™æœ‰è¶Šå¤šçš„è¿œç¦»interest pointçš„å®ä¾‹è¢«ç”¨äºæ„å»ºlocal surrogate explainerï¼Œå¯èƒ½ä¼šå¼•å…¥ä¸€äº›æ–°çš„è§£é‡Šã€‚å…¶æ¬¡å…³äºæ­£åˆ™é¡¹$\\Omega(g)$ï¼Œæˆ‘ä¸ªäººçš„ç†è§£æ˜¯Gç©ºé—´ä¸­ä¼šæœ‰å¾ˆå¤šåœ¨æŸå¤±å‡½æ•°ä¸Šè¡¨ç°ç›¸å½“çš„å‡½æ•°ï¼Œæˆ‘ä»¬è¦ä»ä¸­é€‰å–é‚£äº›å¤æ‚åº¦ä½ï¼Œè§£é‡Šæ€§å¥½çš„ã€‚å…·ä½“çš„ï¼Œè¯¥é¡¹å¯ä»¥ç”¨äºheterogeneous modelsä¹‹é—´çš„é€‰æ‹©ï¼Œæ¯”å¦‚å†³ç­–æ ‘å’Œçº¿æ€§æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç”¨äºhomogeneous modelsä¹‹é—´çš„é€‰æ‹©ï¼Œä¸åŒç‰¹å¾æ•°é‡çš„çº¿æ€§æ¨¡å‹æˆ–è€…ä¸åŒæ·±åº¦ä¸åŒå¶å­èŠ‚ç‚¹æ•°ç›®çš„å†³ç­–æ ‘ç­‰....\n\n\n\nä½†æ˜¯è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬åªå¯¹æŸå¤±å‡½æ•°é¡¹è¿›è¡Œä¼˜åŒ–ï¼Œå¤æ‚åº¦çš„æ­£åˆ™é¡¹æ˜¯é€šè¿‡æˆ‘ä»¬é¢„å…ˆé™åˆ¶æ¨¡å‹çš„å¤æ‚åº¦æ¥å¾—åˆ°çš„ã€‚\n\n### Recipe for raining local surrogate models\n\nå…¶ç®—æ³•æ‰§è¡Œçš„æµç¨‹å¤§è‡´å¦‚ä¸‹:\n\n\n1ï¼‰ é€‰æ‹©æ„Ÿå…´è¶£çš„å®ä¾‹ï¼ˆç»ç”±é»‘ç›’æ¨¡å‹é¢„æµ‹çš„æŸä¸ªå®ä¾‹ï¼‰\n\n2ï¼‰æ‰°åŠ¨æ•°æ®é›†ï¼ˆé‡‡æ ·ï¼‰å¾—åˆ°æ–°çš„æ ·æœ¬ï¼Œå¹¶è¾“å…¥åˆ°é»‘ç›’æ¨¡å‹ä¸­å¾—åˆ°å…¶é¢„æµ‹å€¼ä½œä¸ºå…¶æ ‡ç­¾ã€‚\n\n3ï¼‰å¯¹è¿™äº›æ–°çš„æ ·æœ¬æ ¹æ®ä¸æ„Ÿå…´è¶£å®ä¾‹çš„æ¥è¿‘ç¨‹åº¦ï¼ˆç±»ä¼¼ç‰¹å¾å‘é‡çš„æ¬§æ°è·ç¦»ï¼‰æ¥è¿›è¡Œèµ‹æƒã€‚\n\n4ï¼‰åŸºäºæ–°çš„æ ·æœ¬è®­ç»ƒå¯è§£é‡Šæ¨¡å‹\n\n5ï¼‰é€šè¿‡å¯è§£é‡Šæ¨¡å‹è§£é‡Šæ„Ÿå…´è¶£çš„å®ä¾‹\n\n\n\n\n\n","tags":["Machine learning","Interpretable Machine Learning","LIME"],"categories":["Machine learning","Interpretable Machine Learning"]},{"title":"List comprehension","url":"/2021/04/12/list-comprehension/","content":"\n<!-- more -->\n\næœ€è¿‘å­¦ä¹ äº†åˆ—è¡¨è§£æå¼(List comprehension )ï¼Œå®ƒå±äºPythonä¸­çš„è¯­æ³•ç³–(Syntactic Sugar)ã€‚è¯­æ³•ç³–çš„å‡ºç°ä¸»è¦æ˜¯ä¸ºäº†å†™ç¨‹åºçš„æ—¶å€™èƒ½å°‘å‡ºé”™å¹¶ä¸”ä»£ç å¯ä»¥æ›´ç®€æ´ã€‚è¿™ç¯‡é€šè¿‡LeetCodeçš„17. Letter Combinations of a Phone Numberçš„ä¸€ä¸ªè§£æ³•å¼•å‡ºè¿™ä¸ªè¡¨è¾¾ã€‚\n\n---\n\n# å¼•ä¾‹\n\né¢˜ç›®ä¸èµ˜è¿°äº†ï¼Œç»™å‡ºé“¾æ¥[Letter Combinations of a Phone Number](https://leetcode.com/problems/letter-combinations-of-a-phone-number/)\n\næ¯”è¾ƒè®¤åŒçš„ä¸€ä¸ªè§£æ³•å¦‚ä¸‹:\n\n```python\nclass Solution:\n    def letterCombinations(self, digits: str) -> list:\n        graph = {'2':['a','b','c'], '3':['d','e','f'], '4':['g','h','i'], '5':['j','k','l'], '6':['m','n','o'], '7':['p','q','r','s'], '8':['t','u','v'], '9':['w','x','y','z']}\n        ans=['']\n        if digits==\"\":\n            return [] \n        for i in digits:\n            chars.append(graph[i])\n        for i in range(len(chars)):\n            ans=[c+chars[i][j] for c in ans for j in range(len(chars[i]))]\n        return ans\n```\n\né¦–å…ˆå­—å…¸å­˜å‚¨ä»¥æ•°å­—ä¸ºé”®å¯¹åº”å­—æ¯åˆ—è¡¨ä¸ºå€¼çš„ä¸€ç³»åˆ—itemã€‚æˆ‘ä»¬è¦åšçš„æ˜¯æŠŠæ‰€è¦æ±‚çš„digitså¯¹åº”çš„å¯èƒ½å­—æ¯ç»„åˆå…¨éƒ¨æ‰¾å‡ºæ¥ã€‚\n\n\n\nå…ˆæŠŠæ¯ä¸ªæ•°å­—å¯¹åº”çš„å­—æ¯æ‹¼æ¥åœ¨ä¸€ä¸ªæ•°ç»„é‡Œå³charsï¼Œéœ€è¦æ³¨æ„çš„æ˜¯æ­¤æ—¶çš„charsæ˜¯ä¸ªäºŒç»´æ•°ç»„ï¼Œå› æ­¤len(chars)è¿”å›çš„ä¸ºdigitsçš„é•¿åº¦,ä¹Ÿå³ç»„åˆä¸­ä»»ä¸€å…ƒç´ çš„é•¿åº¦ã€‚\n\n\n\nç´§æ¥ç€ä¸ºäº†å¾—åˆ°å¯èƒ½çš„æ‰€æœ‰ç»„åˆï¼Œæˆ‘ä»¬éœ€è¦æ˜¾å¼çš„å»è®¾è®¡å¾ªç¯ï¼Œè€Œè§£æ³•ä¸­ä¸€ä¸ªåˆ—è¡¨è§£æå¼å°±å®Œæˆäº†æ‰€æœ‰æ“ä½œï¼Œå¾ˆç®€æ´ä¹Ÿå¾ˆä¼˜é›…ã€‚å…·ä½“çš„é€»è¾‘åœ¨æ–‡æœ«ç»™å‡ºã€‚\n\n-----\n\n## å…³äºList comprehension\n\nå½“æˆ‘ä»¬å®šä¹‰æœ‰å†…å®¹çš„listçš„æ—¶å€™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¾å…¥å…ƒç´ å‰åšä¸€äº›è®¡ç®—çš„æ—¶å€™ï¼Œæˆ‘ä»¬é™¤äº†ä½¿ç”¨for å¾ªç¯æ¥æ·»åŠ åˆ—è¡¨å…ƒç´ ï¼Œè¿˜å¯ä»¥åœ¨åˆ—è¡¨å†…ç›´æ¥å†™è§£æå¼è®¡ç®—ã€‚\n\n## List comprehension ç”¨æ³•\n\n### 1. [ expression for i in iterable ]\n\n```python\nans=[i+1 for i in range(10)]\nprint('ans:',ans)\n----------------------------\nans: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n```\n\nå¯¹äºè¿­ä»£å¯¹è±¡è¿›è¡ŒåŠ ä¸€è¿ç®—å¹¶å­˜æ”¾åœ¨ansæ•°ç»„ä¸­ å…¶ä¸­iterable object: range(10) , expression: i+1ã€‚\n\n### 2. [ expression for i in iterable if...]\n\nå¦‚æœæˆ‘ä»¬éœ€è¦å¯¹è¿›å…¥target listä¸­çš„å…ƒç´ è¿›è¡Œç­›é€‰ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹è¿­ä»£è¡¨è¾¾å¼çš„æœ«å°¾åŠ å…¥if è¯­å¥\n\n```python\nans=[i+1 for i in range(10) if i%2==0]\nprint('ans:',ans)\n----------------------------\nans: [1, 3, 5, 7, 9]\n```\n\nä¹‹æ‰€ä»¥é€‰è¿™ä¸ªä¾‹å­ï¼Œæ˜¯ä¸ºäº†ä½¿å¾—è¡¨è¾¾å¼çš„é€»è¾‘å˜å¾—æ›´åŠ æ¸…æ™°ã€‚é€šè¿‡ifè¯­å¥```if i%2==0```æˆ‘ä»¬é€‰å–äº†0~9ä¸­ä¸ºå¶æ•°çš„å…ƒç´ è¿›è¡Œè¡¨è¾¾è¿ç®—ï¼Œ+1ä»¥åå¾—åˆ°```ans: [1, 3, 5, 7, 9]```\n\n### 3. [ expression for i in iterable ifâ€¦ for j in iterable ifâ€¦ â€¦]\n\næ›´å¤æ‚ç‚¹ï¼ŒåŒå¾ªç¯+ifï¼Œ å®ç°äº†5ä»¥å†…ï¼ˆå¤æ•°ï¼Œå•æ•°ï¼‰çš„æ‰€æœ‰ç»„åˆ\n\n```python\nans=[(i,j) for i in range(5) if i%2==0 for j in range(5) if j%2==1]\nprint('ans:',ans)\n----------------------------\nans: [(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]\n```\n\nå®ƒå®ç°çš„é€»è¾‘å¦‚ä¸‹ï¼š\n\n```python\nans=[]\nfor i in range(5):\n    if i%2==1:continue\n    for j in range(5):\n        if j%2==1:ans.append((i,j))\nprint('ans:',ans)\n----------------------------\nans: [(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]\n```\n\n## å°ç»“\n\nå¯ä»¥çœ‹å‡ºæ¥ï¼Œäº‹å®ä¸Šåˆ—è¡¨è¡¨è¾¾å¼æŠŠæ˜¾å¼çš„forå¾ªç¯å’Œifè¯­å¥ç®€åŒ–äº†ã€‚å®é™…ä¸­ï¼Œexpressionå¯¹æ¯ä¸€è½®è¿­ä»£å¾ªç¯ç»“æŸåå¾—åˆ°çš„å…ƒç´ è¿›è¡Œæ“ä½œã€‚\n\n---\n\n# å›åˆ°å¼•ä¾‹\n\n```python\nfor i in range(len(chars)):\n    ans=[c+chars[i][j] for c in ans for j in range(len(chars[i]))]\n```\n\nè¿™æ—¶å€™å›åˆ°å¼•ä¾‹ï¼Œ\n\n{% asset_img Telephone-keypad.png %}\n\nå†æ¬¡å¼ºè°ƒforå¾ªç¯è¿­ä»£çš„æ¬¡æ•°ç­‰äºdigitså­—ç¬¦ä¸²çš„é•¿åº¦ï¼Œä¹Ÿå³æœ€åansä¸­æ¯ä¸ªå…ƒç´ çš„é•¿åº¦ã€‚\n\n\n\næ¥ä¸‹æ¥åˆ†æåœ¨æ¯è½®è¿­ä»£ä¸­åˆ—è¡¨è§£æå¼ç©¶ç«Ÿåšäº†äº›ä»€ä¹ˆ\n\n## å¾ªç¯1\n\né¦–å…ˆç¬¬ä¸€å±‚çš„å¾ªç¯æ˜¯å¯¹äºansä¹Ÿå°±æ˜¯æˆ‘ä»¬target listè¿›è¡Œçš„ï¼Œå˜é‡cå­˜å‚¨å½“å‰éå†å…ƒç´ ã€‚\n\n## å¾ªç¯2\n\nç„¶åç¬¬äºŒå±‚å¾ªç¯æ˜¯å¯¹äº len(chars[i])è¿›è¡Œçš„å®ƒè¿”å›çš„æ˜¯digitä¸­æŸä¸ªæ•°å­—å¯¹åº”çš„å¯èƒ½å­—ç¬¦ä¸ªæ•°ï¼ˆå¦‚å›¾æ•°å­—2æœ‰3ä¸ªå¯¹åº”çš„å­—æ¯ï¼Œè€Œæ•°å­—7å’Œ9æœ‰4ä¸ª), å˜é‡jå­˜å‚¨å½“å‰éå†ç´¢å¼•ã€‚\n\n---\n\n## è¡¨è¾¾å¼\n\n```c+chars[i][j]```chars[i]è¡¨ç¤ºçš„æ˜¯digitsä¸­æ•°å­—çš„ç´¢å¼•ï¼Œjè¡¨ç¤ºçš„æ˜¯è¯¥æ•°å­—å¯¹åº”çš„å¯èƒ½çš„å­—æ¯çš„ç´¢å¼•ï¼ˆå¯¹äºdigits'27',å½“i=0çš„æ—¶å€™ï¼Œjä¼šä»0éå†åˆ°2;å½“i=7çš„æ—¶å€™ï¼Œjä¼šä»0éå†3ï¼‰ã€‚å› æ­¤ï¼Œæ€è€ƒä¸‹å°±èƒ½å¾—çŸ¥ï¼Œå®é™…ä¸Šï¼Œæ¯è½®forå¾ªç¯target listä¸­å­˜æ”¾çš„æ˜¯å‰i+1ä¸ªæ•°å­—çš„å¯èƒ½çš„å­—æ¯ç»„åˆã€‚åˆ—è¡¨è§£æå¼æ‰€å®ç°çš„åŒé‡å¾ªç¯åœ¨å¢åŠ target listä¸­å…ƒç´ çš„ä¸ªæ•°çš„åŒæ—¶é€šè¿‡expressionå¢åŠ å…ƒç´ çš„é•¿åº¦ã€‚\n\n","tags":["Python","List comprehension","LeetCode"],"categories":["Programming","Python","List comprehension"]},{"title":"åŒæŒ‡é’ˆç±»å‹é¢˜è§£ï¼ˆä¸€ï¼‰","url":"/2021/04/07/åŒæŒ‡é’ˆç±»å‹é¢˜è§£ï¼ˆä¸€ï¼‰/","content":"\n<!-- more -->\n\n# å¿«æ…¢æŒ‡é’ˆ\n\nå¤§æ„æ˜¯æŠŠå•é“¾è¡¨ä¸­å€’æ•°ç¬¬nä¸ªç»“ç‚¹ç»™å‰”é™¤ï¼Œé“¾è¡¨ä¸åŒäºæ•°ç»„å¹¶æ²¡æœ‰æ˜¾å¼çš„ç»™å‡ºé•¿åº¦ï¼Œä¹Ÿä¸èƒ½ç®€å•é€šè¿‡ç´¢å¼•å®šä½ã€‚å› æ­¤æ‰¾åˆ°ç›®æ ‡ç»“ç‚¹å°±éœ€è¦ä¸€äº›ç‰¹æ®Šçš„trickï¼Œäº‹å®ä¸Šä¹Ÿæœ‰ç€å¾ˆå¤šå¥‡å¦™çš„è§£æ³•ã€‚ä½†æ˜¯åœ¨è¿™é‡Œï¼Œå¿«æ…¢æŒ‡é’ˆæ˜¯æ¯”è¾ƒå®¹æ˜“æƒ³åˆ°ä¹Ÿæ¯”è¾ƒé«˜æ•ˆçš„è§£æ³•ã€‚å¿«æ…¢æŒ‡é’ˆçš„æ–¹æ³•å¾ˆç®€å•å°±æ˜¯åˆå§‹åŒ–ä¸¤ä¸ªæŒ‡é’ˆï¼Œåœ¨æ¯ä¸€è½®çš„è¿­ä»£ä¸­å®ƒä»¬ç§»åŠ¨çš„æ­¥é•¿å­˜åœ¨å¿«æ…¢å·®å¼‚ã€‚\n\n## 19. Remove Nth Node From End of List\n\n### æ€è·¯\n\nå¯¹äºè¿™é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è®©å¿«æŒ‡é’ˆå…ˆç§»åŠ¨næ­¥ï¼Œç„¶åå†å¯åŠ¨æ…¢æŒ‡é’ˆå¹¶ä¿æŒæ­¥é•¿ä¸º1çš„åŒé€Ÿã€‚æœ€åå½“å¿«æŒ‡é’ˆåˆ°è¾¾é“¾è¡¨å°¾ç»“ç‚¹çš„æ—¶å€™ï¼Œåœæ­¢è¿­ä»£ã€‚å› ä¸ºå¿«æŒ‡é’ˆæ¯”æ…¢æŒ‡é’ˆå¤šç§»åŠ¨äº†næ­¥ï¼Œæ‰€ä»¥å®ƒåˆ°è¾¾ç»ˆç‚¹æ—¶é¢†å…ˆäº†æ…¢æŒ‡é’ˆnæ­¥ï¼Œå› æ­¤æ­¤æ—¶æ…¢æŒ‡é’ˆçš„ç´¢å¼•å³ä¸ºæˆ‘ä»¬ç›®æ ‡ç´¢å¼•ã€‚\n\n### ä»£ç \n\n```python\nclass Solution:\n    def removeNthFromEnd(self, head: ListNode, n: int) -> ListNode:\n        fast=slow=head\n        for i in range(n):\n            fast=fast.next\n        if not fast:\n            return head.next # case: n=size\n        while fast.next:\n            fast=fast.next\n            slow=slow.next\n        slow.next=slow.next.next # delete n_th node from end\n        return head\n```\n\n### è°¬è¯¯ä¸åæ€\n\nä¸€å¼€å§‹æƒ³é”™äº† åœ¨ä¸Šé¢for i = n+1è¿™ä¼šå¯¼è‡´æº¢å‡º, å…¶æ¬¡å¿½ç•¥äº†n=sizeçš„æƒ…å†µã€‚\n\n---\n\n## 141. Linked List Cycle\n\n### æ€è·¯\n\né¢˜ç›®è®©åˆ¤æ–­é“¾è¡¨æ˜¯å¦å­˜åœ¨ç¯ï¼Œæƒ³æˆæ“åœºè¿½å‡»é—®é¢˜ï¼Œå¦‚æœæœ‰ç¯å¿«æŒ‡é’ˆå…ˆè¿›å…¥ç¯ï¼Œç­‰å¾…æ…¢æŒ‡é’ˆè¿›å…¥ç¯åï¼Œæƒ³ä¸‹é€Ÿåº¦v=2å’Œv=1çš„ä¿©äººåœ¨æ“åœºè·‘æ­¥ï¼Œå®ƒä»¬ä¸€å®šä¼šç›¸é‡çš„~\n\n### ä»£ç \n\n```python\nclass Solution:\n    def hasCycle(self, head: ListNode) -> bool:\n        fast=slow=head\n        while (fast and fast.next):\n            fast=fast.next.next\n            slow=slow.next\n            if fast==slow:\n                return True\n        return False\n```\n\n### è°¬è¯¯ä¸åæ€\n\n1. ä¸€å¼€å§‹å¿½ç•¥äº†[]ç©ºé“¾è¡¨çš„æƒ…å†µï¼ŒNonetype æ²¡æœ‰next attributeçš„æŠ¥é”™ã€‚\n\n2. ä¸€å¼€å§‹æ²¡æœ‰æ³¨æ„åˆ°whileé‡Œçš„æ¡ä»¶ï¼Œäº‹å®ä¸Šåº”å½“æ˜¯ï¼Œ```  fast and fast.next```ã€‚and çŸ­è·¯é€»è¾‘è¿ç®—ç¬¦åˆ™å¾ˆå¥½çš„è§£å†³äº†æ— ç¯æƒ…å†µä¸‹æœ€åfastæŒ‡é’ˆæ˜¯å¦åˆ°è¾¾Noneç»“ç‚¹çš„ä¸¤ç§caseï¼Œé˜²æ­¢å¾ªç¯å†…å¿«æŒ‡é’ˆç§»åŠ¨æŠ¥é”™ã€‚\n\n## 142. Linked List Cycle II\n\n## æ€è·¯\n\n{% asset_img image.png %}\n\nquoraä¸Šæ¯”è¾ƒintuitionçš„è§£ç­”:\n\n{% asset_img solution_quora.png %}\n\néœ€è¦æ³¨æ„çš„æ˜¯zçš„é•¿åº¦å¯èƒ½æ˜¯z+n*length(circle), næ˜¯å¤šå°‘ä¸xå’Œç¯çš„é•¿åº¦æœ‰å…³ï¼Œæç«¯ç‚¹è®¾æƒ³ç¯çš„é•¿åº¦æ˜¯1å°±æ‡‚äº†ã€‚\n\n## ä»£ç \n\n```python\nclass Solution:\n    def detectCycle(self, head: ListNode) -> ListNode:\n        fast=slow=head\n        while (fast and fast.next):\n            fast=fast.next.next\n            slow=slow.next\n            if fast == slow: break\n        if not (fast and fast.next): # if no circle\n            return None\n        while head is not fast:\n            head=head.next\n            fast=fast.next\n        return head\n```\n\n## è°¬è¯¯ä¸åæ€\n\n1. ä¸€å¼€å§‹æŠŠç©ºé“¾è¡¨å’Œå•ä¸€å…ƒç´ é“¾è¡¨çš„æƒ…å†µæ‹å‡ºå»äº†ï¼Œå¯¼è‡´å†™çš„æœ‰ç‚¹ç¹çï¼Œäº‹å®ä¸Šåˆ¤æ–­æœ‰æ— ç¯çš„å¾ªç¯å†è¿›è¡Œåˆ¤æ–­è¿™ä¿©caseä¼šæ¯”è¾ƒæ–¹ä¾¿ã€‚\n\n2. å†™åˆ¤æ–­çš„æ—¶å€™ä¸€å¼€å§‹è„‘å­çƒ­äº†ï¼ŒæŠŠnot(fast and fast.next)å†™æˆ not fast and not fast.nextï¼Œå…¶å®ç›¸å½“äºÂ¬((a )âˆ§(b))  ï¼= Â¬a âˆ§ Â¬bï¼Œæ­¤æƒ…æ™¯ä¸‹åº”è¯¥æ˜¯å‰è€…ï¼Œï¼ˆç¦»æ•£æ•°å­¦æ˜¯çœŸçš„å¿˜å…‰äº†QAQï¼‰\n\n","tags":["LeetCode","Double pointer","Link list"],"categories":["Programming","LeetCode","Double pointer"]},{"title":"Lloyd-Max Quantizer","url":"/2021/04/05/Lloyd-Max Quantizer/","content":"# PDFèµ„æ–™\n{% pdf ./Max-Floyd.pdf %}\n# ä½œä¸šè¦æ±‚\n>å‚è€ƒ Max-Floyd.pdf ä¸­çš„example 2, å°†$p(x)$ä¿®æ”¹æˆé«˜æ–¯åˆ†å¸ƒ$N(0,1)$\n\n# ä»£ç å®ç°\n```python\nimport scipy.stats as stats\nfrom scipy.integrate import quad\nimport math\ny1=0.3;y2=0.8;max_iterations=500;precision=1e-9\n# p=1\n# p=stats.norm.pdf(x,0,1)\nnum_func=lambda x: x*stats.norm.pdf(x,0,1)\nden_func=lambda x: stats.norm.pdf(x,0,1)\nfor i in range(max_iterations):\n    b1=(y1+y2)/2\n    Num1,Nerr1=quad(num_func,0,b1)\n    Den1,Derr1=quad(den_func,0,b1) \n    y1=Num1/Den1\n    Num2,Nerr2=quad(num_func,b1,1)\n    Den2,Derr2=quad(den_func,b1,1) \n    tmp=y2\n    y2=Num2/Den2\n    if abs(y2-tmp)<precision:\n        print('iterations:',i)\n        break\nprint('y1:',y1,'y2:',y2,'b1:',b1)\n```\n\n# æ·±å…¥æ€è€ƒ\næœªå®Œå¾…å†™..","tags":["Machine learning","Python","Optimisation"],"categories":["Machine learning"]}]