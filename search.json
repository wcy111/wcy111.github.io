[{"title":"Kernel function and Kernel trick","url":"/2021/11/09/Kernel-function and-Kernel-trick/","content":"\n## Nonlinear Feature Map\n\nTo creat a nonlinear method for regression or classifier is to transform **feature vector** via a **nonlinear feature map** \n$$\n\\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{m} \\tag{1.1}\n$$\n\n\nThis way is pretty intuitive since we can just image adding some features on the original feature vector. Then apply this feature map over whole dataset get $(\\Phi(x_{i}),y_{i})\\quad \\forall i)$\n\n\n\nThus, in regression, the classifier is :\n$$\nf(\\boldsymbol{x})=\\boldsymbol{w}^{T} \\boldsymbol{\\phi}(\\boldsymbol{x})+b \\tag{1.2}\n$$\nWhere $\\boldsymbol{w}\\in \\mathbb{R}^m\\quad b \\in\\mathbb{R}$\n\n## Inner Product Kernels\n\n- Many ML algorithms depend on $\\boldsymbol{\\phi}(\\boldsymbol{x})$ only via *inner products*\n  $$\n  \\langle \\Phi(x),\\phi(x^{\\prime})\\rangle  \\tag{1.3}\n  $$\n\n- for certain $\\Phi$, the function\n  $$\n  k(u,v)=\\langle\\Phi(u),\\Phi(v)\\rangle \\tag{1.4}\n  $$\n  can be computed efficiently even m is huge or possibly infinite. k is called an inner product kernel.\n\n---\n\n### Some important kernels\n\nHomogeneous polynomia kernel\n$$\nk(u,v)=(u^Tv)^{p} \\tag{1.5}\n$$\nInhomogeneous polynomia kernel\n$$\nk(u,v)=(u^Tv+b)^{p} \\tag{1.6}\n$$\nGaussian kernel (not list here)\n\n### SPD kernels\n\n- One way to determine an inner product kernel is to construct $\\Phi$ explicitly.\n\n- Another way is to verify that k is an kernel if it satisfies the following properties\n\n  - $k:\\mathbb{R}^{d}\\times \\mathbb{R}^{d} \\rightarrow\\mathbb{R}$. k is symmetric i.e. $k(u,v)=k(v,u) \\quad\\forall u,v$\n\n  - $k$ is positive definite if:\n    $$\n    \\left[\\begin{array}{ccc}\n    k\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{n}\\right) \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    k\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{n}\\right)\n    \\end{array}\\right]\n    $$\n    is a PSD matrix for all $n\\in \\mathbb{N}$, and $x_{1}...x_{n}\\in\\mathbb{R}^{d}$\n\n- **Theorem**: k is an SPD kernel iff k is an inner product kernel.\n\n---\n\n## The Kernel Trick\n\nA machine learning algorithm is said to be **kernelizable** if it is possible to formulate the algorithm such that all training instances $x_{i}$ and any test instance $x$ occur in inner products of the form$\\langle x_{i},x{_{j}} \\rangle,\\langle x_{i},x \\rangle,\\text{or}\\langle x,x \\rangle$\n\n----\n\nSince we can suppose $\\Phi$ is a feature map associatecd to an inner product kernel k, if we apply a kernelizable algorithm to the training data  like constructing :\n$$\n\\left(\\boldsymbol{\\Phi}\\left(\\boldsymbol{x}_{1}\\right), y_{1}\\right), \\ldots,\\left(\\boldsymbol{\\Phi}\\left(\\boldsymbol{x}_{n}\\right), y_{n}\\right) \\tag{1.7}\n$$\nthen we can formulate the algorithm such that transformed feature vectors only appear via inner products $\\langle\\boldsymbol{\\Phi(x),\\Phi(x^{\\prime})}\\rangle$ with other similar transformed feature vectores.\n\n- this can be implemented by evaluating $k(u,v)=\\langle\\Phi(u),\\Phi(v)\\rangle$ which eliminates the need to ever compute $\\Phi(x)$.\n\n---\n\n## Kernel Ridge Regression\n\nRidge regression is kernelizable. use the kernel trick to extend it to a nonlinear method called ridge regression.\n\n---\n\n### Kernel ridge regresion(without offset)\n\nsince some $\\Phi$ contain a constant term, as with the inhomogeous polynomial kernel, the offset is not always needed\n\nFor the KRR(no offset), we have objective function \n$$\n\\begin{aligned}\n\\min _{w}\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\boldsymbol{w}^{T} \\boldsymbol{x}_{i}\\right)^{2}+\\lambda\\|\\boldsymbol{w}\\|^{2} &=\\frac{1}{n}\\|\\boldsymbol{y}-\\boldsymbol{X} \\boldsymbol{w}\\|^{2}+\\lambda\\|\\boldsymbol{w}\\|^{2} \\\\\n& \\propto \\boldsymbol{w}^{T}\\left(\\boldsymbol{X}^{T} \\boldsymbol{X}+n \\lambda \\boldsymbol{I}\\right) \\boldsymbol{w}-2 \\boldsymbol{y}^{T} \\boldsymbol{X} \\boldsymbol{w}+\\boldsymbol{y}^{T} \\boldsymbol{y}\n\\end{aligned}\n \\tag{1.8}\n$$\nThe solution is\n$$\n\\widehat{\\boldsymbol{w}}=\\left(\\boldsymbol{X}^{T} \\boldsymbol{X}+n \\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{X}^{T} \\boldsymbol{y}\\tag{1.9}\n$$\nWhere \n$$\n\\boldsymbol{y}=\\left[\\begin{array}{c}\ny_{1} \\\\\n\\vdots \\\\\ny_{n}\n\\end{array}\\right], \n\n\\quad \\boldsymbol{X}=\\left[\\begin{array}{c}\n\\boldsymbol{x}_{1}^{T} \\\\\n\\vdots \\\\\n\\boldsymbol{x}_{n}^{T}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times d}\n$$\nNote $X^TX$ is not the gram matrix of the training data (it belongs to $\\mathbb{R^{d\\times d}}$), thus we need to further transform the solution\n\n- apply matrix inversion lemma:\n  $$\n  (\\mathbf{P}+\\mathbf{Q R S})^{-1}=\\mathbf{P}^{-1}-\\mathbf{P}^{-1} \\mathbf{Q}\\left(\\mathbf{R}^{-1}+\\mathbf{S P}^{-1} \\mathbf{Q}\\right)^{-1} \\mathbf{S P}^{-1} \\tag{1.10}\n  $$\n\nAfter simplification we get \n$$\n\\widehat{\\boldsymbol{w}}^{T}=\n\\boldsymbol{y}^{T}\\left(\\mu \\boldsymbol{I}+\\boldsymbol{X} \\boldsymbol{X}^{T}\\right)^{-1} \\boldsymbol{X} \\tag{1.11}\n$$\nNote the Gram matrix $G=XX^{T}$ appears, although the method is still not kernelized because of the matrix $X$ . However, this can be resolved by taking the inner product of $\\widehat{w}$ with a test instance $x$ . Thus introduce the notation \n$$\n\\boldsymbol{G}:=\\left[\\begin{array}{ccc}\n\\left\\langle\\boldsymbol{x}_{1}, \\boldsymbol{x}_{1}\\right\\rangle & \\cdots & \\left\\langle\\boldsymbol{x}_{1}, \\boldsymbol{x}_{n}\\right\\rangle \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\left\\langle\\boldsymbol{x}_{n}, \\boldsymbol{x}_{1}\\right\\rangle & \\cdots & \\left\\langle\\boldsymbol{x}_{n}, \\boldsymbol{x}_{n}\\right\\rangle\n\\end{array}\\right] \\quad \\boldsymbol{g}(\\boldsymbol{x}):=\\left[\\begin{array}{c}\n\\left\\langle\\boldsymbol{x}_{1}, \\boldsymbol{x}\\right\\rangle \\\\\n\\vdots \\\\\n\\left\\langle\\boldsymbol{x}_{n}, \\boldsymbol{x}\\right\\rangle\n\\end{array}\\right]\n$$\nThen we can have classifier as\n$$\n\\begin{aligned}\n\\widehat{f}(\\boldsymbol{x}) &=\\widehat{\\boldsymbol{w}}^{T} \\boldsymbol{x} \\\\\n&=\\boldsymbol{y}^{T}\\left(\\boldsymbol{X} \\boldsymbol{X}^{T}+n \\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{X} \\boldsymbol{x} \\\\\n&=\\boldsymbol{y}^{T}(\\boldsymbol{G}+n \\lambda \\boldsymbol{I})^{-1} \\boldsymbol{g}(\\boldsymbol{x})\n\\end{aligned} \\tag{1.12}\n$$\nThis shows that KRR w/o is kernelizable. So we apply kernek trick by simply selecting a kerenel k and replace $\\langle u,v\\rangle$ with $k(u,v)$ . After substituion, $G$ and $g(x)$ are replaced by \n$$\n\\boldsymbol{K}:=\\left[\\begin{array}{ccc}\nk\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{n}\\right) \\\\\n\\vdots & \\ddots & \\vdots \\\\\nk\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{1}\\right) & \\cdots & k\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{n}\\right)\n\\end{array}\\right], \\quad \\boldsymbol{k}(\\boldsymbol{x}):=\\left[\\begin{array}{c}\nk\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}\\right) \\\\\n\\vdots \\\\\nk\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}\\right)\n\\end{array}\\right]\n$$\n$K$ is called the kernel matrix. Now, the final form of the KRR(w/o) offset predictor is \n$$\n\\widehat{f}(\\boldsymbol{x})=\\boldsymbol{y}^{T}(\\boldsymbol{K}+n \\lambda \\boldsymbol{I})^{-1} \\boldsymbol{k}(\\boldsymbol{x}) \\tag{1.13}\n$$\nNote entire process is equivalent to first applying the feature map $\\Phi$ to all feature vectors, and applying ridge regression w/o in the new feature space. Because all we need is to obtain a nonlinear regression predictor and once we have selected a kernel all the calculation needed for predicting a new point is implicitly finished.\n\n#### Computational Complexity of KRR w/o\n\nThe computational complexity of KRR without offset is $O(n^3)$ which comes from having to invert an $n\\times n$ matrix.  As with regular regression, this can be accelerated using **gradient descent** and related methods\n\n## Kernel Ridge Regression  with Offset\n\nThe derivation of kernel ridge regression with offset is similar as KRR without offset, but with one important additional concept.\n\nFirst, the solution to ridge regression with offset is\n$$\n\\begin{aligned}\n&\\widehat{\\boldsymbol{w}}=\\left(\\tilde{\\boldsymbol{X}}^{T} \\tilde{\\boldsymbol{X}}+n \\lambda \\boldsymbol{I}\\right)^{-1} \\tilde{\\boldsymbol{X}}^{T} \\tilde{\\boldsymbol{y}} \\\\\n&\\widehat{b}=\\bar{y}-\\widehat{\\boldsymbol{w}}^{T} \\overline{\\boldsymbol{x}}\n\\end{aligned} \\tag{2.1}\n$$\nwhere\n$$\n\\tilde{\\boldsymbol{y}}=\\left[\\begin{array}{c}\n\\tilde{y}_{1} \\\\\n\\vdots \\\\\n\\tilde{y}_{n}\n\\end{array}\\right], \\quad \\tilde{y}_{i}=y_{i}-\\bar{y}, \\quad \\tilde{\\boldsymbol{X}}=\\left[\\begin{array}{c}\n\\tilde{\\boldsymbol{x}}_{1}^{T} \\\\\n\\vdots \\\\\n\\tilde{\\boldsymbol{x}}_{n}^{T}\n\\end{array}\\right], \\quad \\tilde{\\boldsymbol{x}}_{i}=\\boldsymbol{x}_{i}-\\overline{\\boldsymbol{x}}\n$$\nHere $\\overline{x}=\\sum _{i}x_{i}$, $\\overline{y}=\\sum _{i}y_{i}$. The regression func estimate is then\n$$\n\\widehat{f}(\\boldsymbol{x})=\\widehat{\\boldsymbol{w}}^{T} \\boldsymbol{x}+\\widehat{b}=\\bar{y}+\\widehat{\\boldsymbol{w}}^{T}(\\boldsymbol{x}-\\overline{\\boldsymbol{x}}) \\tag{2.2}\n$$\nTo kernelize this funciton, we can follow the exact same steps as for KRR without offset to arrive at \n$$\n\\widehat{\\boldsymbol{w}}^{T}=\\tilde{\\boldsymbol{y}}^{T}(\\tilde{\\boldsymbol{G}}+n \\lambda \\boldsymbol{I})^{-1} \\tilde{\\boldsymbol{X}} \\tag{2.3}\n$$\nthen \n$$\n\\widehat{f}(\\boldsymbol{x})=\\bar{y}+\\tilde{\\boldsymbol{y}}^{T}(\\tilde{\\boldsymbol{G}}+n \\lambda \\boldsymbol{I})^{-1} \\tilde{\\boldsymbol{g}}(\\tilde{\\boldsymbol{x}}) \\tag{2.4}\n$$\nWhere $\\tilde{x}=x-\\overline{x}$ and\n$$\n\\tilde{\\boldsymbol{G}}:=\\left[\\begin{array}{ccc}\n\\left\\langle\\tilde{\\boldsymbol{x}}_{1}, \\tilde{\\boldsymbol{x}}_{1}\\right\\rangle & \\cdots & \\left\\langle\\tilde{\\boldsymbol{x}}_{1}, \\tilde{\\boldsymbol{x}}_{n}\\right\\rangle \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\left\\langle\\tilde{\\boldsymbol{x}}_{n}, \\tilde{\\boldsymbol{x}}_{1}\\right\\rangle & \\cdots & \\left\\langle\\tilde{\\boldsymbol{x}}_{n}, \\tilde{\\boldsymbol{x}}_{n}\\right\\rangle\n\\end{array}\\right], \\quad \\tilde{\\boldsymbol{g}}(\\tilde{\\boldsymbol{x}}):=\\left[\\begin{array}{c}\n\\left\\langle\\tilde{\\boldsymbol{x}}, \\tilde{\\boldsymbol{x}}_{1}\\right\\rangle \\\\\n\\vdots \\\\\n\\left\\langle\\tilde{\\boldsymbol{x}}, \\tilde{\\boldsymbol{x}}_{n}\\right\\rangle\n\\end{array}\\right]\n$$\nBasicallly KRR w/  is like KRR w/o offset applied to the mean-centered feature space. In addition, $\\overline{y}$ is added to the predicted output.\n\n---\n\nTo see $\\tilde{G} ~\\text{and}~ \\tilde{g}(\\tilde{x})$ is kernelizable, we can expand the entries in the matrix as \n$$\n\\begin{aligned}\n\\left\\langle\\tilde{x}_{i}, \\tilde{x}_{j}\\right\\rangle &=\\left\\langle x_{i}-\\bar{x}, x_{j}-\\bar{x}\\right\\rangle \\\\\n&=\\left\\langle x_{i}, x_{j}\\right\\rangle-\\frac{1}{n} \\sum_{r=1}^{n}\\left\\langle x_{i}, x_{r}\\right\\rangle-\\frac{1}{n} \\sum_{s=1}^{n}\\left\\langle x_{s}, x_{j}\\right\\rangle+\\frac{1}{n^{2}} \\sum_{r=1}^{n} \\sum_{s=1}^{n}\\left\\langle x_{r}, x_{s}\\right\\rangle\n\\end{aligned}\\tag{2.5}\n$$\n--(calculation a bit tricky)\n\nand\n$$\n\\begin{aligned}\n\\left\\langle\\tilde{\\boldsymbol{x}}_{i}, \\tilde{\\boldsymbol{x}}\\right\\rangle &=\\left\\langle\\boldsymbol{x}_{i} -\\overline{\\boldsymbol{x}}, \\boldsymbol{x}-\\overline{\\boldsymbol{x}}\\right\\rangle \\\\\n&=\\left\\langle\\boldsymbol{x}_{i}, \\boldsymbol{x}\\right\\rangle-\\frac{1}{n} \\sum_{r}\\left\\langle\\boldsymbol{x}_{i}, \\boldsymbol{x}_{r}\\right\\rangle-\\frac{1}{n} \\sum_{s}\\left\\langle\\boldsymbol{x}, \\boldsymbol{x}_{s}\\right\\rangle+\\frac{1}{n^{2}} \\sum_{r} \\sum_{s}\\left\\langle\\boldsymbol{x}_{r}, \\boldsymbol{x}_{s}\\right\\rangle .\n\\end{aligned} \\tag{2.6}\n$$\nTherefore to kernelize ridge regression with offset, we select a kernel k and and replace $\\tilde{G}$\n\nwith $\\tilde{K}$  and $\\tilde{g}(x)$ with $\\tilde{k}(x)$.\n\nThen for $\\tilde{K}$ \n$$\nk\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)-\\frac{1}{n} \\sum_{r=1}^{n} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{r}\\right)-\\frac{1}{n} \\sum_{s=1}^{n} k\\left(\\boldsymbol{x}_{s}, \\boldsymbol{x}_{j}\\right)+\\frac{1}{n^{2}} \\sum_{r=1}^{n} \\sum_{s=1}^{n} k\\left(\\boldsymbol{x}_{r}, \\boldsymbol{x}_{s}\\right)\\tag{2.7}\n$$\nfor $\\tilde{k}(x)$\n$$\nk\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}\\right)-\\frac{1}{n} \\sum_{r} k\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{r}\\right)-\\frac{1}{n} \\sum_{s} k\\left(\\boldsymbol{x}, \\boldsymbol{x}_{s}\\right)+\\frac{1}{n^{2}} \\sum_{r} \\sum_{s} k\\left(\\boldsymbol{x}_{r}, \\boldsymbol{x}_{s}\\right) \\tag{2.8}\n$$\nThus, the final KRR(w/ offset) predictor is \n$$\n\\widehat{f}(\\boldsymbol{x})=\\bar{y}+\\tilde{\\boldsymbol{y}}^{T}(\\tilde{\\boldsymbol{K}}+n \\lambda \\boldsymbol{I})^{-1} \\tilde{\\boldsymbol{k}}(\\boldsymbol{x}) \\tag{2.9}\n$$\nNote  from eq. 2.4 it is very tempting to attempt to kernelize this method by replacing dot products $\\langle\\tilde{x},\\tilde{x^{\\prime}}\\rangle$ with $k(\\tilde{x},\\tilde{x}^{\\prime})$, but this is incorrect. Because we should let $\\Phi$ works for orginal $x$ \n\nfeature space.\n\nTo see it , we can introduce the notation \n$$\n\\tilde{\\Phi}(\\boldsymbol{x}):=\\Phi(\\boldsymbol{x})-\\frac{1}{n} \\sum_{i} \\Phi\\left(\\boldsymbol{x}_{i}\\right)\\tag{2.10}\n$$\n-- point is $\\Phi(\\tilde{x}) \\neq \\tilde{\\Phi}(x)$, so this approach make no sense.\n\nHere we refer to $\\tilde{\\Phi}$ as the *the centered feature map*, $\\tilde{k}(x,x^{\\prime}):=\\langle\\tilde{\\Phi}(x),\\tilde{\\Phi}(x^{\\prime})\\rangle$  as the centered kernel, and $\\tilde{K}$ as the *centered kernel matrix* associated to the training data set. \n\n","tags":["Machine learning","Kernel trick","Kernel logistic regression","feature map"],"categories":["Machine learning","Kernel trick"]},{"title":"Estimation of random vectors","url":"/2021/10/29/Estimation of random vectors/","content":"\n Estimators-MMSE、LMSE、Regression、Linear innovation sequences\n\n<!-- more -->\n\nWrite down just4 understand now and I will let it well-organized later if I have time. :p\n\n\n\n\n\n## Linear estimation of random vectors \n\n\n\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"Moment generating func&Characteristic func","url":"/2021/10/17/Moment-generating-func-Characteristic-func/","content":"\n![img](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYtsAcrE8dz6m4ClTgYHiL0Wtv3QbJaM9o8Q&usqp=CAU)\n\nA generating function is a clothesline on which we hang up a sequence of numbers for display ----Herbert Wlif\n\n<!-- more -->\n\n## Moment generating function\n\n### Definition of MGF\n\nwe can use moment generating function rather than PMF to define or describe a random variable.\n$$\nM(t)=\\mathbb{E}\\left[e^{t X}\\right]=\\int e^{t x} f_{X}(x) d x=\\int e^{t x} d F_{X}(x)\n$$\nWhere $X$ is a random variable of interest, $t$ is one parameter of tunction. And according to the **LOTUS** we can expand the expectation formula.\n\n----\n\nIf $M(t) \\leq \\infin$ on some interval containing the origin, then \n\n(a) $E(X)=M'(0)$    (b) $E[X^k]=M^{(k)}(0)$\n\ne.g.\n$$\n\\begin{aligned}\n&M^{\\prime}(0)=\\left.\\int x e^{t x} f_{X}(x) d x\\right|_{t=0}=\\int x f_{X}(x) d x=\\mathbb{E}[X] \\\\\n&M^{\\prime \\prime}(0)=\\left.\\int x^{2} e^{t x} f_{X}(x) d x\\right|_{t=0}=\\int x^{2} f_{X}(x) d x=\\mathbb{E}\\left[X^{2}\\right]\n\\end{aligned}\n$$\n**Notice that $M$ is function of t!!!**  and actually this structure is well-designed, everytime we take a derivative of this formula we generate a factor $x$, and we take $t =0 $  to eliminate the exponential item thus we get moment generating function.\n\n---\n\n### Property of MGF\n\n- MGF of the sum of independent random variables. \n  $$\n  M_{X+Y}(t)=\\mathbb{E}\\left[e^{t (X+Y)}\\right]=\\mathbb{E}\\left[e^{t X} e^{t Y}\\right]\n  $$\n\n- If $X,Y$ are independent then it equals to $\\mathbb{E}\\left[e^{t X} e^{t Y}\\right]=\\mathbb{E}\\left[e^{t X}]\\mathbb{E}[ e^{t Y}\\right]=M_X(t)M_Y(t)$\n\n- thus when $X_1....,X_n$ Are independently and identically distributed \n  $$\n  M_{W}(t)=\\left(M_{X}(t)\\right)^{n}\n  $$\n\nwhere $W=\\sum_{i=1}^mX_{i}$\n\n---\n\n## Characteristic function\n\nIt's very similar to the MGF but more powerful since for some random varaibles MGF doesn't exist( such as given certain $t$ MGF goes infinity). \n\n----\n\n$$\n\\begin{aligned}\n\\phi(t) &=\\mathbb{E}\\left[e^{i t X}\\right](i=\\sqrt{-1}) \\\\\n&=\\int e^{i t x} f_{X}(x) d x=\\int(\\cos t x+i \\cdot \\sin t x) f_{X}(x) d x \\\\\n&=\\mathbb{E}[\\cos t X]+i \\mathbb{E}[\\sin t X]\n\\end{aligned}\n$$\n\nProperties:\n- $\\phi(0)=\\mathbb{E}\\left[e^{i 0 X}\\right]=\\mathbb{E}[1]=1$\n- $|\\phi(t)| \\leq \\int\\left|e^{i t x}\\right| f_{X}(x) d x=\\int f_{X}(x) d x=1 .$ So $\\phi(t)$ exists while $M_{t}$ may not.\n- If $X$ and $Y$ are independent $\\phi_{X+Y}(t)=\\mathbb{E}\\left[e^{i t(X+Y)}\\right]=\\mathbb{E} e^{i t X} e^{i t Y}=\\phi_{X}(t) \\phi_{Y}(t)$\n- $Y=a X+b, a, b \\in[R]$\n\n$$\n\\phi_{Y}(t)=\\mathbb{E}\\left[e^{i t(a X+b)}\\right]=\\mathbb{E}\\left[e^{i t b} e^{i a t X}\\right]=e^{i t b} \\mathbb{E}\\left[e^{i a t X}\\right]=e^{i t b} \\phi_{X}(a t)\n$$\n\n---\n\n### Charateristic function for gaussian\n\n\n\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"Transformations(functions)","url":"/2021/10/08/Transformations-functions/","content":"\n<!-- more -->\n\n## Inverse function\n\n![img](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Inverse_Function.png/220px-Inverse_Function.png)\n\nIn mathematics, an **inverse function** (or **anti-function**) is a function  that \"reverses\" another function: if the function *f* applied to an input *x* gives a result of *y*, then applying its inverse function *g* to *y* gives the result *x*, i.e., *g*(*y*) = *x* if and only if *f*(*x*) = *y*.The inverse function of *f* is also denoted as ${\\displaystyle f^{-1}}$\n\n----\n\n## Transformations (functions)\n\nLet $X$ be a random variable and $Y=g(X)$.  Given the **PDF** of $X$, find the **PDF** of **Y** .\n\n**REMARK**: The general way to find the **PDF** is to first find the **CDF** and then differentiate it to obtain the **PDF**. \n\n### Monotonous case\n\n#### 1 strictly increasing\n\nSay $g$ is **strictly increasing** and its inverse function is $h$ i.e., $h=g^{-1}$ which is also **increasing**.\n\nThe first step is to know the **CDF** of **R.V**. $Y$ and plug-in existing conditions,\n$$\nF_{Y}(y)=P(Y \\leq y)=P(h(Y) \\leq h(y))=P(X \\leq h(y))=F_{X}(h(y))\n$$\n**REMARK**:the second equation we apply $h$ function on both side thus we get $h(Y)=X$ and according to the monotone increasing of $h$ the inequality sign remains unchanged. Then with the defination of **CDF** we get $F_{X}(h(y))$\n\nThus, we can **differentiate** CDF to get PDF where chain rule will be used:\n$$\nf_{Y}(y)=\\frac{d}{d y} F_{Y}(y)=\\frac{d}{d y} F_{X}(h(y))=f_{X}(h(y)) \\frac{d h(y)}{d y}\n$$\n\n#### 2 strictly decreasing\n\nwhen it comes to decreasing case, we have minor change, see\n$$\nF_{Y}(y)=P(Y \\leq y)=P(h(Y) \\geq h(y))=1-P(X \\leq h(y))=1-F_{X}(h(y))\n$$\n\n$$\nf_{Y}(y)=\\frac{d}{d y} F_{Y}(y)=-\\frac{d}{d y} F_{X}(h(y))=-f_{X}(h(y)) \\frac{d h(y)}{d y}\n$$\n\n### No Monotonous case \n\nSay we have $Y=X^2$, $X-U(-1,1) $ \n\nobviously function is not monitonous so we can not use the conclusion aforementioned. \n\nTo solve this, firstly get the possible range of Y i.e. $Support(Y) = [0,1]$, so we assume $y \\in Y$\n$$\nF_{Y}(y)=P(Y \\leq y)=P\\left(X^{2} \\leq y\\right)=P\\left(\\left\\{x: x^{2} \\leq y\\right\\}\\right)=P(X \\in[-\\sqrt{y}, \\sqrt{y}])\n$$\nNote $X^2 \\leq y$ actually returns a set i.e. $\\set{x:x^2 \\leq y}$, \n$$\nP(X \\in[-\\sqrt{y}, \\sqrt{y}])=\\int_{-\\sqrt{y}}^{\\sqrt{y}} f_{X}(x) d x=\\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{2} d x=\\sqrt{y}\n$$\nso\n$$\nf_{Y}(y)= \\begin{cases}F_{Y}^{\\prime}(y)=\\frac{1}{2 \\sqrt{y}} & y \\in[0,1] \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\n### Find the mapping\n\n\n\nLet $X$ and $Y$ be two random variables with PDFs $f_X$ and $f_Y$ respectively. Find a transformation $g$ such that $Y=g(X)$.\n\n**REMARK**: Since **uniform distribution** can generated any distributions , so if we can find such a function $g$ we are able to sample in the $X$ without have simulation of $Y$\n\n- Uniqueness can be guaranteed only if we assume g to be **monotone non-decreasing** or **monotone non-increasing.**\n\n--\n\n#### Example \n\n$$\n\\text { Example: } X \\sim \\mathcal{U}[0,1] \\text { and } Y \\sim \\text { Exponential }(\\lambda)\n$$\n\nFirstly, we can know $F_{X}=x$,  so $F_X(h(y))=h(y)$, \n\n\n\nSince we have known from the previous conclusion that $F_X(h(y))=F_Y(y)$ , hence we get \n$$\nh(y)=F_Y(y)=\\int_{0}^{y} \\lambda \\exp (-\\lambda t) d t=1-\\exp (-\\lambda y)\n$$\nagain $g(X)=h^-1(X)$, so to get the function $g$  we need to calculate inverse of function $h$, thus we get \n$$\ny=-\\frac{1}{\\lambda} \\log (1-h(y)): g(X)=\n\\frac{-1}{\\lambda} \\log (1-X)\n$$\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"CD(Coordinate Descent)","url":"/2021/10/07/CD Coordinate Descent/","content":"\n<!-- more -->\n\n","tags":["Machine learning","Optimization","CD"],"categories":["Math","Optimization"]},{"title":"SVM(SaVeMe)","url":"/2021/10/05/SVM-SaVe-Me/","content":"\n![img](data:image/jpeg;base64,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)\n\nSVM(Support Vector Machine)\n\n<!-- more -->\n\n## SVM(~~SaVeMe~~Support Vector Machine)\n\nTry to organize all the thing from scratch.\n\n---\n\n## Subgradients\n\n#### why subgradient\n\nwhen we potentially want to use gradient method, one thing we should ensure that that function we derive the gradient should be differentiable otherwise we should consider the subgradient.\n\n\n\n---\n\nhard margin\n\nsoft margin(si geometric meaning)\n\nlagrangian (why it will not change the solution)\n\nlagrangian dual prob\n\nduality\n\n— necessity\n\n— sufficiency\n\n— strong duality\n\n— how it holds?\n\n— weak\n\ndual optimization prob\n\nKKT (how to use KKT to solve normal problem)\n\nSVM derivation\n\nKKT\n\nSVM dual\n\nSMO\n\nfinal predicter\n\nSV(how it related to si)\n\nhinge loss\n\n— core point is get from slackness complementary and multiply 1/C  will not change the optimize problem.\n\n— do not forget the kkt condition2 it will constraint the range of si, so that apply hinge loss smoothly\n\n------\n\nSVR deriviation(SMO geometric)\n\n","tags":["Machine learning","SVM"],"categories":["Machine learning","SVM"]},{"title":"Continuous Random Variables","url":"/2021/10/02/EECS501-Notes3/","content":"\n<!-- more -->\n\n### Continuous Random Variables\n\n### LECTURE 7-8 \n\n### Continuous Probability Models: \n\nWhen it comes to **continuous random variables** the prob is assign to **interval**. because when the outcome is interval the points will be infinite and in that case the only choose we can take is let every point has the probabilty equals zero.\n\n#### Conitunuous probability space\n\nSo we need to redefine the **event space** when our **sample space** is **uncountable**\n\n![image-20211002153347891](/Users/loststars/Library/Application Support/typora-user-images/image-20211002153347891.png)\n\n---\n\n### Cumulative density function\n\n\n\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"Variance&Conditional Variance","url":"/2021/10/02/EECS501-Notes2/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n## Variance&Conditional Variance\n\n### LECTURE 6-7 Discrete Random Variables\n\n\n\n### Variance\n\nVariance of a random variable is defined as follows:\n$$\n\\operatorname{Var}(X)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^{2}\\right]\n$$\n**REMARK:** $E(X)$ is one way to summarize the PMF $P_X$, but it connot capture **randomness/uncertainty** thus we need variance.\n\n#### Alternative expresion for variance\n\n$$\n\\begin{aligned}\n\\operatorname{Var}(X) &=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^{2}\\right] \\\\\n&=\\mathbb{E}\\left[X^{2}-2 X \\mathbb{E}[X]+\\mathbb{E}[X]^{2}\\right] \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}[2 X \\cdot \\mathbb{E}[X]]+\\mathbb{E}\\left[(\\mathbb{E}[X])^{2}\\right] \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-2 \\mathbb{E}[X] \\mathbb{E}[X]+(\\mathbb{E}[X])^{2} \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-(\\mathbb{E}[X])^{2} \\\\\n&=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}^{2}[X]\n\\end{aligned}\n$$\n\nNote that $Var(X)\\geq 0$ and it equals 0 if and only if $X$ is a **constant function** \n\n### Conditional variance\n\nconditional variance is analogous to conditional expectation. Let $A$ be some event and $X,Y$ be some random variables.\n\n1. $\\operatorname{Var}(X \\mid A)=\\mathbb{E}\\left[(X-\\mathbb{E}[X \\mid A])^{2} \\mid A\\right]=\\sum_{x}(x-\\mathbb{E}[X \\mid A])^{2} \\cdot P_{X \\mid A}(x)$\n\n   $X|A$ is random variable of $X$\n\n   Alternatively, $\\operatorname{Var}(X \\mid A)=\\mathbb{E}\\left[X^{2} \\mid A\\right]-\\mathbb{E}^{2}[X \\mid A]$\n\n   Similarly, $\\operatorname{Var}(X \\mid Y=y)=\\operatorname{Var}(X \\mid\\{Y=y\\})=\\mathbb{E}\\left[(X-\\mathbb{E}[X \\mid Y=y])^{2} \\mid Y=y\\right]$\n\n2. $\\operatorname{Var}(X \\mid Y)(y)=\\operatorname{Var}(X \\mid Y=y)$\n\n   $X|Y$ is a random variable of $Y$ thus a function of $Y$ \n\n   $\\operatorname{Var}(X \\mid Y)=\\mathbb{E}\\left[X^{2} \\mid Y\\right]-\\mathbb{E}^{2}[X \\mid Y]$\n\n### Law of total variance ***\n\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]+\\operatorname{Var}(\\mathbb{E}[X \\mid Y])\n$$\n\n- why we need the second item?\n\n  consider $Var(X|X)$ , it is a function of $X$ , so $Var(X|X)_(x)=Var(X|X=x)=Var(X=x)=0$, for any $x$ the function give the zero output, so we say $Var(X|X)=0$ is a function of constant, the $E[Var(X)]=0$] obviously, if we don't have the second item the equation above will not make sense.\n\n  ---\n\n  **PROVE**\n  $$\n  \\begin{aligned}\n  \\mathbb{E}[\\operatorname{Var}(X \\mid Y)] &=\\mathbb{E}\\left[\\mathbb{E}\\left[X^{2} \\mid Y\\right]-\\mathbb{E}^{2}[X \\mid Y]\\right] \\\\\n  &=\\mathbb{E}\\left[\\mathbb{E}\\left[X^{2} \\mid Y\\right]\\right]-\\mathbb{E}\\left[\\mathbb{E}^{2}[X \\mid Y]\\right] \\\\\n  &=\\mathbb{E}\\left[X^{2}\\right]-\\left(\\operatorname{Var}(Z)+\\mathbb{E}^{2}[Z]\\right) \\\\\n  &=\\mathbb{E}\\left[X^{2}\\right]-(\\mathbb{E}[\\mathbb{E}[X \\mid Y]])^{2}-\\operatorname{Var}(Z) \\\\\n  &=\\mathbb{E}\\left[X^{2}\\right]-(\\mathbb{E}[X])^{2}-\\operatorname{Var}(Z)\n  \\end{aligned}\n  $$\n  Thus, $\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]+\\operatorname{Var}(\\mathbb{E}[X \\mid Y])=\\operatorname{Var}(X)$\n\n---\n\n- The **first step** we just expand the variance and **the second** we use the Linearity of expectation.\n- Then to be concise, let $Z$ denote $X|Y$,  the first item **in third step** become $E[X^2]$ because the property of **smoothing**.\n- using smoothing again we get fourth step\n- then let $Var(X)$ substitute the first two items in **step 5**, we then get all we want. \n\n--------\n\n### Small practice\n\n- We have two bins {1; 2} and each bin has three types of balls {0; 2; 4}: A bin is\n\nrandomly selected  first and then a ball is drawn from the bin. The fraction of each\n\ntype of balls in each bin is shown in the following table.\n\n| -    | 0    | 2    | 4    |\n| ---- | ---- | ---- | ---- |\n| Bin1 | 0.6  | 0.3  | 0.1  |\n| Bin2 | 0.1  | 0.3  | 0.6  |\n\nLet X denote the type of the ball selected. Calculate Var (X)\n\n-----\n\n**analysis**: we can solve it by calculating the pmf of $X$, but we should use the marginal distribution to calculate it from the distribution of joint $X,Y$, it will not be trivial since this is just a discrete case. \n\nbut the purpose of this small practice is to use **LOTV**, thus string all the concept together. \n\n---\n\n**Solution**:\n\nSo let us first expand $Var(X)$ by **LOTV**:\n$$\nVar(X)=\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]+\\operatorname{Var}(\\mathbb{E}[X \\mid Y]) \\tag{1}\n$$\nNote that $E[X|Y]$  and $Var[X|Y]$ are both **function of Y** thus both **random variables**. and to be convise we denote them $Z1~~Z2$ respectively. \n$$\nVar(X)=\\mathbb{E}[\\operatorname{Z2}]+\\operatorname{Var}(\\mathbb{E}[Z1]) \\tag{2}\n$$\nWhen calculate the **expectation and variance** of a random variable we should know its all valid **real value and correspondent pmf**. \n\n---\n\nfor $Z1$, since its function of Y and Y is a discrete  R.V.  with only two valid value hence we can get two \n\n**conditional expectation**\n$$\n\\begin{aligned}\n&\\{Z 1=1\\}=\\{E[X \\mid Y=1]\\}=\\sum_{x \\in v a l(x)} x * p(x \\mid y=1)=1 \\\\\n&\\ldots \\quad p(Z 1=1)=\\sum_{x} p(x \\mid y=1)=p(Y=1) \\\\\n&\\{Z 1=2\\}=\\{E[X \\mid Y=2]\\}=\\sum_{x \\in v a l(x)} x * p(x \\mid y=2)=3 \\\\\n&\\cdots \\quad p(Z 1=2)=\\sum_{x} p(x \\mid y=2)=p(Y=2)\n\\end{aligned}\n$$\n**Note** $X|Y=1$ is function of X, when we calculate its expectation, actually we are just calculating a conditional expectation (**which just change the pmf to conditional pmf given conditions**) .\n\nsimilarly, we have **conditional variance**\n\n$$\n\\begin{gathered}\n\\{Z 2=1\\}=\\operatorname{Var}[X \\mid Y=1]=\\sum_{x}(x-E[X \\mid Y=1])^{2} p(x \\mid y=1) \\\\\n=(0-1)^{2} * 0.6+(2-1)^{2} * 0.3+(4-1)^{2} * 0.1=1.8 \\\\\n\\cdots \\quad p(Z 2=1)=P(Y=1)=1 / 2 \\\\\n\\{Z 2=2\\}=\\operatorname{Var}[X \\mid Y=2]=\\sum_{x}(x-E[X \\mid Y=2])^{2} p(x \\mid y=1) \\\\\n=(0-3)^{2} * 0.6+(2-3)^{2} * 0.3+(4-3)^{2} * 0.1=1.8 \\\\\n \\cdots \\quad p(Z 2=2)=P(Y=2)=1 / 2\n\\end{gathered}\n$$\n\nSo we have get all the information need to calculate the **expectation** of R.V. $Z2$ and **variance** of R.V. $Z1$. \n\nthe former is $E[Z2]=\\sum_{z_{2}}z_{2}*P(z_{2})=1.8$ and the latter is $\\operatorname{Var}[Z 1]=\\sum_{z_{1}}\\left(z_{1}-E[Z 1]\\right)^{2} * p\\left(z_{1}\\right)=1$ where $E(Z1)=2$, so we get the answer **2.8**\n\n\n\n**REMARK**:  when using **LOTV**, we should firstly specify  **two R.Vs** and calculating all their **real value-pdf** when calculate their value, note that we are calculate the **conditional expectation** or conditional variance of another random variable i.e. the $X$ in $X|Y$ which result a real value. and the pmf\n","tags":["Math","Probability and Random Processes"],"categories":["Math","Probability and Random Processes"]},{"title":"Conditional PMF&Expectation","url":"/2021/10/01/EECS501-Notes1/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n\n## Conditional PMF&Expectation\n### LECTURE 6-7 Discrete Random Variables\n\n\nFor the **conditional PMF** we have\n$$\nP_{X \\mid Y}(x \\mid y)=P(\\{X=x\\} \\mid Y=y)=\\frac{P(\\{X=x\\} \\cap\\{Y=y\\})}{P(\\{Y=y\\})}=\\frac{P_{X Y}(x y)}{P_{Y}(y)}\n$$\nAs for the **conditional Expectation**, we first talk about the case \n$$\n\\mathbb{E}[X \\mid A]=\\sum_{x} x P_{X \\mid A}(x)\n$$\n$X|A$ can be regarded as a **R.V. conditioned on event A**(a bunch of sets from sample space), the difference between $E[X] ~and~E[X|A]$  is the latter changed the **PMF $P_X~to~P_{X|A}$** when calculating.\n\nThen we elaborate on the case\n$$\ng(y)=\\mathbb{E}[X \\mid Y](y \\mid)=\\mathbb{E}[X \\mid Y=y]\n$$\nwhere **event A** is substituted by the **R.V.** $Y$ , note that in this circumstance the expectation should be a **function of Y** which means **function of function** mapping the event $\\set{Y=y_i}$ to real numbers, and hence it is also a R.V.\n\n\n\n### Independence of a R.V. from a event\n\n####  independence of a R.V. from a event\n\n$$\nP_{X \\mid A}(x)=P_{X}(x) \\quad \\forall x\n$$\n\n####  independence of two R.V. \n\n$$\n\\begin{gathered}\nP_{X Y}(x, y)=P_{X}(x) P_{Y}(y) \\quad \\forall x, y \\\\\n\\Rightarrow P_{X \\mid Y}(x \\mid y)=P_{X}(x)\n\\end{gathered}\n$$\n\n#### independence of several R.V. \n\n$$\nP_{X Y Z}(x, y, z)=P_{X}(x) P_{Y}(y) P_{Z}(z), \\forall x, y, z\n$$\n\nNote that it seems different form the **definition** of **independent of three events**, but actually they are **essentially equivalent.** \n\nReason is we can derive the following form in three events just by applying the marginal distribution. \n$$\n\\begin{aligned}\nP_{X Y}(x, y) &=\\sum_{z} P_{X Y}(x, y, z) \\\\\n&=\\sum_{z} P_{X}(x) P_{Y}(y) P_{Z}(z)=P_{X}(x) P_{Y}(y)\n\\end{aligned}\n$$\n\n\n### LOTE: Law of total expectation\n\nEnsure that we have the probability space: $(\\Omega, \\mathcal{F}, P)$ with $B_1,B_2...B_n$ be a **partition** of the $\\Omega$. Then let $X$ be a R.V. on $\\Omega$ with PMF $P_X$, Then we can have \n$$\n\\mathbb{E}[X]=\\sum_{i=1}^{n} P\\left(B_{i}\\right) \\mathbb{E}\\left[X \\mid B_{i}\\right]\n$$\n\n### Some important property of expectation of R.V.\n\n#### 1. Smoothing/law of iterated expectation $\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\mathbb{E}[Y]$\n\nWhen we are asked to calculate $E[Y]$ but we find it a little bit difficult, we can try to **calculate left hand side equation instead**\n\n**emphasis again**: $E[Y|X]$ is function of R.V. $X$ , so when calculating the $E[E[Y|X]]$, we need to know all the potential value of R.V. $Z=g(X)$ also the corresponding probability value of $X$\n\n e.g. $\\mathbb{E}[Y \\mid X=1]=\\sum_{y} x P_{Y \\mid X}(y \\mid 1)$ is one of the value of R.V. and its corresponding pmf is $P(X=1)$\n\n#### 2. $\\mathbb{E}[h(X) \\mid X]=h(X)$\n\nwe can just remember this equation by: when we know $X$ then $h(X)$ become real value and the expectation of constant is itself.\n\n#### 3. Substitution $\\mathbb{E}[g(X, Y) \\mid X=x] \\mid=\\mathbb{E}[g(x, Y) \\mid X=x]$\n\n**PROVE：**\n$$\n\\begin{gathered}\n\\mathbb{E}[Z \\mid X=x]=\\sum_{y} \\mathbb{E}[Z \\mid X=x, Y=y] P(Y=y \\mid X=x) \\\\\n=\\sum_{y} g(x, y) P(Y=y \\mid X=x)\\\\ \\text{[LOTE]} \\\\\n\n\\mathbb{E}\\left[Z_{x} \\mid X=x\\right]=\\sum_{y} g(x, y) P(Y=y \\mid X=x)\\\\\n\\text{[LOTUS]}\n\\end{gathered}\n$$\n#### 4. $\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X]$\n\n\n\n#### 5. Towering $\\mathbb{E}[\\mathbb{E}[X \\mid Y, Z] \\mid Z]=\\mathbb{E}[X \\mid Z]$\n\nA little trivial, but just remember two things:\n\n1. $\\mathbb{E}_{X \\mid Y, Z}[X \\mid Y, Z]$ is a function of two random variables Y and Z\n\n2. $\\mathbb{E}_{Y \\mid Z}\\left[\\mathbb{E}_{X \\mid Y, Z}[X \\mid Y, Z] \\mid Z\\right]=\\mathbb{E}_{Y \\mid Z}[g(Y, Z) \\mid Z]$ is a function of Z\n\n---\n\n**My intuition: when we calculate the expectation of function of multiple r.v. it can work as flatten which have the same sense when we reduce dimention in matrix** \n\n","tags":["Math","Probability and Random Processes","Conditional PMF&Expectation"],"categories":["Math","Probability and Random Processes"]},{"title":"赫尔曼·黑塞《雾中》","url":"/2021/08/06/雾中/","content":"\n<img src=\"/images/fog.jpg\" width=\"500\">\n\n## 雾中\n\n在雾中散步真是奇妙！\n\n一木一石都很孤独，\n\n没有一棵树看到别棵树，\n\n棵棵都很孤独。\n\n当我生活得开朗之时，\n\n我在世上有很多友人；\n\n如今，\n\n由于大雾弥漫，\n\n再也看不到任何人。\n\n确实，\n\n不认识黑暗的人，\n\n决不能称为明智之士，\n\n难摆脱的黑暗悄悄地\n\n把他跟一切人隔离。\n\n在雾中散步真是奇妙！\n\n人生就是孑然孤独的样子。\n\n独处。\n\n没有一个人了解别人，\n\n人人都很孤独。\n\n### Im Nebel\n\nSeltsam, im Nebel zu wandern!\nEinsam ist jeder Busch und Stein,\nKein Baum sieht den andern,\nJeder ist allein.\n\nVoll von Freunden war mir die Welt,\nAls noch mein Leben licht war;\nNun, da der Nebel fällt,\nIst keiner mehr sichtbar.\n\nWahrlich, keiner ist weise,\nDer nicht das Dunkel kennt,\nDas unentrinnbar und leise\nVon allen ihn trennt.\n\nSeltsam, im Nebel zu wandern!\nLeben ist Einsamsein.\nKein Mensch kennt den andern,\nJeder ist allein.\n\n---\n\n{% meting \"1859874\" \"netease\" \"song\" \"autoplay\"%}\n\n\n\n","tags":["Life","Poem"],"categories":["Life","Poem"]},{"title":"线性代数笔记（四）","url":"/2021/07/30/线性代数笔记（四）/","content":"\n<!-- more -->\n\n**MIT 18.06 P7-8**\n\n7. Solving Ax = 0: Pivot Variables, Special Solutions\n\n8.  Solving Ax = b: Row Reduced Form R\n\n{% asset_img image.png %}\n\n> p6中介绍了两种构成矩阵A子空间的方法，p7,p8则介绍了具体求解Ax=0,Ax=b的方法\n\n###  求解Ax=0，主变量，特解\n\n>明确主变量和自由变量的概念后，可以定义主列和自由列，对其操作可以得到Ax=0的特解，特解的linear combination可以得到null space，reduced row  echelon form则可以进一步形式化解的表征。\n\n$$\nGiven \\qquad A=\\left[\\begin{array}{cccc}\n1 & 2 & 2 & 2 \\\\\n2 & 4 & 6 & 8 \\\\\n3 & 6 & 8 & 10\n\\end{array}\\right]\n$$\n\n#### AX=0的特解\n\n$$\n\\mathrm{A}=\\left[\\begin{array}{rrrr}\n1 & 2 & 2 & 2 \\\\\n2 & 4 & 6 & 8 \\\\\n3 & 6 & 8 & 10\n\\end{array}\\right] \\stackrel{\\text {elimination }}\\longrightarrow\\left[\\begin{array}{llll}\n\\underline{1} & 2 & 2 & 2 \\\\\n0 & 0 & \\underline{2} & 4 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]=\\mathrm{U}\n$$\n\n消元完成后，得到两个**主变量（pivot variable, 用下划线标出）**,代表**矩阵A的秩(rank)**为2, 即r=2. \n\n主变量所在的列为**主列(pivot column)**,其余列为**自由列(free column)**. 自由列中的变量为**自由变量(free variable)**. 对于一个m*n的矩阵, **自由变量的个数为n-r**,对于矩阵A为4-2=2. \n\n求解时，常赋值给自由列，随后求解主变量的值。求特解时将其中一个自由列赋值为1其他为0，随后通过back substitution求解方程组, 以此遍历所有自由列，**得到n-r个特解**。\n\n例如：\n$$\nUx=0\\rightarrow\\left\\{\\begin{array}{ll}\n x_{1}&+2x_{2}&+2x_{3} &+2x_{4}   =0 \\\\\n \t\t\t&\t\t\t\t&+2x_{3} &+4x_{4}   =0 \\\\\n\n\\end{array}\\right.\n$$\n观察消元矩阵U可以得到col2,col4为自由列，因此$x_{2},  x_{4}$​​​​为自由变量，令$x_{2}=1,  x_{4}=0$​​​​,我们有**特解1**=$[-2~~1~~0~~0]^T$​​​​ , 令$x_{2}=0,  x_{4}=1$​​​​我们有**特解2**=$[2~~0~-2~~1]^T$​​​​​​​\n\n\n\n(因为row3是row1和row2的linear combination, 所有消元的过程中row3变成了zero vector, 在back substitution的过程中体现为可以省去求解冗余的方程式)\n\n#### 特解构成零空间null space\n\n为了更高效的求解null space, 这里应用到了特解，因为特解构成的方式决定了它们都是independent的, 所以它们的linear combination可以高效的解得null space.因此结合上例即\n$$\na\\left[\\begin{array}{c}\n2 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n\\end{array}\\right]+b\n\\left[\\begin{array}{c}\n2 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{array}\\right]\n$$\n\n\n#### 规范的阶梯形矩阵\n\n$$\n\\mathrm{U}=\\left[\\begin{array}{llll}\n\\underline{1} & 2 & 2 & 2 \\\\\n0 & 0 & \\underline{2} & 4 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\longrightarrow\\left[\\begin{array}{cccc}\n\\underline{1} & 2 & 0 & -2 \\\\\n0 & 0 & \\underline{1} & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]=\\mathrm{R}\n$$\n\n将矩阵U化简为**R矩阵(Reduced row echelon form)**，主元为1其上下的元素为0. 对矩阵R进行列交换得到以下的形式\n$$\n\\mathrm{R}=\\left[\\begin{array}{cccc}\n\\underline{1} & 2 & 0 & -2 \\\\\n0 & 0 & \\underline{1} & 2 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right] \\stackrel{\\text {col exchange}}\\longrightarrow\n\\left[\\begin{array}{cc|cc}\n1 & 0 & 2 & -2 \\\\\n0 & 1 & 0 & 2 \\\\\n\\hline 0 & 0 & 0 & 0\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\mathrm{I} & \\mathrm{F} \\\\\n0 & 0\n\\end{array}\\right]=R_{col}\n$$\n**I(identity)为单位矩阵**由简化后pivot column组成，**F(free)为free column组成的矩阵**. 我们要计算\n\n随后计算$R_{col}N=0$​​​​，可以得到\n$$\nN=\\begin{bmatrix}-F\\\\I\\end{bmatrix}=\\begin{bmatrix}-2&2\\\\0&-2\\\\1&0\\\\0&1\\end{bmatrix} \\quad \\text{where order of x component is }\\begin{bmatrix}x_{1}\\\\x_{3}\\\\x_{2}\\\\x_{4}\\end{bmatrix}\n$$\n**P.S.** 这里教授讲的时候一开始绕的很晕，突然搞出一个R又突然分块矩阵，后来想明白这里相当于是一种优雅的计算方式，最后可以以简洁的形式表征求得的解，它把原本需要通过back substitution来完成的操作，抽象到matrix运算中，此外教授提到MATLAB中特解也是这种方法求的，那么我估摸着这种方法应该也能节省计算复杂度。（还有比较容易混淆的地方是，这边教授说这里的几个矩阵变化不会改变特解，但是需要注意的是col exchange以后，对应x向量中component是要跟着变换位置的，所以求得的n是更换了component位置的) . BTW col exchange是要让free column并在一起形成F，方便把assign的操作用单位矩阵替掉，把问题简化成$I*?+F*I=0$​​​, 这里也能体现出之前求R的意义，相当于提前把答案归一化——妙就完事了。\n\n----\n\n### 求解Ax=b 可解性，解的结构\n\n>在Ax=0解的基础上，可以继续探讨了解Ax=b的可解性，以及解的结构。\n\n#### Ax=b的可解性\n\n$$\nGiven~3\\times4~matrix \\qquad A=\\left[\\begin{array}{cccc}\n1 & 2 & 2 & 2 \\\\\n2 & 4 & 6 & 8 \\\\\n3 & 6 & 8 & 10\n\\end{array}\\right] \\\\assume ~~~~~~~~~b= \\begin{bmatrix}1\\\\5\\\\6\\end{bmatrix}\n$$\n\n求Ax=b的特解，可先写出其增广矩阵[A|b]的形式，并进行消元\n$$\n\\left[\\begin{array}{cccc|c}\n1 & 2 & 2 & 2 & \\mathrm{~b}_{1} \\\\\n2 & 4 & 6 & 8 & \\mathrm{~b}_{2} \\\\\n3 & 6 & 8 & 10 & \\mathrm{~b}_{3}\n\\end{array}\\right]\\stackrel{\\text{elimination}}\\longrightarrow \\left[\\begin{array}{cccc|c}\n1 & 2 & 2 & 2 & \\mathrm{~b}_{1} \\\\\n0 & 0 & 2 & 4 & \\mathrm{~b}_{2}-2 \\mathrm{~b}_{1} \\\\\n0 & 0 & 0 & 0 & \\mathrm{~b}_{3}-\\mathrm{b}_{2}-\\mathrm{b}_{1}\n\\end{array}\\right]\n$$\n在之前已经明确了当**$b\\in C(A)$​(b属于A的列空间)**时,其必有解。\n\n这里从行的角度来看，0行是由各行的linear combination得到, 因此有解的条件(**solvability condition on b**)等价于向量b满足$b_{3}-b_{2}-b_{1}=0$​​​​​​。\n\n**P.S. 它们是Ax=b的必要条件, 这里教授没有细讲，而是接着讲解的结构**\n\n#### Ax=b解的结构\n\n##### Ax=b的特解\n\n令矩阵A中所有自由变量为0，求解方程关于主变量的解\n$$\n\\begin{cases} x_1 & + & 2x_3 & = & 1 \\\\ & & 2x_3 & = & 3 \\\\ \\end{cases}\n$$\n解得\n$$\n\\begin{cases} x_1 & = & -2 \\\\ x_3 & = & \\frac{3}{2} \\\\ \\end{cases}→x_{p}=\\begin{bmatrix}-2\\\\0\\\\ \\frac{3}{2}\\\\0\\end{bmatrix}\n$$\n\n##### Ax=b的解集\n\n**Ax=b的解集由其特解和A的null space组成**，A的null space由Ax=0特解的linear combination组成, 在本文第一阶段已经给出。因此对于矩阵实例A, 其解集可以表示为\n$$\n\\mathrm{x}_{\\text {complete }}=\\left[\\begin{array}{c}\n-2 \\\\\n0 \\\\\n\\frac{3}{2} \\\\\n0\n\\end{array}\\right]+\\mathrm{c}_{1}\\left[\\begin{array}{c}\n-2 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{array}\\right]+\\mathrm{c}_{2}\\left[\\begin{array}{c}\n2 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{array}\\right]\n$$\n即$X_c=X_p+nullspace(A)$​​\n\n---\n\nP.S. 这一部分教授作图讲了这个$X_c$​​,  得到的解系$X_{complete}$​相当于把在四维空间中的子空间null space移动到$X_{p}$​(向量空间中的一个点)的位置。显然$X_{p}$不为0向量, 因此$X_{c}$​​不再经过原点，因此其不为subspace, 这也是之前教授讲Ax=b中x所构成的空间不构成子空间的具体原因.\n\n##### 矩阵秩与Ax=b解 \n\n$$\n\\begin{array}{c|c|c|c}\n\\mathrm{r}=\\mathrm{m}=\\mathrm{n} & \\mathrm{r}=\\mathrm{n}<\\mathrm{m} & \\mathrm{r}=\\mathrm{m}<\\mathrm{n} & \\mathrm{r}<\\mathrm{m}, \\mathrm{r}<\\mathrm{n} \\\\\n\\mathrm{R}=\\mathrm{I} & \\mathrm{R}=\\left[\\begin{array}{l}\n\\mathrm{I} \\\\\n0\n\\end{array}\\right] & \\mathrm{R}=\\left[\\begin{array}{ll}\n\\mathrm{I} & \\mathrm{F}\n\\end{array}\\right] & \\mathrm{R}=\\left[\\begin{array}{cc}\n\\mathrm{I} & \\mathrm{F} \\\\\n0 & 0\n\\end{array}\\right] \\\\\n\\text { 1 solution } & 0 \\text { or } 1 \\text { solution } & \\infty \\text { solution } & 0 \\text { or } \\infty \\text { solution }\n\\end{array}\n$$\n\n对于任意$m\\times n$​​​​矩阵A, 其r(rank,秩)满足$r\\leq min(m,n)$​​\n\n- 当列满秩即r=n<m时(case 2), 没有free variable 因此nullspace里只有zero vector, 通解就是特解加上zero vector, 因此当$b\\in  C(A)$​时有唯一解（或者说消元后因为有zero row的存在需要combination of row对应的b的式子=0）, 也即解得的$X_{p}$​, 否则无解。\n\n- 当行满秩即r=m<n时(case 3), 有n-r个free variable , 因此有n-r个特解，构成nullspace加上特解因此有无数个。\n\n  P.S. 这里因为没有zero row 所以b没有任何限制所以一定有解，我的理解是因为col rank=m, 因此$C(A)=R^m$, 而b一定是属于$R^m$的，所以原本的前提在这个情况下一定成立。\n\n- 当行列满秩时r=m=n(case1), 结合考虑case2和case3, 因为没有free variable所以nullspace 里只有zero vector，通解即Ax=b的特解，又因为r=m则$b \\in C(A)$​​, 所以一定有解。因此只有唯一解。\n\n- 当行列均不满秩时r<m<n时, 其当$b \\in C(A)$ 时等价于case3的情况，而当$b \\in C(A)$​时则无解。\n\n","tags":["Math","Linear Algebra"],"categories":["Math","Linear Algebra"]},{"title":"线性代数笔记（三）","url":"/2021/07/13/线性代数笔记（三）/","content":"\n<!-- more -->\n\n**MIT 18.06 P5-6**\n\n5. Transposes, Permutations, Spaces R^n\n6. Column Space and Nullspace\n\n----\n\n## -转置, 置换, 空间\n\n### 置换矩阵\n\n上节课讲到的LU分解有要求在消元过程中是不能出现行交换的。因此教授给出$PA=LU$来表示包含行交换的消元过程。其中P用来交换行的位置以避免出主元(pivot)位置为0的情况。\n\n更具体地，P代表**置换矩阵(Permutation Matrix)**, 它可以由单位矩阵进行行重组而得到。需要注意的是单位矩阵属于置换矩阵但是并不进行任何行交换。一个n阶方阵有n!种可能的置换矩阵。\n\n并且$P$有一个性质即$P^{-1}=P^{T}$（其实就是**正交矩阵**), 因此$P^{T}P=I$.\n\n### 转置\n\n$\\begin{bmatrix}1&3\\\\2&3\\\\4&1\\end{bmatrix}^T=\\begin{bmatrix}1&2&4\\\\3&3&1\\end{bmatrix}$\n\n转置即矩阵中所有行向量变成列向量，可以由式$(A^T)_{ij}=A_{ji}$表示\n\n### 对称矩阵\n\n**对称矩阵(Symmetric Matrix)** ,即$A^{T}=A$, 转置后得到的仍为原来的矩阵。此外任何矩阵的转置乘上该矩阵得到的必定是对称矩阵, 因为$(P^{T}P)^T=P^TP\\\\$\n\n### 向量空间\n\n向量空间(Vector Spaces)要满足其中的component经过一定的operation(根据定义的rules)以后还在原来的向量空间中，也即需要是封闭的。 比如二维向量空间$R^{2}$, 它代表所有的二维Real Vectors的集合。比如x-y plane的第一象限就不是vector spaces, 因为其不满足**数乘封闭**(乘负的scalar，同理不包括原点的space也同样不满足零乘封闭)\n\n### 子空间\n\n子空间是向量空间中的向量空间, 比如$R^{2}$的子空间(subspace)有1. $R^2$本身 2. 所有经过[0,0]的直线 3. $Z$零向量。\n\n#### 如何创造子空间\n\n$R^{n}$的子空间可以由空间中某个矩阵实例A所有列向量的Linear combinations来组成，该空间被称为Column space, 记作$C(A)$。但是形成的空间$R^{m}$的m取决于那些列向量的性质, 但至少```m<=len(row(A))```。\n\n\n\n## -列空间和零空间\n\n教授继续接着子空间的话题, $R^{3}$两个子空间P和L的并集$P\\cup L$（平面和线）并不是子空间,因为当P的实例加上L的实例得到的结果不在并集中，也即不封闭。但是$P\\cap L$ 则属于$R^{3}$子空间。\n\n----\n\n$$\ngiven\\qquad A=\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}_{4 \\times3}\n$$\n\n这里三个列向量都是矩阵A所属的$R^{4}$的子空间, 它的列空间 $C(A)$由三个列向量所有的Linear combination组成,。这里由于只有三个列向量，因此$C(A)$无法得到full space $R^{4}$（具体原因下小节阐述）。\n\n为了进一步阐述原因，教授首先将问题提炼为列空间是否可以覆盖原来的向量空间$R^{4}$ ?(Does linear combination of C(A) build $R^{4}$  ), 为了解答这个问题教授结合几节课前的linear combination, 将上式可以改写为\n$$\n\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=\\begin{bmatrix}b_{1}\\\\b_{2}\\\\b_{3}\\\\b_{4}\\end{bmatrix}\n$$\n于是问题就可以转化为Ax=b是否总有对于任意四维向量b的解?(Does Ax=b have a solution for every b)。 很显然四个方程三个未知数不一定有解。于是教师又引申出了两个新的问题，\n\n1. 怎么样的b向量可以有解?\n2. 这些b满足什么性质?\n\n关于1，显然零向量$\\begin{bmatrix}0& 0& 0\\end{bmatrix}^T$显然是一个解, 因为Ax=0一定有解, 其次b若是三个列向量中的任意一个也一定有解(这点从列向量的linear combination来理解就很直观, 就是01的摆放)，从这点推广开来，只要$b\\in C(A)$那么就一定有解，因为这个b就是通过线性组合得到，求解只是一个反推的过程。\n\n关于2，target b满足 $b\\in C(A)$\n\n---\n\n而如果直接从矩阵A的列向量入手，我们发现col1 col2 col3并不独立，它们线性相关，col1 和 col2构成平面, col3是平面内的一个向量，因此$C(A)=R^{2}$.\n\n### 零空间\n\n\n$$\n\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}\n$$\n\n矩阵A的零空间(Null Space)即所有满足Ax=0中x组成的空间，上式的例子中A的零空间$N(A)$属于$R^{3}$. 随后教授简单说明了下**零空间一定构成子空间**，并给出了简单直觉上的证明：\n\nx的任意两个实例，$Av=0$ $Aw=0$ ，显然$A(v+w)=0$ （分配率）, 因此满足加法封闭, 此外显然也满足数乘分布。\n\n末尾教授为了强调为什么右侧是零向量以及其意义,给了如下的例子\n\n$$\n\\begin{bmatrix}1&1&2\\\\2&1&3\\\\3&1&4\\\\4&1&5\\end{bmatrix}\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\end{bmatrix}\n$$\n\n----\n\n这时候显然x并不构成子空间, 因为零向量[0 0 0]并不在x构成的空间内，这违背了向量空间的定义。\n\n## 总结\n\n这两节教授主要介绍了两种构成矩阵A的full space子空间的方式：\n\n1. 从Ax=b的A入手, 满足Ax=b始终有解的b向量所构成的空间，也即A矩阵列向量的linear combination(列空间, column space)。\n2. 从Ax=b的b入手, 令b=0, 即满足Ax=0的x向量所构成的空间。\n\n本质上，在满足向量空间特性的要求以外最重要的一点是两种方式都确保了零向量一定存在于解得的空间中。\n\n","tags":["Math","Linear Algebra"],"categories":["Math","Linear Algebra"]},{"title":"线性代数笔记（二）","url":"/2021/07/06/线性代数笔记（二）/","content":"\n<!-- more -->\n\n**MIT 18.06 P3-4**\n\n3. Multiplication and Inverse Matrices \n\n4. Factorization into A = LU\n\n## 矩阵乘法和逆矩阵\n\n###  矩阵乘法\n\n教授讲解了四种方法来看待两个矩阵相乘，假设矩阵A(m$\\times$p), 矩阵B(p$\\times$n),矩阵运算的前提是左矩阵的行数等于右矩阵的列数,得到左矩阵行数乘上右矩阵列数规模的新矩阵C=A$\\times$B(m$\\times$n).\n\n#### 1-元素的角度-行列内积\n\n行列内积明确了C矩阵中每个元素(entry)的由来，如公式\n\n$$\nc_{ij}=row_{i}\\sdot column_{j}=\\sum_{k=1}^{p}e_{ik}e_{kj}\n$$\n\n#### 2-向量线性组合角度-行向量的角度\n$$\n{\n\\begin{bmatrix} % matrix A\n.&.&&.\\\\\n.&.&&.\\\\\n\na_{k1}&a_{k2}&...&a_{kp}\\\\\n.&.&&.\\\\ \n.&.&&.\\\\ \n\\end{bmatrix} \n\\begin{bmatrix} % matrix B\nB_{row1}\\\\B_{row2}\\\\\n...\\\\\n...\\\\B_{rowp}\n\\\\\n\\end{bmatrix} =  % matrix C\n\\begin{bmatrix}\n...\\\\\n...\\\\\na_{k1}B_{row1}+a_{k2}B_{row2}+...a_{kp}B_{rowp}\n\\\\...\n\\\\\n...\n\\end{bmatrix}\n}\n$$\n如果我们从行的角度来看，把C矩阵看作数个行向量，**C矩阵中任意第k行向量可以看作是矩阵B行向量关于矩阵A第i行元素的线性组合**。这很有用！**消元运算**中，课堂的例子教授都是以行向量的线性组合角度来口算和阐述问题的。\n\n#### 3-向量线性组合角度-列向量的角度\n$$\n{\n\\begin{bmatrix} % matrix A\nA_{col1}&\nA_{col2}&\n...&\nA_{colp}\n\\\\\n\\end{bmatrix}\n\\begin{bmatrix} % matrix B\n.&.&&b_{1k}&.&\\\\\n.&.&&b_{2k}&.\\\\\n\n.&.&...&.&.\\\\\n.&.&&.&.\\\\ \n.&.&&b_{pk}&.\\\\ \n\\end{bmatrix} \n\n =  % matrix C\n\n\\begin{bmatrix} % matrix A\n...&\n...&\nC_{colk}&\n...&\n...\n\\\\\n\\end{bmatrix}\n}\n\\\\\nwhere \\qquad C_{col_{k}}= b_{1k}A_{col1}+b_{2k}A_{col2}+\n...+\n...+b_{pk}A_{colp}\n$$\n如果从列的角度看，C矩阵看作数个列向量，**C矩阵中任意第k列都可以看作是矩阵A列向量关于矩阵B第k列元素的线性组合**。\n\n#### 4-**列向量乘行向量的角度**\n$$\n\\begin{bmatrix} % matrix A\nA_{col1}&\nA_{col2}&\n...&\nA_{colp}\n\\\\\n\\end{bmatrix}\n\\begin{bmatrix} % matrix B\nB_{row1}\\\\B_{row2}\\\\\n...\\\\\n...\\\\B_{rowp}\n\\\\\n\\end{bmatrix}\n=A_{col1}B_{row1}+A_{col2}B_{row2}+...A_{colp}B_{rowp}\n$$\n**P.S.** 讲到这里教授提了下$A_{coli}B_{rowi}$的大小， 即(m,1)$\\times$(1,n)→(m,n)，这与最终得到的矩阵的大小是相等的，因此更多行列向量乘的累加让得到的矩阵更加逼近最后的结果。\n\n---\n\n然后教授又提到了分块矩阵，之前听课的时候还纳闷为啥一下子提到分块矩阵，后来整理的时候发现上面的行列相乘也可以看作是分块矩阵的一个特例。\n\n分块矩阵乘法如下：\n$$\n\\begin{array}{l}\n\\left[\\begin{array}{l|l} %vline\n\\mathrm{A}_{1} & \\mathrm{~A}_{2} \\\\\n\\hline \\mathrm{A}_{3} & \\mathrm{~A}_{4} %hline\n\\end{array}\\right]\\left[\\begin{array}{c|c}\n\\mathrm{B}_{1} & \\mathrm{~B}_{2} \\\\\n\\hline \\mathrm{B}_{3} & \\mathrm{~B}_{4}\n\\end{array}\\right]=\\left[\\begin{array}{c|c}\n\\mathrm{A}_{1} \\mathrm{~B}_{1}+\\mathrm{A}_{2} \\mathrm{~B}_{3} & \\mathrm{~A}_{1} \\mathrm{~B}_{2}+\\mathrm{A}_{2} \\mathrm{~B}_{4} \\\\\n\\hline \\mathrm{A}_{3} \\mathrm{~B}_{1}+\\mathrm{A}_{4} \\mathrm{~B}_{3} & \\mathrm{~A}_{3} \\mathrm{~B}_{2}+\\mathrm{A}_{4} \\mathrm{~B}_{4}\n\\end{array}\\right]\\\\\n\\end{array}\n$$\n### 逆（方阵）\n\n并非所有方阵都有逆；而如果逆存在，则$A^{-1}A=I=AA^{-1}$。对于方阵左逆等于右逆，但是对于矩阵而言左逆不一定等于右逆。\n\n有逆的矩阵被称为可逆的或非奇异(non-singular)的,不可逆的如$\\begin{bmatrix}2 &4\\\\1&2\\end{bmatrix}$, 之前提到过也即冗余的情况。它们同样也满足determinant(A)=0。\n\n---\n\n随后教授介绍了高斯-若尔当（Gauss-Jordan）方法来计算矩阵的逆，\n\n其基本思想是通过将$\\begin{array}{l|l}A&I\\end{array}$通过消元法将左侧的A变化成$I$ 来自动将右侧的单位矩阵变成A的逆矩阵，也即$\\begin{array}{l|l}I&A^{-1}\\end{array}$。\n\n\n\n## LU分解\n\n教授先给了一些等式，\n\nAB的逆矩阵：\n\n$$AA^{-1}=I=A^{-1}A\\\\(AB)·(B^{-1}A^{-1})$$\n\n因此AB的逆矩阵是$B^{-1}A^{-1}$,这里教授讲了个笑话，说这就像穿先得穿袜子再穿鞋子,而脱的时候则先脱鞋后脱袜。我的理解本质上还是矩阵乘法不满足交换律导致的。\n\n由此教授又抛出了$A^{T}$逆矩阵的问题,通过下列等式,\n$$\n\\begin{array}{l}\n\\begin{array}{l}\n\\left(\\mathrm{A} \\cdot \\mathrm{A}^{-1}\\right)^{\\mathrm{T}}=\\mathrm{I}^{\\mathrm{T}} \\\\\n\\left(\\mathrm{A}^{-1}\\right)^{\\mathrm{T}} \\cdot \\mathrm{A}^{\\mathrm{T}}=\\mathrm{I}\n\\end{array}\\\\\n\\end{array}\n$$\n$A^{T}$的逆矩阵为$(A^{-1})^{T}$\n\n### LU分解\n\nLU分解也即三角分解，L代表下三角矩阵(lower triangle matrix), U代表(upper triangle matrix), 它是高斯-若尔当消元法的一种表达形式，它的基本思想是左乘A的消元矩阵的逆矩阵，使得等式左边只剩下A而右边U矩阵则左乘上变化矩阵的逆矩阵。\n$$\n\\begin{array}{l}\nE_{p} \\cdot \\ldots \\cdot E_{3} \\cdot E_{2} \\cdot E_{1} \\cdot A=U \n\\\\\nE_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots \\cdot E_{p}^{-1} \\cdot E_{p} \\cdot \\ldots \\cdot E_{3} \\cdot E_{2} \\cdot E_{1} \\cdot A=E_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots \\cdot E_{p}^{-1} \\cdot U \\\\\nI \\cdot A=\\left(E_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots \\cdot E_{p}^{-1}\\right) \\cdot U\n\\end{array}\n$$\n其中\n$$\nA=L \\cdot U \\\\ L=E_{1}^{-1} \\cdot E_{2}^{-1} \\cdot E_{3}^{-1} \\cdot \\ldots E_{p}^{-1} \n$$\n### LU分解的前提\n\n1. 矩阵是方阵\n\n2. 矩阵是可逆的\n3. 消元过程中没有0主元出现, 即不能行交换\n\n----\n\n### LU分解的计算量\n$$\n\\left[\\begin{array}{cccc}\n\\mathrm{a}_{11} & \\mathrm{a}_{12} & \\cdots & \\mathrm{a}_{1 \\mathrm{n}} \\\\\n\\mathrm{a}_{21} & \\mathrm{a}_{22} & \\cdots & \\mathrm{a}_{2 \\mathrm{n}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathrm{a}_{\\mathrm{n} 1} & \\mathrm{a}_{\\mathrm{n} 2} & \\cdots & \\mathrm{a}_{\\mathrm{nn}}\n\\end{array}\\right] \\stackrel{\\text { 消元 }}{\\longrightarrow}\\left[\\begin{array}{cccc}\n\\mathrm{a}_{11} & \\mathrm{a}_{12} & \\cdots & \\mathrm{a}_{1 \\mathrm{n}} \\\\\n0 & \\mathrm{a}_{22} & \\cdots & \\mathrm{a}_{2 \\mathrm{n}} \\\\\n0 & \\vdots & \\ddots & \\vdots \\\\\n0 & \\mathrm{a}_{\\mathrm{n} 2} & \\cdots & \\mathrm{a}_{\\mathrm{nn}}\n\\end{array}\\right]\n$$\n这是消元的第一步,其运算操作主要为<multiply,subtract>,因此对于主元a11其需要1次相乘和99次相减(注意:LU分解中没有行交换),又因为每行向量有100个元素，所以第一次操作的运算为100*100,随后一次递减。可以得到其总的运算量为$O(n^{2}+(n-1)^{2}+\\cdots+2^{2}+1^{2})$，即$O(\\frac{n^3}{3})$\n\n**P.S.** \n$$\n\\sum_{k=1}^{n} k^{2}=1^{2}+2^{2}+3^{2}+\\cdots+n^{2}=\\frac{n^{3}}{3}+\\frac{n^{2}}{2}+\\frac{n}{6}=\\frac{n(n+1)(2 n+1)}{6}\n$$\n\n\n### 置换矩阵\n\n最后教授为下节课铺垫提到了置换矩阵(permutation matrix),  n阶置换矩阵有$\\left(\\begin{array}{c}\n\\mathrm{n} \\\\1\\end{array}\\right)$个,即n!\n\n","tags":["Math","Linear Algebra"],"categories":["Math","Linear Algebra"]},{"title":"线性代数笔记（一）","url":"/2021/07/03/线性代数笔记（一）/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n**MIT 18.06 P1-2**\n\n1. The Geometry of Linear Equations\n2. Elimination with Matrices \n\n## P1 方程组的几何解释\n\n教授从求解线性方程组$\\left\\{\\begin{array}{ll}\n2 x&-y   =0 \\\\\n-x & +2y =3\n\\end{array}\\right.$开始, \n\n随后给出其矩阵形式$\\left[\\begin{array}{cc}\n2 & -1 \\\\\n-1 & 2\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\mathrm{x} \\\\\n\\mathrm{y}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n3\n\\end{array}\\right]$ 这里第一个矩阵被称为系数矩阵A, 第二个则称为向量x而第三个为向量b,因此线性方程组可以表示为Ax=b.\n\n----\n\n\n\n从传统几何意义上看，求解方程组即求解两条直线的交点，但是我们也可以将上式看作向量的**线性组合（linear combination）**，也即$\\mathrm{x}\\left[\\begin{array}{c}\n2 \\\\\n-1\n\\end{array}\\right]+\\mathrm{y}\\left[\\begin{array}{c}\n-1 \\\\\n2\n\\end{array}\\right]=\\left[\\begin{array}{c}\n0 \\\\\n3\n\\end{array}\\right]$, 通过组合矩阵A的两个列向量来得到矩阵b,\n\n无论是以哪种角度, 同样的, 我们都可以得到(x,y)=(1,2)。\n\n---\n\n教授在这个环节启示大家，试想两个列向量所有的线性组合的集合是什么----是平面。那俩列向量什么情况下，这些线性组合无法构成一个平面（解方程的角度的话即无法求得唯一解）----两个列向量在一条直线上的时候，也即它们之间存在倍数关系。\n\n显然，这样俩向量无论如何线性组合都一定是在直线上，因此无法构成一个平面。换句话来说向量之中存在冗余。这里仅仅探讨了二维的情况，这个现象可以推广到高维上。机器学习中特征的冗余本质上就是这个情况的具象。并且在线性代数中会有更专业的名词去描述这一现象，也即之后涉及到的矩阵是否**奇异（singular）、可逆（inverse）**的性质。\n\n-----\n\n最后教授建议，对于矩阵相乘\n$\n\\mathrm{A}=\\left[\\begin{array}{ll}\n2 & 5 \\\\\n1 & 3\n\\end{array}\\right], \\mathrm{x}=\\left[\\begin{array}{l}\n1 \\\\\n2\n\\end{array}\\right]\n$\n比起传统的使用向量内积（inner product）\n\n$\\left[\\begin{array}{ll}2 & 5\\end{array}\\right] \\cdot\\left[\\begin{array}{ll}1 & 2\\end{array}\\right]^{\\mathrm{T}}=12,\\left[\\begin{array}{ll}1 & 3\\end{array}\\right] \\cdot\\left[\\begin{array}{ll}1 & 2\\end{array}\\right]^{\\mathrm{T}}=7$\n\n他更建议以下式列向量的线性组合的方式来看。\n\n$\\left[\\begin{array}{ll}2 & 5 \\\\ 1 & 3\\end{array}\\right]\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right]=1\\left[\\begin{array}{l}2 \\\\ 1\\end{array}\\right]+2\\left[\\begin{array}{l}5 \\\\ 3\\end{array}\\right]=\\left[\\begin{array}{c}12 \\\\ 7\\end{array}\\right]$\n\n----\n\n## P2 矩阵消元\n\n### 消元方法\n\n给出三元方程组$\\begin{cases}\n-x &+2y&+z=2\\\\\n3x &+8y&+z=12\\\\\n&+4y&+z=2\n\\end{cases}$ ,\n\n对应的矩阵形式为$Ax=b→\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n x\\\\\n y\\\\\n z\\\\\n\\end{bmatrix}\n=\\begin{bmatrix}\n 2\\\\\n 12\\\\\n 2\\\\\n\\end{bmatrix}$\n\n\n\n消元的操作教授给出了详细的讲解，操作如下\n\n$\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}\\stackrel{\\text{row2-3row1}}{\\longrightarrow}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}\\stackrel{\\text{row3-2row2}}{\\longrightarrow}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&0&5\\\\\\end{bmatrix}$\n\n这里需要注意的是b矩阵需要和A矩阵进行相同的变换，因此在我们实际运算中，我们则会把b矩阵并行放置于A矩阵后面写成增广矩阵(augmented matrix)的形式，如下\n\n$\n\\left[\\begin{array}{ll}\n\\mathrm{A} \\mid \\mathrm{b}\n\\end{array}\\right]=\\left[\\begin{array}{ccc|c}\n1 & 2 & 1 & 2 \\\\\n3 & 8 & 1 & 12 \\\\\n0 & 4 & 1 & 2\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{ccc|c}\n1 & 2 & 1 & 2 \\\\\n0 & 2 & -2 & 6 \\\\\n0 & 4 & 1 & 2\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{ccc|c}\n1 & 2 & 1 & 2 \\\\\n0 & 2 & -2 & 6 \\\\\n0 & 0 & 5 & -10\n\\end{array}\\right]\n$\n\n随后回代(back substitution)得到方程组$\\begin{cases}x &+2y&+z&=2\\\\\n& 2y &-2z&=6\\\\\n&&5z&=-10\\end{cases}$即可求解。\n\n---\n\n### 消元矩阵\n\n此课第二部分教授介绍了消元矩阵，原始矩阵乘以对应的消元矩阵可以完成上述row2-3row1,row3-2row2操作。\n\n这里作为铺垫，教授提到了之前所说的以linear combination的角度来看矩阵相乘也即$\\begin{bmatrix}v_1 &v_2&v_3\\end{bmatrix}\\begin{bmatrix}3 \\\\4\\\\5\\end{bmatrix}=3v1+4v2+5v3$ \n\n这里显而易见，右侧的矩阵是对左侧的矩阵进行列的线性组合。但事实上在消元的过程中我们是对矩阵的行进行操作。因此我们可以将其看作是左侧矩阵对右侧的行向量进行线性组合。\n\n$\\begin{bmatrix}3 &4&5\\end{bmatrix}\\begin{bmatrix}row1 \\\\row2\\\\row3\\end{bmatrix}=3*row1+4*row2+5*row3$\n\n\n\n由此，我们可以左乘消元矩阵来对原始矩阵进行消元操作。对于第一步row2-3row1主需要在最初的矩阵A左侧乘上消元矩阵$E_{21}$也即$E_{21}A=W→\\begin{bmatrix}1&0&0\\\\\n -3&1&0\\\\\n 0&0&1\\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}$\n\n----\n\n**P.S.** 这里关于如何口算$E_{21}$,可以从右侧行向量的线性组合来理解，W矩阵的第一行是由A矩阵中每一行关于$E_{21}$矩阵的第一行对应元素的线性组合。因为矩阵相乘时左侧矩阵的行数和右侧矩阵的列数总是相等的，因此$E_{21}$第一行元素的列索引与其相乘的行向量的行索引的是相等的。运算时只需要要遍历索引然后相加即可得到结果。比如第一行即为$1*[1\\quad2 \\quad1]+0*[3\\quad 8\\quad1]+0*[3\\quad 8\\quad1]$得到$[1\\quad 2\\quad1]$。\n\n这里很自然的，我们可以发现如果$E_{21}$中某一行只有对角元素为1其余为0，那么目标矩阵W中对应的那行与原来A矩阵的那行相同，也即不发生变化。因为行向量的线性组合中只有原本的那行且系数为1。而若原始矩阵乘上一个只有对角线上的元素为1其余皆为0的矩阵时，原始矩阵不发生任何变化。而这样的矩阵在线性代数中被定义为**单位矩阵(Identity Matrix)**  $I=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}$\n\n---\n\n类似的, 我们可以得到$E_{32}$, 如下所示\n\n$E_{32}W=U→\\begin{bmatrix}1&0&0\\\\\n 0&1&0\\\\\n 0&-2&1\\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&0&5\\\\\\end{bmatrix}$ 所以整个消元的过程即为$E_{32}(E_{21}A)=U$,这里由于矩阵运算中结合律是并证明适用的，我们可以写成$(E_{32}E_{21})A=U$,因此消元的过程其实是可以一步到位的.\n\n-----\n\n**P.S. **单位矩阵$I=\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}$是主对角线元素为1其余为0 ，而若某一矩阵是副对角线的元素皆为1其余为0$\\begin{bmatrix}0&0&1\\\\0&1&0\\\\1&0&0\\end{bmatrix}$又有什么含义吗----这其实是**置换矩阵(Permutation Matrix)**，可用于交换目标矩阵的行向量\n\n$\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}=\\begin{bmatrix}c&d\\\\a&b\\end{bmatrix}$，也可交换列向量$\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}=\\begin{bmatrix}b&a\\\\d&c\\end{bmatrix}$\n\n----\n\n### 逆运算\n\n到现在，我们可以将A通过乘上变换矩阵变到U，那么从U变回A，即消元的逆运算又是怎么样的呢？是不是对于任何U都有逆变换可以回到A？\n\n以$E_{21}$为例, $E_{21}A=W→\\begin{bmatrix}1&0&0\\\\\n -3&1&0\\\\\n 0&0&1\\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}$,为了让W变回A，也即求解\n\n$?W=A→\\begin{bmatrix}\\\\\n &?&\\\\\n \\\\\\end{bmatrix}\\begin{bmatrix}1&2&1\\\\\n 0&2&-2\\\\\n 0&4&1\\\\\\end{bmatrix}=\\begin{bmatrix}1&2&1\\\\\n 3&8&1\\\\\n 0&4&1\\\\\\end{bmatrix}$. 显然由于原来的消元操作为row2-3row1因此我们只需要将row2被减去的3row1加回去就可以得到原始的A，也即取消变化矩阵$E_{21}$带来的改变。因此将其看作行向量的线性组合，row1和row3元素不变，row2加回row1的三倍，得到$\\begin{bmatrix}1&0&0\\\\3&1&0\\\\0&0&1\\end{bmatrix}$, 而该矩阵也即为$E_{21}$的逆矩阵,用$E_{21}^{-1}$表示，它们满足$E_{21}^{-1}E_{21}=I$。因此，这里引出了线性代数中逆矩阵的概念：矩阵$E$的矩阵着记作$E^{-1}$，并且有$E^{-1}E=I$.\n\n\n\n","tags":["Math","MIT 18.06"],"categories":["Math","Linear Algebra"]},{"title":"Python补基础（一）","url":"/2021/06/15/Python中的引用/","content":"\n<!-- more -->\n\n一些Python的基础知识，涉及变量、对象、引用、计数、拷贝\n\n## 一些抄下来的废话\n\n-Python为**动态解释性**语言，赋值不需要事先**声明变量**，类型是运行过程中自动决定的\n\n-Python中都是**引用**\n\n-Python中**变量**和**对象**的关系是**引用**\n\n## Python中的变量、对象、引用\n\n### 变量（variable）\n\n- 第一次赋值即创建，再次'赋值'会改变改变的值\n\n- 变量名本身是无类型的，对象才有类型，变量只是引用了对象\n- 变量需要在使用前赋值，否则报错\n\n### 对象（Object）\n\n- 对象有类型\n\n- 对象生成时会得到一块内存空间来存储其值\n\n- 每个对象有两个标准的头部信息\n\n  - 类型标识符：标识对象的类型\n  - 引用计数器：决定是否回收对象\n\n### 引用（Reference）\n\n- Python中**变量**到**对象**的连接叫**引用**\n- **引用**是一种关系，通过**内存**中的**指针**形式实现\n- 赋值操作时，自动建立**变量**和**对象**之间的关系，即引用\n---\n\n**举个栗子**：\n\n  ```python\n  str='hello world'\n  print('type:',type(str),'\\n','value:','\\n',str,'id:',id(str))\n  ```\n\n  得到结果\n\n  ```shell\n  type: <class 'str'> \n  value: \n  hello world id: 140225804592048\n  ```\n\n>type查看对象str的类型标识符，结果为str即字符串类型，value查看对象str的值为'hello world',id用来查看对象的内存地址。\n>\n>赋值语句 ```str='hello world'``` 实际上是做的是，1 -替值为hello world的字符串对象开辟内存地址 2 -变量str引用对象的地址，相当于指针指向地址。\n\n## Python标准数据类型\n\n简单介绍了变量，对象和引用的基础知识，接下来介绍下Python中五大标准数据类型\n\nNumbers \n\nString\t\n\nList\n\nTuple\n\nDictionary\n\n- 在python中, String tuple number是不可变数据类型而 list dict是可变数据类型\n\n## 可变对象和不可变对象\n\n可变对象创建之后仍可继续修改，不可变对象则不可修改。具体一点：\n\n- 对象所指向的内存中的值可以被改变。变量（准确的说是引用）改变后，实际上是其所指的值直接发生改变，并没有发生复制行为，也没有开辟新的地址，通俗点说就是原地改变\n\n- 不可变对象所指向的内存中的值不能被改变。当改变某个变量时候，由于其所指的值不能被改变，相当于把原来的值复制一份后再改变，这会开辟一个新的地址，变量再指向这个新的地址\n\n  \n\n## 赋值、浅拷贝、深拷贝\n\n赋值：对象的引用\n\n浅拷贝：拷贝父对象，不会拷贝对象内部的子对象\n\n深拷贝：完全拷贝对象\n\n----\n\n先占个坑，明天下课回来继续更。\n","tags":["Python"],"categories":["Programming","Python"]},{"title":"终点和意义","url":"/2021/05/22/终点和意义/","content":"\n<!-- more -->\n\n{% meting \"5265370\" \"netease\" \"song\" %}\n\n意义困扰了我很多年，第一次意识到人生终点那会，心里真的很害怕，会恐惧也抗拒，那种凝视虚空的感觉让我感受到孤独无助。我甚至会在脑海里想象至亲离世时手握着他们苍白的手亦或是自己年迈衰老濒死之际自己的孩子在病床前守候的场景，就像电影《返老还童》开头渲染那一般。高考前的那段时间以及大学的前几年对终点的恐惧都让我十分挣扎尤其是在独自一人的难眠之夜。\n\n\n\n我觉得这是一件很残酷的事情，因为无论我在人生跑道上漫步或是奔跑领先或是落后，最后都有那么一个终点等着我，到那时无论心中还有多大的执念我也只能放手。可我想一直跑我并不想结束我也害怕结束，甚至贪婪的我还想和我的朋友亲人们一直在一起。但是终点就在那并且丝毫不受任何主观的影响甚至不存在客观上的改变。\n\n\n\n就像美剧《True Detective》中Rust的那段话:“我认为人类的意识是进化过程中的一个可悲的错误，这让我们变得太有自我意识了。自然从自身中抽离出一部分又化为自然，但从自然法则来说我们是不该存在的生物，我们被“拥有自我”这一幻觉给奴役了。因为感官体验和感觉相结合，被设定成让我们相信我们每个人都是某个人，可事实上我们谁都不是。我认为对于所有物种来说，最崇高的事情就是拒绝被设定、停止繁殖、手牵手走向灭亡。”\n\n\n\n个体拥有强烈的自我意识的确是荒谬的，这种强烈的自我意识让我很难过的洒脱，也很难积极投入到现实中，我是多想像《牛氓》中描述的那样“无论我活着，还是死去，我都是一只牛氓，快乐地飞来飞去。”就那么漫无目的的飞来飞去多好啊！\n\n\n\n对终点的思考似乎并不像是突然在我脑海中蹦出，我想这一切应该是归结于那时候自身拒绝社交把自己封闭在自己狭小的舒适圈中，因为曾经在多次目睹人性的恶毒后，精疲力尽，开始对人群和集体有了莫名的恐惧，我主动把自己边缘化了，把自己从集体中抽离出来。也因此我对一系列社会活动不再上心，于是乎那几年感觉自己浑浑噩噩的,靠游戏和动漫麻醉自己。意义的缺失让我感到迷茫，对于大家热衷的GPA，科研，竞赛都让我提不起兴趣来。我只是按部就班完成已经变成习惯的日常。\n\n\n\n那段时间观察身边的人成为了唯一的乐趣，人群中有为了填补心中的自卑感甘愿被虚荣奴役的，也有为了蝇头小利绞尽脑汁去利用别人并为此沾沾自喜的，还有笑里藏刀极尽恶毒的虚伪之人......当然也不乏真诚正直的，善良无所求真心帮助鼓励他人的。但是无论如何，大家好像都很忙碌，似乎身边的人都没有去在意既定的终点。\n\n\n\n一时间感觉自己就像是一个在人来人往的十字街口的幽灵，站在岔路中央，来来往往的行人从我身上穿过，漫无目的游荡的却似乎只有我一个。我想我是我迷失了,在这荒诞的人生中寻找意义就像是缘木求鱼。\n\n\n\n直到经历了很多事情后我才明白，就像臧克家的那句“人生永远追逐幻光，谁把幻光看作幻光，谁便沉入无边苦海” ，也许意义从来不在于意义本身。\n\n\n\n现实的拷打是我摆脱这个桎梏的重大契机，因为我从来不是一个可以从抽象概念之中建立起更高层认知的人，本能的怀疑让我无法专注于其中。本质上，我是那种需要空间和具象的人，我更擅长从实例中去抽象解构概念然后再去构建新的概念。也因此注定了现实和实践才是我摆脱这一牢笼的关键。\n\n\n\n还记得那段时间接连的情感上的挫败，朋友的利用和背叛以及直面残酷的生死离别让我感受到巨大的痛苦。消极情绪将我吞噬，那段时间十分嗜睡，因为梦中的世界还没变的一塌糊涂。可是一旦梦又醒了，一切还是照旧。\n\n\n\n幸运的是，假期里我收到了朋友的一次旅行邀约。是去新西兰，南半球的一个小岛国，也是在那里我见到了更大的世界，体验到了更多的新奇之物。\n\n\n\n那时候，出去看一看的想法开始萌生，也是从那时候我才真正开始对人生开始了模糊的规划，就像是模电中的正反馈。之后一切都开始悄然改变，不断地突破不断地和新的人接触，我体验到了更多，觉得充实。开始慢慢的不去思考人生的意义，也不再去畏惧终点。我找到了自己的路，并且只想坚定的走下去。\n\n\n\n叔本华说生命是一团欲望，欲望得不到满足就痛苦，满足便无聊，人生在痛苦和无聊之间摇摆。而我的欲望仅仅是不断去体验去突破人生的边际罢了。\n\n{% asset_img Newzealand.jpeg %}\n\n","tags":["Life","Meaning"],"categories":["Life","Record"]},{"title":"实验室Linux服务器环境部署(一)","url":"/2021/04/27/实验室Linux服务器环境部署（一)/","content":"\n<!-- more -->\n\n{% asset_img image.png %}\n\n因为并非CS科班，之前少有机会接触这方面的内容，对于计算机的底层架构和Linux都是比较陌生的，因此此篇只浅显介绍下大致原理以及miniconda创建虚拟环境并通过本地jupyter连接远程服务器的流程。\n\n----\n\n## 一、关于服务器\n\n服务器硬件配置如下：\n\n| CPU      | XEON 5115*2                    |\n| -------- | ------------------------------ |\n| 内存     | DDR4 2666 16G*8                |\n| RAID卡   | 2GB SAS RAID卡                 |\n| GPU卡    | NV 2080TI*6                    |\n| 固态硬盘 | 480G 2.5 SATA 6Gb R SSD        |\n| 机械硬盘 | 1.8TB 2.5寸 10K 12Gb SAS硬盘*5 |\n| 电源模块 | 2000W 电源模块X4               |\n\n 操作系统：**Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-128-generic x86_64)**\n\n每个团队都分配到了账号，管理员在创建目录的时候给每个团队建立了一个主目录，通常在/home下，团队对自己主目录的文件拥有所有权，可以用于进行各种操作。 \n\n```shell\nssh teamn@x.x.x.x\n```\n\n这一步是通过ssh远程连接服务器，其中x.x.x.x是服务器的地址，teamn则是分配到的账号。紧接着会让你输入密码(关于ssh的详细信息可参考[[1](https://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html)]\n\n```shell\nteamn@x.x.x.x's password: \n```\n\n这里是看不见输入的密码长度的，只需要在键盘上盲打然后按回车就行。密码正确后会显示登录服务器名和账号。可以输入命令**ls**来看当前目录的所有文件，连接时我们处于/home/teamn的用户主目录，我们也可以使用命令**cd ..**返回上级目录或者**cd file_name** (file_name当前目录下某个文件夹的名字)来进入到子目录中。\n\n```shell\nnvidia-smi\n```\n\nBTW上述命令可以查看GPU状态\n\n## 二、在服务器上安装Linux版Anaconda\n\n这里我直接本地下载然后，上传到服务器的目录，然后执行\n\n```shell\nbash Anaconda3-2020.11-Linux-x86_64.sh\n```\n\n进行安装，安装完以后可以输入conda进行验证。\n\n##  三、创建虚拟环境\n\n### -虚拟环境简单介绍\n\n目前我理解的创建虚拟环境的动机主要是 1.不同项目或者库的依赖不同，同一个包在不同项目中对应版本的不同，因此很难兼容，频繁的去upgrade或者downgrade明显过于繁琐。 2. 一个团队大家共用一个账号，如果共用一个环境显然会变得更加混乱。因此创建虚拟环境就十分有必要，使用时激活，环境配置都在激活的环境中进行，环境之间互不影响，并且可以本地或者远程clone别人的环境来进行自己的部署开发。 \n\n事实上，环境管理的工具很多，有virtualenv, Pipenv, conda,docker等。我们熟知的pip是包管理工具, virtualenv可以管理环境。而conda两者兼有，因此这里我们使用conda,conda还有miniconda，其中后者更加轻便。\n\n### -创建环境\n\n这里只介绍几种简单用法，具体可以参考官方文档[[2](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands)]\n\n创建名为myenv的环境\n```shell\nconda create --name myenv\n```\n创建默认环境带python为3.6版本\n```shell\nconda create -n myenv python=3.6\n```\n创建带有scipy库的默认环境\n```shell\nconda create -n myenv scipy\nor\nconda create -n myenv python\nconda install -n myenv scipy\n```\n\n### -常用命令\n\n```conda list```\n\n类似于pip list,可以查看当前环境目录安装的所有库\n\n```conda info -e```\n\n查看创建的所有环境，会显示所有环境的名字\n\n```conda activate env_name```\n\n激活环境, env_name是环境的名字\n\n```conda deactivate env_name```\n\n关闭环境\n\n## 四、本地连接服务器端Jupyter notebook\n\n这里简述实现流程，具体内容也可参考官方文档[[3](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html)]\n\n### - 在服务器端配置jupyter configuration\n\n**第一步,**生成jupyter笔记本的默认配置文件，如果已经存在会提示你是否要初始化。\n\n```shell\njupyter notebook --generate-config\n```\n\n**第二步，**设置笔记本的登录密码(免密登录的设置参考官方文档)\n\n```shell\njupyter notebook password\n---------------------------\nEnter password:  ****\nVerify password: ****\n[NotebookPasswordApp] Wrote hashed password to /Users/you/.jupyter/jupyter_notebook_config.json\n```\n\n**第三步，**修改jupyter笔记本配置文件，用文本编辑器打开jupyter_notebook_config.py文件，将其中对应行注释符#去掉修改字段或者直接在文件任一位置添加:\n\n```shell\nc.NotebookApp.ip = '*'\nc.NotebookApp.open_browser = False\n```\n\n这里文本编辑器使用vim.关于vim的操作可以参考[4](https://www.ruanyifeng.com/blog/2018/09/vimrc.html)\n\n### - 在服务器端开启jupyter server\n\n 完成基本设置好，然后在中bash输入\n\n```jupyter notebook  ```\n\n默认的端口为8888，但是有时候已经有人使用了该端口，为了避免冲突可以指定开启端口号\n\n```shell\njupyter notebook --port YYYY(四位任意)\n```\n\n如果在本机这样操作会直接在浏览器跳出jupyter web页面，在服务器端因为没有GUI并且在config中设置了 no browser，因此  bash 会提示server 运行在B端口。接下来我们就可以放着先不管了。\n\n### -创建本地到服务器端的映射\n\n事实上也可以改jupyter config的一些配置，直接远程访问该jupyter（remote address:YYYY），这里由于某些原因我失败了暂时选择了如下方法。\n\n在本地终端输入:\n\n```ssh -N -f -L localhost:XXXX:localhost:YYYY  team6@192.168.156.31```\n\n-N 告诉SSH没有命令要被远程执行\n\n-f 告诉SSH在后台执行\n\n-L 指定port forwarding的配置 远程端口是YYYY 本地是XXXX\n\n相当于一个映射，把服务器地址映射到 localhost的一个端口。\n\n---\n\n随后打开任意浏览器地址栏输入localhost:XXXX并输入密码 xxxx（这是我之前在jupyter config里设置的）,即可进入 。\n\n这里如果我们想在jupyter上运行Shell命令，只需要在代码运行的cell前面加! ,   如果需要交互 可以在末尾加--yes 或者-- yes *\n\n## 五、上传下载文件\n\n如果是使用了远程终端软件，图形界面内自备了可视化接口就可方便上传下载文件。在bash中我们可以使用shell命令scp进行操作。\n\nscp local_dir teamn@192.168.x.x:remote_dir\n\n## References\n\n[1]https://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html\n\n[2]https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands\n\n[3]https://jupyter-notebook.readthedocs.io/en/stable/public_server.html\n\n[4]https://www.ruanyifeng.com/blog/2018/09/vimrc.html","tags":["Linux","Server","Environment"],"categories":["Linux","Server"]},{"title":"Lambda function","url":"/2021/04/23/Lambda-function/","content":"\n<!-- more -->\n\n## Basic idea\n\n第一次接触到匿名函数还是在学JAVA的时候，它能让代码变得简洁(pithy anonymity )。在Python中其主要是由保留字(keyword)中的Lambda实现的。它的原理很简单，但是有很多用法，配合其他的语法往往有奇效。\n\n\n\n事实上，在之前的博文{% post_link 'Lloyd-Max Quantizer' %}中代码实现部分的第7，8行就使用到了这个表达，当时的context是要对一个服从正态分布的随机变量进行积分，而积分函数quad需要输入积分形式的参数以进行积分。显然，lambda表达式让代码变得十分简洁易读。\n\n---\n\n{% asset_img lambda.png %}\n\n上图官方doc对Lambda的介绍，lambda_expr用来声明匿名函数，然后```lambda parameters: expression```语句为其生成一个函数对象，它与上图中def函数作用相同。\n\n## Usage\n\n下面简单介绍下常见的用法\n\n## 1. 单一参数\n\n实现2*x+1的运算\n\n```python\ndef f(x):\n  return x*2+1\nprint(f)\nprint(lambda x: x*2+1)\nprint('def way:',f(2))\nprint('lambda way',(lambda x: x*2+1)(2))\ng=lambda x: x*2+1\nprint(g(2))\n-----------------------------------------------\n# output\n'<function f at 0x7f4cbe4d2050>'\n'<function <lambda> at 0x7f4cbe4d2440>'\n'def way: 5'\n'lambda way: 5'\n'5'\n\n```\n\n通过上图，可以看到lambda表达式开辟了一个函数空间，我们可以选择赋予其一个函数名比如上面的g，也可以直接使用如表达式```(lambda x: x*2+1)(2)```\n\n---\n\n## 2. 多参数或者无参数\n\n### 多参数\n\n实现名和姓的合并输出\n\n```python\nname=lambda fn,ln: fn.strip().title()+\" \"+ln.strip().title()\nprint(name('LAST  ','   XUAN'),'\\n',name('shiny  ','   ruo'))\n-----------------------------------------------\n# output\n'Last Xuan'\n'Shiny Ruo'\n```\n\n其中strip和title是为了自动纠正不规范输入的函数，前者去掉内容收尾冗余的空格后者让首字母大写其余小写。\n\n### 无参数\n\nlambda表达式不添加参数，相当于过程函数，函数执行函数体内的句子但不返回任何值。\n\n```python\nprocess_func=lambda : print('nothing to return')\nprocess_func()\n-----------------------------------------------\n# output\n'nothing to return'\n```\n\n### 内嵌于其他函数\n\n事实上函数也可以作为函数的参数，一开始quad函数就是这种情况的一个实例。这种情况下所需的函数往往并不是很复杂，但是又需要有一定的灵活性，那么lambda函数就显得很便捷。如常用的sort,filter,map,reduce等，具体的用法在下一次更新~\n\n\n\n","tags":["Python","Lambda function"],"categories":["Programming","Python","Lambda function"]},{"title":"Interpretable Machine Learning(LIME-1)","url":"/2021/04/15/Interpretable Machine Learning/","content":"\n## 关于LIME\n因为研究需要，得弄懂kernel SHAP所以先得弄明白LIME，不想这东西还挺有意思的，该算法发表在2016的KDD上，先挂个介绍视频吧。\n\n{% youtube hUnRCxnydCc %}\n视频简单形象介绍了LIME以及该算法的motivation和intuition. 总结一下，我们可以用LIME去1) 在几个旗鼓相当（性能相似）的模型中做选择。2）去鉴别不值得信任的模型并改善。3）从模型中得到新的发现灵感。\n\n具体一点，\n1）的应用主要是在满足metric需要的模型之间找到更适合需求的模型，比如有的模型虽然perform well但是解释性一团糟，又比如有的语言模型涉嫌种族歧视……\n\n2）有一些模型perform beyond expectation，有很大嫌疑发生了data leakage（我就被这个坑惨了），比如说用来鉴别学生属于哪个班级，模型将学生ID作为特征，而由ID可直接推出学生班级。又比如图像领域，识别北极熊和棕熊，模型将雪地背景作为判别image 是否为北极熊的重要特征。这些模型虽然表现的很好但是却毫无意义（本质为过拟合），在部署上线后会变得一塌糊涂。\n\n3）这方面应用就比较灵活了，可以用于异常检测，也可以用于特征选择或者构建新的powerful feature....\n\n## LIME算法\n### IDEA\nLIME（Local Interpretable Model-Agnostic Explanations )属于局部代理模型，是一种可解释的模型用于解释黑盒机器模型对单个实例（individual）的预测。它的想法非常直觉，首先我们仅保留训练好的黑盒模型，然后扰动数据生成新的样本，通过黑盒模型得到这些样本的预测值作为LIME explainer的label，训练LIME explainer，由于explainer对比原来的黑盒模型更加简单，我们可以通过它作为原始黑盒模型的代理对感兴趣的样本点进行解释和分析。\n\n实际上，explainer可以是任何模型，但是因为复杂度的因素，Lasso（linear regression with L1）和decision tree通常被选作explainer.\n\n### Mathematics \n\n数学上，带有模型复杂度（可解释性）正则项限制的局部代理模型\n$$\n\\operatorname{explanation}(x)=\\arg \\min _{g \\in G} L\\left(f, g, \\pi_{x}\\right)+\\Omega(g)\n$$\n其中f函数代表待解释的black-box model, g函数则是在G函数空间中的一个解释性模型，$π_x$代表感兴趣样本x的邻样本范围的大小。\n\n\n\n显然，$π_x$如果越大，则有越多的远离interest point的实例被用于构建local surrogate explainer，可能会引入一些新的解释。其次关于正则项$\\Omega(g)$，我个人的理解是G空间中会有很多在损失函数上表现相当的函数，我们要从中选取那些复杂度低，解释性好的。具体的，该项可以用于heterogeneous models之间的选择，比如决策树和线性模型，也可以用于homogeneous models之间的选择，不同特征数量的线性模型或者不同深度不同叶子节点数目的决策树等....\n\n\n\n但是这里需要注意的是，在实际操作中，我们只对损失函数项进行优化，复杂度的正则项是通过我们预先限制模型的复杂度来得到的。\n\n### Recipe for raining local surrogate models\n\n其算法执行的流程大致如下:\n\n\n1） 选择感兴趣的实例（经由黑盒模型预测的某个实例）\n\n2）扰动数据集（采样）得到新的样本，并输入到黑盒模型中得到其预测值作为其标签。\n\n3）对这些新的样本根据与感兴趣实例的接近程度（类似特征向量的欧氏距离）来进行赋权。\n\n4）基于新的样本训练可解释模型\n\n5）通过可解释模型解释感兴趣的实例\n\n\n\n\n\n","tags":["Machine learning","Interpretable Machine Learning","LIME"],"categories":["Machine learning","Interpretable Machine Learning"]},{"title":"List comprehension","url":"/2021/04/11/list-comprehension/","content":"\n<!-- more -->\n\n最近学习了列表解析式(List comprehension )，它属于Python中的语法糖(Syntactic Sugar)。语法糖的出现主要是为了写程序的时候能少出错并且代码可以更简洁。这篇通过LeetCode的17. Letter Combinations of a Phone Number的一个解法引出这个表达。\n\n---\n\n# 引例\n\n题目不赘述了，给出链接[Letter Combinations of a Phone Number](https://leetcode.com/problems/letter-combinations-of-a-phone-number/)\n\n比较认同的一个解法如下:\n\n```python\nclass Solution:\n    def letterCombinations(self, digits: str) -> list:\n        graph = {'2':['a','b','c'], '3':['d','e','f'], '4':['g','h','i'], '5':['j','k','l'], '6':['m','n','o'], '7':['p','q','r','s'], '8':['t','u','v'], '9':['w','x','y','z']}\n        ans=['']\n        if digits==\"\":\n            return [] \n        for i in digits:\n            chars.append(graph[i])\n        for i in range(len(chars)):\n            ans=[c+chars[i][j] for c in ans for j in range(len(chars[i]))]\n        return ans\n```\n\n首先字典存储以数字为键对应字母列表为值的一系列item。我们要做的是把所要求的digits对应的可能字母组合全部找出来。\n\n\n\n先把每个数字对应的字母拼接在一个数组里即chars，需要注意的是此时的chars是个二维数组，因此len(chars)返回的为digits的长度,也即组合中任一元素的长度。\n\n\n\n紧接着为了得到可能的所有组合，我们需要显式的去设计循环，而解法中一个列表解析式就完成了所有操作，很简洁也很优雅。具体的逻辑在文末给出。\n\n-----\n\n## 关于List comprehension\n\n当我们定义有内容的list的时候，特别是在放入元素前做一些计算的时候，我们除了使用for 循环来添加列表元素，还可以在列表内直接写解析式计算。\n\n## List comprehension 用法\n\n### 1. [ expression for i in iterable ]\n\n```python\nans=[i+1 for i in range(10)]\nprint('ans:',ans)\n----------------------------\nans: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n```\n\n对于迭代对象进行加一运算并存放在ans数组中 其中iterable object: range(10) , expression: i+1。\n\n### 2. [ expression for i in iterable if...]\n\n如果我们需要对进入target list中的元素进行筛选，我们可以对迭代表达式的末尾加入if 语句\n\n```python\nans=[i+1 for i in range(10) if i%2==0]\nprint('ans:',ans)\n----------------------------\nans: [1, 3, 5, 7, 9]\n```\n\n之所以选这个例子，是为了使得表达式的逻辑变得更加清晰。通过if语句```if i%2==0```我们选取了0~9中为偶数的元素进行表达运算，+1以后得到```ans: [1, 3, 5, 7, 9]```\n\n### 3. [ expression for i in iterable if… for j in iterable if… …]\n\n更复杂点，双循环+if， 实现了5以内（复数，单数）的所有组合\n\n```python\nans=[(i,j) for i in range(5) if i%2==0 for j in range(5) if j%2==1]\nprint('ans:',ans)\n----------------------------\nans: [(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]\n```\n\n它实现的逻辑如下：\n\n```python\nans=[]\nfor i in range(5):\n    if i%2==1:continue\n    for j in range(5):\n        if j%2==1:ans.append((i,j))\nprint('ans:',ans)\n----------------------------\nans: [(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]\n```\n\n## 小结\n\n可以看出来，事实上列表表达式把显式的for循环和if语句简化了。实际中，expression对每一轮迭代循环结束后得到的元素进行操作。\n\n---\n\n# 回到引例\n\n```python\nfor i in range(len(chars)):\n    ans=[c+chars[i][j] for c in ans for j in range(len(chars[i]))]\n```\n\n这时候回到引例，\n\n{% asset_img Telephone-keypad.png %}\n\n再次强调for循环迭代的次数等于digits字符串的长度，也即最后ans中每个元素的长度。\n\n\n\n接下来分析在每轮迭代中列表解析式究竟做了些什么\n\n## 循环1\n\n首先第一层的循环是对于ans也就是我们target list进行的，变量c存储当前遍历元素。\n\n## 循环2\n\n然后第二层循环是对于 len(chars[i])进行的它返回的是digit中某个数字对应的可能字符个数（如图数字2有3个对应的字母，而数字7和9有4个), 变量j存储当前遍历索引。\n\n---\n\n## 表达式\n\n```c+chars[i][j]```chars[i]表示的是digits中数字的索引，j表示的是该数字对应的可能的字母的索引（对于digits'27',当i=0的时候，j会从0遍历到2;当i=7的时候，j会从0遍历3）。因此，思考下就能得知，实际上，每轮for循环target list中存放的是前i+1个数字的可能的字母组合。列表解析式所实现的双重循环在增加target list中元素的个数的同时通过expression增加元素的长度。\n\n","tags":["Python","List comprehension","LeetCode"],"categories":["Programming","Python","List comprehension"]},{"title":"双指针类型题解（一）","url":"/2021/04/06/双指针类型题解（一）/","content":"\n<!-- more -->\n\n# 快慢指针\n\n大意是把单链表中倒数第n个结点给剔除，链表不同于数组并没有显式的给出长度，也不能简单通过索引定位。因此找到目标结点就需要一些特殊的trick，事实上也有着很多奇妙的解法。但是在这里，快慢指针是比较容易想到也比较高效的解法。快慢指针的方法很简单就是初始化两个指针，在每一轮的迭代中它们移动的步长存在快慢差异。\n\n## 19. Remove Nth Node From End of List\n\n### 思路\n\n对于这题，我们可以让快指针先移动n步，然后再启动慢指针并保持步长为1的同速。最后当快指针到达链表尾结点的时候，停止迭代。因为快指针比慢指针多移动了n步，所以它到达终点时领先了慢指针n步，因此此时慢指针的索引即为我们目标索引。\n\n### 代码\n\n```python\nclass Solution:\n    def removeNthFromEnd(self, head: ListNode, n: int) -> ListNode:\n        fast=slow=head\n        for i in range(n):\n            fast=fast.next\n        if not fast:\n            return head.next # case: n=size\n        while fast.next:\n            fast=fast.next\n            slow=slow.next\n        slow.next=slow.next.next # delete n_th node from end\n        return head\n```\n\n### 谬误与反思\n\n一开始想错了 在上面for i = n+1这会导致溢出, 其次忽略了n=size的情况。\n\n---\n\n## 141. Linked List Cycle\n\n### 思路\n\n题目让判断链表是否存在环，想成操场追击问题，如果有环快指针先进入环，等待慢指针进入环后，想下速度v=2和v=1的俩人在操场跑步，它们一定会相遇的~\n\n### 代码\n\n```python\nclass Solution:\n    def hasCycle(self, head: ListNode) -> bool:\n        fast=slow=head\n        while (fast and fast.next):\n            fast=fast.next.next\n            slow=slow.next\n            if fast==slow:\n                return True\n        return False\n```\n\n### 谬误与反思\n\n1. 一开始忽略了[]空链表的情况，Nonetype 没有next attribute的报错。\n\n2. 一开始没有注意到while里的条件，事实上应当是，```  fast and fast.next```。and 短路逻辑运算符则很好的解决了无环情况下最后fast指针是否到达None结点的两种case，防止循环内快指针移动报错。\n\n## 142. Linked List Cycle II\n\n## 思路\n\n{% asset_img image.png %}\n\nquora上比较intuition的解答:\n\n{% asset_img solution_quora.png %}\n\n需要注意的是z的长度可能是z+n*length(circle), n是多少与x和环的长度有关，极端点设想环的长度是1就懂了。\n\n## 代码\n\n```python\nclass Solution:\n    def detectCycle(self, head: ListNode) -> ListNode:\n        fast=slow=head\n        while (fast and fast.next):\n            fast=fast.next.next\n            slow=slow.next\n            if fast == slow: break\n        if not (fast and fast.next): # if no circle\n            return None\n        while head is not fast:\n            head=head.next\n            fast=fast.next\n        return head\n```\n\n## 谬误与反思\n\n1. 一开始把空链表和单一元素链表的情况拎出去了，导致写的有点繁琐，事实上判断有无环的循环再进行判断这俩case会比较方便。\n\n2. 写判断的时候一开始脑子热了，把not(fast and fast.next)写成 not fast and not fast.next，其实相当于¬((a )∧(b))  ！= ¬a ∧ ¬b，此情景下应该是前者，（离散数学是真的忘光了QAQ）\n\n","tags":["LeetCode","Double pointer","Link list"],"categories":["Programming","LeetCode","Double pointer"]},{"title":"关于友谊","url":"/2021/04/05/些许记录和感悟/","content":"\n  就这样大学快毕业了，在杭州这座城市也呆了十年。很多事情恍惚间好像还是发生在昨天。我不是一个热衷于表达自我的人，我更倾向于去观察去感受。但是在这个结点我总觉得该写些什么去记录一些过往。可能是想在这梦幻般的二十几年中捕捉一点真实，也可能是想以这个方式分担背负在身上的压抑了很久的情感包袱。这一系列记录也将在我的技术博客中以友谊，爱情，爱好，学习，以及我对人生对这个世界的一些浅薄认知慢慢展开。\n\n## 论友谊\n\n  不知道为什么先是友谊，也许是我最近因此而烦恼吧？关于友谊，我的朋友很少，自然的社交对我来说从来都是一件很具挑战的事情。从小学开始，除了同班同学我很难结识新的朋友。这里需要澄清的一点大概是，并不是说我是一个很恶劣的人。只是过往经历和性格的原因导致我很少去主动认识新朋友，对于那些向我抛出橄榄枝的同学，我经常因为不知所措而冷漠回应，留下了不友好的印象。\n\n\n\n  当然，有时候我也能幸运的交到聊得来玩的开的好朋友，但是对于矛盾误会糟糕的处理以及对于维持关系的疏忽，许多朋友最后都失望离开。**这样的事情仍在发生，我似乎也从未很好吸取教训，讽刺的是直到寂寞再次袭来时才自私的开始意识到朋友的重要性。**\n\n\n\n  重新审视友谊，我发现自己一直以来没有很好理解什是友谊。小时候我总以为友谊是物质上的交互，借给同桌的橡皮擦，得到的生日礼物。后来我发现, 友谊不仅仅是物质上的交互也是情感的交流。“你讲的笑话好好笑”，“你真幽默”，“谢谢你安慰我”诸如此类。再后来，我发现友谊是包容是忍让，是真心真意为对方着想，希望对方可以变得更好并由衷的为此而高兴，是一种比较高级的情感。**它是对等的而不是建立在利益之上的互相利用或者是怜悯施舍对方从中获得优越感**。\n\n\n\n  友谊的敌人是猜疑和嫉妒，这无可厚非，因为人性如此，这是基因所带来的。本质上它们都是一种情绪，有时候超出我们的控制。我深受其害并且十分厌恶这种情绪，因为它野蛮不很优雅，并且它们破坏了很多友谊，也因此让我失去了很多很好的朋友。\n\n\n\n我曾经花了很多时间去留意这种本能的情绪也尝试阅读相关专业的研究。简而言之我倾向于认为，它来源于一种认知冲突，它在比较中将别人拥有而自己未有的落差转换为一种失去或是不得的感受，但是内心却不认可这个比较带来的冲突，为了解决这个冲突，内心进化出一种心理策略来平衡这种认知的矛盾。 这种情绪会在与自己不认可的人的比较中更为强烈。对内，这是一种很强的内耗，对外，它会表现出强的言语或者行为上的攻击性。\n\n\n\n  显而易见，它似乎没有任何益处。但事实上在生活中，甚至是朋友亲人之间这也十分常见，有时候它甚至能支配言语和行动。我清楚的记得，以前交好的一位朋友（带我入英雄联盟深坑的罪魁祸首），我们经常在一起玩一起打游戏，但是每次大小考成绩一出我总是拿自己和他的比较，如果我总分高就开心的不行浑身愉悦，要是低就会变得很低落。有时候很正常的关于学习经验的对话，我总会认为这是他变相的炫耀。终于一次尖锐的言语不由自主的从嘴里说出，面面相觑中，友谊已经出现无法恢复的伤痕。\n\n\n\n  那时候我还意识不到那种心情，直到后来在我取得些许成就之时，满心欢喜分享给好朋友的时候，对方刻薄的回应，我才体会到这是什么滋味，也明白了之前的自己究竟做了些什么蠢事。\n\n\n\n  不断的反思，对于嫉妒/妒忌这类的情感，我认为大方的承认别人的好，感受到自己的不足，虚心的去学习去改变自己才是对的。You only live once, 为什么不能少一点狭隘多一点坦诚呢？\n\n\n\n  友谊有敌人当然也有朋友，我认为友谊的朋友应该是包容和信任，它就像是友谊的修补剂，让接近分崩离析的友谊重新牢固，让其从牢固变得坚不可摧。但这种情感是反本能的，因为包容和信任意味着在受到背叛或者是威胁后选择依然相信。从概率论来说，像是一种条件概率，意味着你受到下次背叛或者威胁的概率将会变得更大。\n\n\n\n  但其实这也是合理的，因为朋友之所以能成为朋友就存在其他互相欣赏的地方。而且人无完人，每个人有自己的境遇有着不同的困惑也分别处在不同的人生阶段，矛盾和误会不可避免，生活的不如意也会将这种情绪放大无数倍。\n\n\n\n  回头望去，一路上其实有不少的朋友包容过我，鼓励过我。他们之中有的已经和我走散了，有的还是我的好朋友。但是不管如何，我都很感激他们，因此往后也希望自己变得更加包容，尝试去信任他人鼓励他人。我想这也是英文中常被提到的Be nice的含义吧。\n\n\n\n  说了这么多，我愈发觉得纯洁的友谊是多么难得多么奢侈的一样东西。所幸，在我屈指可数的朋友之中我还能瞥见它的影子。这也让我更加坚定的让自己变得更好，去珍惜维护它们，不像以前一样再毁了它。\n\n","tags":["Life","Friendship"],"categories":["Life","Record"]},{"title":"Lloyd-Max Quantizer","url":"/2021/04/05/Lloyd-Max Quantizer/","content":"# PDF资料\n{% pdf ./Max-Floyd.pdf %}\n# 作业要求\n>参考 Max-Floyd.pdf 中的example 2, 将$p(x)$修改成高斯分布$N(0,1)$\n\n# 代码实现\n```python\nimport scipy.stats as stats\nfrom scipy.integrate import quad\nimport math\ny1=0.3;y2=0.8;max_iterations=500;precision=1e-9\n# p=1\n# p=stats.norm.pdf(x,0,1)\nnum_func=lambda x: x*stats.norm.pdf(x,0,1)\nden_func=lambda x: stats.norm.pdf(x,0,1)\nfor i in range(max_iterations):\n    b1=(y1+y2)/2\n    Num1,Nerr1=quad(num_func,0,b1)\n    Den1,Derr1=quad(den_func,0,b1) \n    y1=Num1/Den1\n    Num2,Nerr2=quad(num_func,b1,1)\n    Den2,Derr2=quad(den_func,b1,1) \n    tmp=y2\n    y2=Num2/Den2\n    if abs(y2-tmp)<precision:\n        print('iterations:',i)\n        break\nprint('y1:',y1,'y2:',y2,'b1:',b1)\n```\n\n# 深入思考\n未完待写..","tags":["Machine learning","Python","Optimisation"],"categories":["Machine learning"]},{"title":"神经网络中的数据表示","url":"/2021/04/04/2021神经网络中的数据表示/","content":"\n# 神经网络中的数据表示\n\n矩阵运算加块了神经网络的计算速度，而在应用中数据常常存储在多维Numpy数组中，其也被称为张量(tensor). 当前基本所有机器学习系统都使用张量作为数据结构，它对这个领域尤为的重要，以TensorFlow的命名就可窥见。\n\n张量这一概念的核心在与，它是数据容器。 其中矩阵就是二维张量，张量事实上是矩阵向任意维度的推广。（张量的维度(dimension)常被称作轴(axis)，张量轴的个数被称为阶（rank））\n\n-----\n## 标量（scalar）0D张量\n Numpy中，一个 float32和float64的数字就是一个0D张量。其可用ndim属性来查看。0D张量的ndim==0,如下图所示。\n\n{% asset_img image-20210306204113139.png %}\n\n\n## 向量（vector）1D张量\n\n很多数据中标签是存储在1D张量中的。其形式如下\n\n{% asset_img image-20210306204514761.png %}\n\n\n这个向量中有6个元素，被称为6D向量，易与张量的阶数混淆。事实上6D向量只有一个轴，沿着轴有6个维度(dimensionality)。6D张量才有6个轴。其中，维度既可以表示某个轴上的元素个数，也可以表示张量中的轴数。后者更准确的叫法应该是6阶张量。但是6D张量这种写法常见。\n\n## 矩阵(matrix) 2D张量\n\n向量组成的数组叫矩阵。矩阵有两个轴，通常称为行（row）和列(column)，其形式如下\n\n{% asset_img image-20210306210108337.png %}\n3X2的矩阵\n\n\n## 3D张量\n{% asset_img image-20210306210342187.png %}\n\n以CIFAR10数据集为例\n\n{% asset_img image-20210306210616667.png %}\n\n3D张量分别存储 高度 宽度和颜色深度，其大小为32*32像素，颜色深度维数为3，分别对应RGB三个颜色通道。\n\n\n## 4D张量\n\n关于4D张量：当许多图片累加起来时3D张量就不足以存储，需要增加一个拥有N维的轴来存储N个图片样本。\n\n\n## 5D张量\n\n关于5D张量：其中视频数据可以被看做事5D张量的少数数据类型之一，视频可以被看做由一系列帧（Frame）构成。每一个帧可以保存在一个形状为（height,width,color_depth）的3D向量中，一系列帧可以保存在(frames,height,width,color_depth)的4D张量中，不同视频数据组成的批量可以保存在5D张量中(samples,frames,height,width,color_depth)","tags":["Deep Learning","Data Science"],"categories":["Programming"]}]